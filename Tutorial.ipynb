{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import air\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##### Custom package #####\n",
    "import pytorchGLM as pglm\n",
    "from pytorchGLM.main.training import train_network\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({'font.size':         10,\n",
    "                     'axes.linewidth':    2,\n",
    "                     'xtick.major.size':  3,\n",
    "                     'xtick.major.width': 2,\n",
    "                     'ytick.major.size':  3,\n",
    "                     'ytick.major.width': 2,\n",
    "                     'axes.spines.right': False,\n",
    "                     'axes.spines.top':   False,\n",
    "                     'font.sans-serif':   \"Arial\",\n",
    "                     'font.family':       \"sans-serif\",\n",
    "                     'pdf.fonttype':      42,\n",
    "                     'xtick.labelsize':   10,\n",
    "                     'ytick.labelsize':   10,\n",
    "                     'figure.facecolor': 'white'\n",
    "\n",
    "                    })\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  Niell lab Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "args = pglm.arg_parser(jupyter=True)\n",
    "\n",
    "##### Modify default argments if needed #####\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['date_ani']        = dates_all[0]\n",
    "args['free_move']       = True\n",
    "args['train_shifter']   = True\n",
    "args['Nepochs']         = 10000\n",
    "\n",
    "ModelID = 1\n",
    "params, file_dict, exp = pglm.load_params(args,ModelID,file_dict=None,exp_dir_name=None,nKfold=0,debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pglm.load_aligned_data(file_dict, params, reprocess=False)\n",
    "params = pglm.get_modeltype(params)\n",
    "datasets, network_config = pglm.load_datasets(file_dict,params,single_trial=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,xpos,y = datasets['xtr'][:10],datasets['xtr_pos'][:10],datasets['ytr'][:10]\n",
    "print(x.shape,xpos.shape,y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_aligned_data calls 2 functions when preprocessing the raw data: \n",
    "- format_raw_data: formats the raw data based on file_dict and params\n",
    "- interp_raw_data: interpolates the formamted data from format_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, goodcells = pglm.format_raw_data(file_dict,params)\n",
    "model_data = pglm.interp_raw_data(raw_data,raw_data['vid']['vidTS'],model_dt=params['model_dt'],goodcells=goodcells)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Formatting Base Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to formatting any custom datasets. The key components are: \n",
    "- Formatting data\n",
    "    - Train/Test Splits\n",
    "    - Inputs (time,in_features)\n",
    "    - Optional Inputs (time,pos_features)\n",
    "    - Outputs (time,Ncells)\n",
    "- network_config\n",
    "    - in_features: input dims\n",
    "    - Ncells: output dims\n",
    "    - initW: How to initialize weights, 'zero' or 'normal' \n",
    "    - optimizer: optmimizer to use: 'adam' or 'sgd'\n",
    "    - lr_w: learning rate for weights\n",
    "    - lr_b: learning rate for bias\n",
    "    - lr_m: learning rate for additional inputs\n",
    "    - single_trial: flag for single trial or hyperparam seach\n",
    "    - L1_alpha: L1 regularization parameter. Single value or hyperparam search\n",
    "    - L1_alpham: L1 regularization parameter. Single value or hyperparam search\n",
    "    - L2_lambda: L2 regularization parameter. Single value or hyperparam search\n",
    "    - L2_lambda_m: L2 regularization parameter. Single value or hyperparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pglm.arg_parser(jupyter=True)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_BaseModel_params(args,exp_dir_name='Testing',ModelID=0,nKfold=0,debug=False):\n",
    "    \"\"\" Load parameter dictionary for custom BaseModel network. Minimal implementation \n",
    "        adabpting to custom datasets\n",
    "\n",
    "    Args:\n",
    "        args (dict): Argument dictionary \n",
    "        exp_dir_name (str): name of experiment. \n",
    "        ModelID (int, optional): Model Identification number. Defaults to 0.\n",
    "        exp_dir_name (str, optional): Optional experiment directory name if using own data. Defaults to None.\n",
    "        nKfold (int, optional): Kfold number for versioning. Defaults to 0.\n",
    "        debug (bool, optional): debug=True does not create experiment directories. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        params (dict): dictionary of parameters\n",
    "        exp (obj): Test_tube object for organizing files and tensorboard\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "    from pathlib import Path\n",
    "    from test_tube import Experiment\n",
    "    \n",
    "    ##### Create directories and paths #####\n",
    "    date_ani2 = '_'.join(args['date_ani'].split('/'))\n",
    "    data_dir = Path(args['data_dir']).expanduser() / args['date_ani'] / args['stim_cond'] \n",
    "    base_dir = Path(args['base_dir']).expanduser()\n",
    "    save_dir = (base_dir / args['date_ani'] / args['stim_cond'])\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ##### Set up test_tube versioning #####\n",
    "    exp = Experiment(name='ModelID{}'.format(ModelID),\n",
    "                        save_dir=save_dir / exp_dir_name, \n",
    "                        debug=debug,\n",
    "                        version=nKfold)\n",
    "\n",
    "    save_model = exp.save_dir / exp.name / 'version_{}'.format(nKfold)\n",
    "\n",
    "    params = {\n",
    "        ##### Data Parameters #####\n",
    "        'data_dir':                 data_dir,\n",
    "        'base_dir':                 base_dir,\n",
    "        'exp_name_base':            base_dir.name,\n",
    "        'stim_cond':                args['stim_cond'],\n",
    "        'save_dir':                 save_dir,\n",
    "        'exp_name':                 exp.save_dir.name,\n",
    "        'save_model':               save_model,\n",
    "        'date_ani2':                date_ani2,\n",
    "        'model_dt':                 args['model_dt'],\n",
    "        ##### Model Parameters #####\n",
    "        'ModelID':                  ModelID,\n",
    "        'lag_list':                 [0], # List of which timesteps to include in model fit\n",
    "        'Nepochs':                  args['Nepochs'],\n",
    "        'Kfold':                    args['Kfold'],\n",
    "        'NoL1':                     args['NoL1'],\n",
    "        'NoL2':                     args['NoL2'],\n",
    "        'initW':                    'zero',\n",
    "        'train_shifter':            False,\n",
    "        'model_type':               'pytorchGLM_custom', # For naming files\n",
    "    }\n",
    "\n",
    "    params['nt_glm_lag']=len(params['lag_list']) # number of timesteps for model fits\n",
    "    params['data_name'] = '_'.join([params['date_ani2'],params['stim_cond']])\n",
    "    \n",
    "    ##### Saves yaml of parameters #####\n",
    "    if debug==False:\n",
    "        params2=params.copy()\n",
    "        for key in params2.keys():\n",
    "            if isinstance(params2[key], Path):\n",
    "                params2[key]=params2[key].as_posix()\n",
    "\n",
    "        pfile_path = save_model / 'model_params.yaml'\n",
    "        with open(pfile_path, 'w') as file:\n",
    "            doc = yaml.dump(params2, file, sort_keys=True)\n",
    "\n",
    "    return params, exp\n",
    "\n",
    "\n",
    "def initialize_GP_inputs(Npats,length_scale,batch_size,Nx_low,Nx,Ny_star,Nr,seed=42,multi_input=False,pytorch=True):\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "    ##### Set random seed #####\n",
    "    np.random.seed(seed+1)\n",
    "    torch.manual_seed(seed+1)\n",
    "    ##### Initialize RBF kernels #####\n",
    "    rbf = RBF(length_scale=length_scale)\n",
    "    genX = np.arange(Npats)[:,np.newaxis]\n",
    "    genY = np.arange(Npats)[:,np.newaxis]\n",
    "    Kx = rbf(genX,genX)\n",
    "    Ky = rbf(genY,genY)\n",
    "    if multi_input:\n",
    "        ##### Initialize inputs #####\n",
    "        x_low0 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
    "        x_low1 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
    "        x_expand = torch.randn(size=(batch_size,Nx_low,Nx)).float()\n",
    "        x0 = torch.bmm(x_low0,x_expand)\n",
    "        x1 = torch.bmm(x_low1,x_expand)\n",
    "        x_all = torch.stack((x0,x1),dim=1).float()\n",
    "        ##### Initialize target patterns #####\n",
    "        y_all = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Ky,size=(1,Ny_star,Nr))),3,2)\n",
    "        y_all = ((y_all/torch.max(torch.max(torch.abs(y_all),dim=1,keepdim=True)[0],dim=2,keepdim=True)[0]).repeat(batch_size,1,1,1))\n",
    "    else:\n",
    "        ##### Initialize inputs #####\n",
    "        x_low0 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
    "        x_expand = torch.randn(size=(batch_size,Nx_low,Nx)).float()\n",
    "        x_all = torch.bmm(x_low0,x_expand)#.numpy()\n",
    "        # x_all = torch.from_numpy((x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)).float()\n",
    "        ##### Initialize target patterns #####\n",
    "        y_all = torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Ky,size=(1,Nr)))\n",
    "        y_all = torch.transpose((y_all/torch.max(torch.max(torch.abs(y_all),dim=1,keepdim=True)[0],dim=2,keepdim=True)[0]).repeat(batch_size,1,1),-1,-2)\n",
    "\n",
    "    if pytorch:\n",
    "        x_all = x_all.float()\n",
    "        y_all = y_all.float()\n",
    "    else:\n",
    "        x_all = x_all.float().numpy()\n",
    "        y_all = y_all.float().numpy()\n",
    "\n",
    "    return x_all, y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "args = pglm.arg_parser(jupyter=True)\n",
    "\n",
    "##### Modify default argments if needed #####\n",
    "args['base_dir']        = '~/Research/SensoryMotorPred_Data/Testing'\n",
    "args['fig_dir']         = '~/Research/SensoryMotorPred_Data/FigTesting'\n",
    "args['data_dir']        = '~/Goeppert/nlab-nas/Dylan/freely_moving_ephys/ephys_recordings/'\n",
    "args['date_ani']        = '011523/TestAni'\n",
    "args['stim_cond']       = 'Control'\n",
    "args['Nepochs']         = 50\n",
    "args['NoL1']            = True\n",
    "args['NoL2']            = False\n",
    "args['model_dt']        = 0\n",
    "\n",
    "params, exp = load_BaseModel_params(args=args,exp_dir_name='CustomData',ModelID=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "##### Generating data #####\n",
    "x_all,y_all = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=10,pytorch=True)\n",
    "x_all, y_all = x_all.squeeze(),y_all.squeeze()\n",
    "y_all = (y_all+1)/2\n",
    "x_all = (x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)\n",
    "\n",
    "##### Train/Test Splits ####\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "frac = 0.1\n",
    "nT = x_all.shape[0]\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "train_idx, test_idx = next(iter(gss.split(np.arange(x_all.shape[0]), groups=groups)))\n",
    "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
    "xtr,xte = x_all[train_idx], x_all[test_idx]\n",
    "xtr_pos,xte_pos = torch.zeros_like(xtr).float(),torch.zeros_like(xte).float()\n",
    "ytr,yte = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "print('X:',xtr.shape,'Xpos:',xtr_pos.shape,'y:',ytr.shape)\n",
    "print('X:',xte.shape,'Xpos:',xte_pos.shape,'y:',yte.shape)\n",
    "params['nk'] = xtr.shape[-1]\n",
    "params['Ncells'] = ytr.shape[-1]\n",
    "meanbias = torch.mean(y_all,dim=0)\n",
    "\n",
    "xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias=xtr.to(device), xte.to(device), xtr_pos.to(device), xte_pos.to(device), ytr.to(device), yte.to(device), meanbias.to(device)\n",
    "datasets = {\n",
    "            'xtr':xtr,\n",
    "            'xte':xte,\n",
    "            'xtr_pos':xtr_pos,\n",
    "            'xte_pos':xte_pos,\n",
    "            'ytr':ytr,\n",
    "            'yte':yte,\n",
    "            'meanbias':meanbias,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['initW'] = 'normal' #'zero' # 'normal'\n",
    "params['optimizer'] = 'sgd'\n",
    "network_config,initial_params = pglm.make_network_config(params,single_trial=0,custom=True)\n",
    "network_config['lr_w'] = .001\n",
    "network_config['lr_b'] = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tloss_trace,vloss_trace,model,optimizer = train_network(network_config,**datasets, params=params,filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make prediction #####\n",
    "yhat = model(xte.to(device),xte_pos.to(device)).detach().cpu().numpy().squeeze()\n",
    "yt = yte.cpu().detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(10,4))\n",
    "ax = axs[0]\n",
    "cmap = pglm.discrete_cmap(vloss_trace.shape[-1],'jet')\n",
    "for cell in range(vloss_trace.shape[-1]):\n",
    "    ax.plot(vloss_trace[:,cell],c=cmap(cell))\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('loss')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=vloss_trace.shape[-1]))\n",
    "cbar = fig.colorbar(sm,ax=ax,format=None,shrink=0.7,pad=0.01)\n",
    "cbar.outline.set_linewidth(1)\n",
    "cbar.set_label('output dim')\n",
    "cbar.ax.tick_params(labelsize=12, width=1,direction='in')\n",
    "\n",
    "ncell = 0\n",
    "ax = axs[1]\n",
    "ax.plot(yt[:,ncell],c='k',label='actual')\n",
    "ax.plot(yhat[:,ncell],c='r',label='predicted')\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "ax.set_title('cc={:.03}'.format(np.corrcoef(yhat[:,ncell],yt[:,ncell])[1,0]))\n",
    "ax.legend(fontsize=10,ncol=1,labelcolor='linecolor', markerscale=0, handlelength=0, handletextpad=0,loc=\"upper left\",frameon=False, bbox_to_anchor=(.8, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "x_all2,y_all2 = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=50,pytorch=False)\n",
    "x_all2, y_all2 = x_all2.squeeze(),y_all2.squeeze()\n",
    "y_all2 = (y_all2+1)/2\n",
    "x_all2 = x_all2/np.max(np.abs(x_all2))\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "frac = 0.1\n",
    "nT = x_all2.shape[0]\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "train_idx, test_idx = next(iter(gss.split(np.arange(x_all2.shape[0]), groups=groups)))\n",
    "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
    "xtr2,xte2 = x_all2[train_idx], x_all2[test_idx]\n",
    "xtr_pos2,xte_pos2 = np.zeros_like(xtr2),np.zeros_like(xte2)\n",
    "ytr2,yte2 = y_all2[train_idx], y_all2[test_idx]\n",
    "\n",
    "\n",
    "l1 = LinearRegression()\n",
    "l1.fit(xtr2,ytr2)\n",
    "yhat2 = l1.predict(xte2)\n",
    "print('cc=',np.corrcoef(yhat2[:,ncell],yte2[:,ncell])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1,figsize=(4,4))\n",
    "ax = axs\n",
    "ax.plot(yt[:,ncell],c='k',label='actual')\n",
    "ax.plot(yhat2[:,ncell],c='r',label='predicted')\n",
    "ax.legend(fontsize=10,ncol=1,labelcolor='linecolor', markerscale=0, handlelength=0, handletextpad=0,loc=\"upper left\",frameon=False, bbox_to_anchor=(.8, 1))\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "ax.set_title('cc={:.03}'.format(np.corrcoef(yhat2[:,ncell],yt[:,ncell])[1,0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune Training: Parallel Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details about Ray Tune see: https://docs.ray.io/en/latest/tune/key-concepts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "args = pglm.arg_parser(jupyter=True)\n",
    "\n",
    "##### Modify default argments if needed #####\n",
    "args['base_dir']        = '~/Research/SensoryMotorPred_Data/Testing'\n",
    "args['fig_dir']         = '~/Research/SensoryMotorPred_Data/FigTesting'\n",
    "args['data_dir']        = '~/Goeppert/nlab-nas/Dylan/freely_moving_ephys/ephys_recordings/'\n",
    "args['date_ani']        = '011523/TestAni'\n",
    "args['stim_cond']       = 'Control'\n",
    "args['Nepochs']         = 50\n",
    "args['NoL1']            = True\n",
    "args['NoL2']            = True\n",
    "args['model_dt']        = 0\n",
    "\n",
    "params, exp = load_BaseModel_params(args=args,exp_dir_name='CustomData',ModelID=0)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "##### Generating data #####\n",
    "x_all,y_all = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=10,pytorch=True)\n",
    "x_all, y_all = x_all.squeeze(),y_all.squeeze()\n",
    "y_all = (y_all+1)/2\n",
    "x_all = (x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)\n",
    "\n",
    "##### Train/Test Splits ####\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "frac = 0.1\n",
    "nT = x_all.shape[0]\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "train_idx, test_idx = next(iter(gss.split(np.arange(x_all.shape[0]), groups=groups)))\n",
    "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
    "xtr,xte = x_all[train_idx], x_all[test_idx]\n",
    "xtr_pos,xte_pos = torch.zeros_like(xtr).float(),torch.zeros_like(xte).float()\n",
    "ytr,yte = y_all[train_idx], y_all[test_idx]\n",
    "\n",
    "print('X:',xtr.shape,'Xpos:',xtr_pos.shape,'y:',ytr.shape)\n",
    "print('X:',xte.shape,'Xpos:',xte_pos.shape,'y:',yte.shape)\n",
    "\n",
    "params['nk'] = xtr.shape[-1]\n",
    "params['Ncells'] = ytr.shape[-1]\n",
    "meanbias = torch.mean(y_all,dim=0)\n",
    "xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias=xtr.to(device), xte.to(device), xtr_pos.to(device), xte_pos.to(device), ytr.to(device), yte.to(device), meanbias.to(device)\n",
    "datasets = {\n",
    "            'xtr':xtr,\n",
    "            'xte':xte,\n",
    "            'xtr_pos':xtr_pos,\n",
    "            'xte_pos':xte_pos,\n",
    "            'ytr':ytr,\n",
    "            'yte':yte,\n",
    "            'meanbias':meanbias,\n",
    "        }\n",
    "\n",
    "params['initW'] = 'normal' #'zero' # 'normal'\n",
    "params['optimizer'] = 'sgd'\n",
    "network_config, initial_params = pglm.make_network_config(params,custom=True)\n",
    "network_config['lr_w'] = tune.loguniform(1e-4, 1e-2)\n",
    "network_config['lr_b'] = tune.loguniform(1e-2, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`network_config` contains the key parameters for the network and this is where we can choose which hyperparameters to optimize. In this example, we will optimize the learning rate of the weights and bias of the network to get the best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = [\n",
    "    {\"lr_w\": 0.001,\"lr_b\": 0.1, },\n",
    "]\n",
    "algo = HyperOptSearch(points_to_evaluate=initial_params)\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True,include_dashboard=True)\n",
    "\n",
    "sync_config = tune.SyncConfig()  # the default mode is to use use rsync\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train_network,**datasets, params=params),\n",
    "        resources={\"cpu\": 2, \"gpu\": .5}),\n",
    "    tune_config=tune.TuneConfig(metric=\"avg_loss\",mode=\"min\",search_alg=algo,num_samples=num_samples),\n",
    "    param_space=network_config,\n",
    "    run_config=air.RunConfig(local_dir=params['save_model'], name=\"NetworkAnalysis\",sync_config=sync_config,verbose=2)\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result(\"avg_loss\", \"min\")\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"avg_loss\"]))\n",
    "##### Get experiment dataframe and best network\n",
    "df = results.get_dataframe()\n",
    "best_network = list(params['save_model'].rglob('*{}.pt'.format(best_result.metrics['trial_id'])))[0]\n",
    "\n",
    "##### Save experiment data and save best network as h5 #####\n",
    "exp_filename = '_'.join([params['model_type'],params['data_name']]) + 'experiment_data.h5'\n",
    "exp_best_dict = {'best_network':best_network,'trial_id':best_result.metrics['trial_id'],'best_config':best_result.config}\n",
    "pglm.h5store(params['save_model'] / ('NetworkAnalysis/{}'.format(exp_filename)), df, **exp_best_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(best_network,network_config,params,xte,xte_pos,yte,device='cpu'):\n",
    "    \"\"\"Evaluates ray tune experiment and hyperparameter search\n",
    "\n",
    "    Args:\n",
    "        best_network (str): path to best network model '.pt' file\n",
    "        network_config (dict): network_config for best network\n",
    "        params (dict): key parameters dictionary\n",
    "        xte (Tensor): test input data \n",
    "        xte_pos (Tensor): test additional input data\n",
    "        yte (Tensor): target test data\n",
    "        device (str, optional): device to load data onto. Defaults to 'cpu'.\n",
    "    \"\"\"\n",
    "    import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
    "    from pytorchGLM.main.models import model_wrapper,BaseModel\n",
    "\n",
    "    ##### Load best network from saved ray experiment ######\n",
    "    state_dict, _ = torch.load(best_network,map_location='cpu')\n",
    "    model = model_wrapper((network_config,BaseModel))\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    ##### Load data into device and predict test set ######\n",
    "    xte, xte_pos, yte = xte.to(device), xte_pos.to(device), yte.to(device)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(xte,xte_pos)\n",
    "\n",
    "    ##### Smooth Firing rates and save ######\n",
    "    actual_smooth = yte.detach().cpu().numpy()\n",
    "    pred_smooth = yhat.detach().cpu().numpy()\n",
    "    cc_test = np.array([(np.corrcoef(pred_smooth[:,celln],actual_smooth[:,celln])[0, 1]) for celln in range(pred_smooth.shape[1])])\n",
    "    \n",
    "    GLM_Dict = {\n",
    "        'actual_smooth': actual_smooth,\n",
    "        'pred_smooth': pred_smooth,\n",
    "        'cc_test': cc_test,\n",
    "        }\n",
    "\n",
    "    for key in state_dict.keys():\n",
    "        GLM_Dict[key]  = state_dict[key].cpu().numpy()\n",
    "\n",
    "    model_name = '{}_ModelID{:d}_dt{:03d}_T{:02d}_NB{}_Best.h5'.format(params['model_type'], params['ModelID'],int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'])\n",
    "    ioh5.save(params['save_model']/model_name,GLM_Dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_eval(best_network,network_config,params,xte,xte_pos,yte,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_filename = list((params['save_model']/'NetworkAnalysis').rglob('*experiment_data.h5'))[0]\n",
    "df,meta_data = pglm.h5load(exp_filename)\n",
    "best_network=meta_data['best_network']\n",
    "best_config = meta_data['best_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
    "model_name = '{}_ModelID{:d}_dt{:03d}_T{:02d}_NB{}_Best.h5'.format(params['model_type'], params['ModelID'],int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'])\n",
    "GLM_Data = ioh5.load(params['save_model']/model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_Data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGLM_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeb3b4f9986e9903770e85426a5ea2a9c74e9cc9c9c2883aec8e061002a2af84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
