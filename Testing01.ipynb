{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ray\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import warnings \n",
    "import argparse\n",
    "import yaml\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import xarray as xr\n",
    "\n",
    "from ray import tune\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from typing import Tuple\n",
    "from asyncio import Event\n",
    "from test_tube import Experiment\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import binned_statistic\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from kornia.geometry.transform import Affine\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import Utils.io_dict_to_hdf5 as ioh5\n",
    "from Utils.utils import *\n",
    "from Utils.params import *\n",
    "from Utils.format_raw_data import *\n",
    "from Utils.format_model_data import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- model_dt:     (float) size of time bins in seconds\n",
    "- date_ani:     (str) date and animal ID\n",
    "- base_dir:     (str) base directory\n",
    "- save_dir:     (str) directory where processed data is going to be saved\n",
    "- data_dir:     (str) directory where raw data is held\n",
    "- downsamp_vid: (int) factor videos are downsampled by\n",
    "- lag_list:     (list) which timesteps to include in fits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input arguments\n",
    "args = arg_parser(jupyter=True)\n",
    "\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['date_ani']        = dates_all[0]\n",
    "args['free_move']       = True\n",
    "args['train_shifter']   = True\n",
    "args['NoL1']            = False\n",
    "args['NoL2']            = False\n",
    "args['do_shuffle']      = False\n",
    "args['Nepochs']         = 10000\n",
    "\n",
    "\n",
    "ModelID = 1\n",
    "params, exp = load_params(args,ModelID,exp_dir_name=None,nKfold=0,debug=False)\n",
    "\n",
    "file_dict = {'cell': 0,\n",
    "            'drop_slow_frames': False,\n",
    "            'ephys': list(params['data_dir'].glob('*ephys_merge.json'))[0].as_posix(),\n",
    "            'ephys_bin': list(params['data_dir'].glob('*Ephys.bin'))[0].as_posix(),\n",
    "            'eye': list(params['data_dir'].glob('*REYE.nc'))[0].as_posix(),\n",
    "            'imu': list(params['data_dir'].glob('*imu.nc'))[0].as_posix() if params['stim_cond'] == params['fm_dir'] else None,\n",
    "            'mapping_json': Path('~/Research/Github/FreelyMovingEphys/config/channel_maps.json').expanduser(),\n",
    "            'mp4': True,\n",
    "            'name': params['date_ani2'] + '_control_Rig2_' + params['stim_cond'],  # 070921_J553RT\n",
    "            'probe_name': 'DB_P128-6',\n",
    "            'save': params['data_dir'].as_posix(),\n",
    "            'speed': list(params['data_dir'].glob('*speed.nc'))[0].as_posix() if params['stim_cond'] == 'hf1_wn' else None,\n",
    "            'stim_cond': 'light',\n",
    "            'top': list(params['data_dir'].glob('*TOP1.nc'))[0].as_posix() if params['stim_cond'] == params['fm_dir'] else None,\n",
    "            'world': list(params['data_dir'].glob('*world.nc'))[0].as_posix(), \n",
    "            'ephys_csv': list(params['data_dir'].glob('*Ephys_BonsaiBoardTS.csv'))[0].as_posix()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_raw_data(file_dict, params, medfiltbins=11, **kwargs):\n",
    "#     \"\"\" Formatting raw data for Niell Lab freely moving ephys data \n",
    "\n",
    "#     Args:\n",
    "#         file_dict (dict): file dictionary containing raw data paths.\n",
    "#         params (dict): parameter dictionary holding key parameters for formatting.\n",
    "#         medfiltbins (int, optional): filter bin size for smoothing. Defaults to 11.\n",
    "\n",
    "#     Returns:\n",
    "#         raw_data (dict): returns formatted dictionary of raw data\n",
    "#         goodcells (pd.DataFrame): returns a DataFrame with ephys unit information\n",
    "#     \"\"\"\n",
    "    \n",
    "#     ##### Set up condition shorthand #####\n",
    "#     if file_dict['imu'] is not None:\n",
    "#         has_imu = True\n",
    "#         has_mouse = False\n",
    "#     else:\n",
    "#         has_imu = False\n",
    "#         has_mouse = True\n",
    "        \n",
    "#     # open worldcam\n",
    "#     print('opening worldcam data')\n",
    "#     world_data = xr.open_dataset(file_dict['world'])\n",
    "#     world_vid_raw = np.uint8(world_data['WORLD_video'])\n",
    "#     # resize worldcam\n",
    "#     sz = world_vid_raw.shape # raw video size\n",
    "#     # if size is larger than the target 60x80, resize by 0.5\n",
    "#     if sz[1]>160:\n",
    "#         downsamp = 0.5\n",
    "#         world_vid = np.zeros((sz[0],int(sz[1]*downsamp),int(sz[2]*downsamp)), dtype = 'uint8')\n",
    "#         for f in range(sz[0]):\n",
    "#             world_vid[f,:,:] = cv2.resize(world_vid_raw[f,:,:],(int(sz[2]*downsamp),int(sz[1]*downsamp)))\n",
    "#     else:\n",
    "#         # if the worldcam has already been resized when the nc file was written in preprocessing, don't resize\n",
    "#         world_vid = world_vid_raw.copy()\n",
    "#     del world_vid_raw\n",
    "#     gc.collect()\n",
    "\n",
    "#     # world timestamps\n",
    "#     worldT = world_data.timestamps.copy()\n",
    "#     if params['free_move'] == True:\n",
    "#         print('opening top data')\n",
    "#     #     # open the topdown camera nc file\n",
    "#         top_data = xr.open_dataset(file_dict['top'])\n",
    "#         top_speed = top_data.TOP1_props[:,0].data\n",
    "#         topT = top_data.timestamps.data.copy() # read in time timestamps\n",
    "#     #     top_vid = np.uint8(top_data['TOP1_video']) # read in top video\n",
    "#         # clear from memory\n",
    "#         del top_data\n",
    "#         gc.collect()\n",
    "#     else: \n",
    "#         topT = []\n",
    "#         top_speed = []\n",
    "        \n",
    "#     # load IMU data\n",
    "#     if has_imu:\n",
    "#         print('opening imu data')\n",
    "#         imu_data = xr.open_dataset(file_dict['imu'])\n",
    "#         try:\n",
    "#             accT = imu_data.IMU_data.sample # imu timestamps\n",
    "#             acc_chans = imu_data.IMU_data # imu dample data\n",
    "#         except AttributeError:\n",
    "#             accT = imu_data.__xarray_dataarray_variable__.sample\n",
    "#             acc_chans = imu_data.__xarray_dataarray_variable__\n",
    "        \n",
    "#         # raw gyro values\n",
    "#         gx = np.array(acc_chans.sel(channel='gyro_x_raw'))\n",
    "#         gy = np.array(acc_chans.sel(channel='gyro_y_raw'))\n",
    "#         gz = np.array(acc_chans.sel(channel='gyro_z_raw'))\n",
    "#         gz = (gz-np.mean(gz))*7.5 # Rescale gz\n",
    "#         # gyro values in degrees\n",
    "#         gx_deg = np.array(acc_chans.sel(channel='gyro_x'))\n",
    "#         gy_deg = np.array(acc_chans.sel(channel='gyro_y'))\n",
    "#         gz_deg = np.array(acc_chans.sel(channel='gyro_z'))\n",
    "#         # pitch and roll in deg\n",
    "#         groll = medfilt(np.array(acc_chans.sel(channel='roll')),medfiltbins)\n",
    "#         gpitch = medfilt(np.array(acc_chans.sel(channel='pitch')),medfiltbins)\n",
    "#     else: \n",
    "#         accT = []\n",
    "#         gz = []\n",
    "#         groll = []\n",
    "#         gpitch = []\n",
    "#     # load optical mouse nc file from running ball\n",
    "#     if file_dict['speed'] is not None:\n",
    "#         print('opening speed data')\n",
    "#         speed_data = xr.open_dataset(file_dict['speed'])\n",
    "#         try:\n",
    "#             spdVals = speed_data.BALL_data\n",
    "#         except AttributeError:\n",
    "#             spdVals = speed_data.__xarray_dataarray_variable__\n",
    "#         try:\n",
    "#             spd = spdVals.sel(move_params = 'speed_cmpersec')\n",
    "#             spd_tstamps = spdVals.sel(move_params = 'timestamps')\n",
    "#         except:\n",
    "#             spd = spdVals.sel(frame = 'speed_cmpersec')\n",
    "#             spd_tstamps = spdVals.sel(frame = 'timestamps')\n",
    "#     print('opening ephys data')\n",
    "#     # ephys data for this individual recording\n",
    "#     ephys_data = pd.read_json(file_dict['ephys'])\n",
    "#     # sort units by shank and site order\n",
    "#     ephys_data = ephys_data.sort_values(by='ch', axis=0, ascending=True)\n",
    "#     ephys_data = ephys_data.reset_index()\n",
    "#     ephys_data = ephys_data.drop('index', axis=1)\n",
    "#     # spike times\n",
    "#     ephys_data['spikeTraw'] = ephys_data['spikeT']\n",
    "#     print('getting good cells')\n",
    "#     # select good cells from phy2\n",
    "#     goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "#     units = goodcells.index.values\n",
    "#     # get number of good units\n",
    "#     n_units = len(goodcells)\n",
    "#     # plot spike raster\n",
    "#     plt.close()\n",
    "#     print('opening eyecam data')\n",
    "#     # load eye data\n",
    "#     eye_data = xr.open_dataset(file_dict['eye'])\n",
    "#     eye_vid = np.uint8(eye_data['REYE_video'])\n",
    "#     eyeT = eye_data.timestamps.copy()\n",
    "#     # plot eye postion across recording\n",
    "#     eye_params = eye_data['REYE_ellipse_params']\n",
    "#     # define theta, phi and zero-center\n",
    "#     th = np.array((eye_params.sel(ellipse_params = 'theta'))*180/np.pi)\n",
    "#     phi = np.array((eye_params.sel(ellipse_params = 'phi'))*180/np.pi)\n",
    "#     eyerad = eye_data.REYE_ellipse_params.sel(ellipse_params = 'longaxis').data\n",
    "\n",
    "#     print('adjusting camera times to match ephys')\n",
    "#     # adjust eye/world/top times relative to ephys\n",
    "#     ephysT0 = ephys_data.iloc[0,12]\n",
    "#     eyeT = eye_data.timestamps  - ephysT0\n",
    "#     if eyeT[0]<-600:\n",
    "#         eyeT = eyeT + 8*60*60 # 8hr offset for some data\n",
    "#     worldT = world_data.timestamps - ephysT0\n",
    "#     if worldT[0]<-600:\n",
    "#         worldT = worldT + 8*60*60\n",
    "#     if params['free_move'] is True and has_imu is True:\n",
    "#         accTraw = accT - ephysT0\n",
    "#     if params['free_move'] is False and has_mouse is True:\n",
    "#         speedT = spd_tstamps - ephysT0\n",
    "#     if params['free_move'] is True:\n",
    "#         topT = topT - ephysT0\n",
    "\n",
    "#     ##### Clear some memory #####\n",
    "#     del eye_data \n",
    "#     gc.collect()\n",
    "\n",
    "#     if file_dict['drop_slow_frames'] is True:\n",
    "#         # in the case that the recording has long time lags, drop data in a window +/- 3 frames around these slow frames\n",
    "#         isfast = np.diff(eyeT)<=0.05\n",
    "#         isslow = sorted(list(set(chain.from_iterable([list(range(int(i)-3,int(i)+4)) for i in np.where(isfast==False)[0]]))))\n",
    "#         th[isslow] = np.nan\n",
    "#         phi[isslow] = np.nan\n",
    "\n",
    "\n",
    "#     print(world_vid.shape)\n",
    "#     # calculate eye veloctiy\n",
    "#     dEye = np.diff(th)\n",
    "#     accT_correction_file = params['save_dir']/'acct_correction_{}.h5'.format(params['data_name'])\n",
    "#     # check accelerometer / eye temporal alignment\n",
    "#     if (accT_correction_file.exists()):# & (reprocess==False):\n",
    "#         accT_correction = ioh5.load(accT_correction_file)\n",
    "#         offset0    = accT_correction['offset0']\n",
    "#         drift_rate = accT_correction['drift_rate']\n",
    "#         accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "#         found_good_offset = True\n",
    "#     else:\n",
    "#         if (has_imu):\n",
    "#             print('checking accelerometer / eye temporal alignment')\n",
    "#             lag_range = np.arange(-0.2,0.2,0.002)\n",
    "#             cc = np.zeros(np.shape(lag_range))\n",
    "#             t1 = np.arange(5,len(dEye)/60-120,20).astype(int) # was np.arange(5,1600,20), changed for shorter videos\n",
    "#             t2 = t1 + 60\n",
    "#             offset = np.zeros(np.shape(t1))\n",
    "#             ccmax = np.zeros(np.shape(t1))\n",
    "#             acc_interp = interp1d(accTraw, (gz-3)*7.5)\n",
    "#             for tstart in tqdm(range(len(t1))):\n",
    "#                 for l in range(len(lag_range)):\n",
    "#                     try:\n",
    "#                         c, lag= nanxcorr(-dEye[t1[tstart]*60 : t2[tstart]*60] , acc_interp(eyeT[t1[tstart]*60:t2[tstart]*60]+lag_range[l]),1)\n",
    "#                         cc[l] = c[1]\n",
    "#                     except: # occasional problem with operands that cannot be broadcast togther because of different shapes\n",
    "#                         cc[l] = np.nan\n",
    "#                 offset[tstart] = lag_range[np.argmax(cc)]    \n",
    "#                 ccmax[tstart] = np.max(cc)\n",
    "#             offset[ccmax<0.1] = np.nan\n",
    "#             del ccmax, dEye\n",
    "#             gc.collect()\n",
    "#             if np.isnan(offset).all():\n",
    "#                 found_good_offset = False\n",
    "#             else:\n",
    "#                 found_good_offset = True\n",
    "\n",
    "#         if has_imu and found_good_offset is True:\n",
    "#             print('fitting regression to timing drift')\n",
    "#             # fit regression to timing drift\n",
    "#             model = LinearRegression()\n",
    "#             dataT = np.array(eyeT[t1*60 + 30*60])\n",
    "#             model.fit(dataT[~np.isnan(offset)].reshape(-1,1),offset[~np.isnan(offset)]) \n",
    "#             offset0 = model.intercept_\n",
    "#             drift_rate = model.coef_\n",
    "#             del dataT\n",
    "#             gc.collect()\n",
    "#         elif file_dict['speed'] is not None or found_good_offset is False:\n",
    "#             offset0 = 0.1\n",
    "#             drift_rate = -0.000114\n",
    "#         if has_imu:\n",
    "#             accT_correction = {'offset0': offset0, 'drift_rate': drift_rate}\n",
    "#             ioh5.save(accT_correction_file,accT_correction)\n",
    "#             accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "#             del accTraw\n",
    "#             gc.collect()\n",
    "\n",
    "\n",
    "#     print('correcting ephys spike times for offset and timing drift')\n",
    "#     for i in ephys_data.index:\n",
    "#         ephys_data.at[i,'spikeT'] = np.array(ephys_data.at[i,'spikeTraw']) - (offset0 + np.array(ephys_data.at[i,'spikeTraw']) *drift_rate)\n",
    "#     goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "\n",
    "\n",
    "#     ##### Calculating image norm #####\n",
    "#     print('Calculating Image Norm')\n",
    "#     start = time.time()\n",
    "#     sz = np.shape(world_vid)\n",
    "#     world_vid_sm = np.zeros((sz[0],int(sz[1]/params['downsamp_vid']),int(sz[2]/params['downsamp_vid'])),dtype=np.uint8)\n",
    "#     for f in range(sz[0]):\n",
    "#         world_vid_sm[f,:,:] = cv2.resize(world_vid[f,:,:],(int(sz[2]/params['downsamp_vid']),int(sz[1]/params['downsamp_vid'])))\n",
    "\n",
    "#     del world_vid\n",
    "#     gc.collect()\n",
    "\n",
    "#     raw_data = {\n",
    "#                 'eye':{\n",
    "#                     'th':     th,\n",
    "#                     'phi':    phi,\n",
    "#                     'eyerad': eyerad,\n",
    "#                     'eyeTS':  eyeT,\n",
    "#                 },\n",
    "#                 'acc': {\n",
    "#                     'gz':     gz,\n",
    "#                     'roll':  groll,\n",
    "#                     'pitch': gpitch,    \n",
    "#                     'accTS':  accT,\n",
    "#                 },\n",
    "#                 'top':{\n",
    "#                     'speed': top_speed,\n",
    "#                     'topTS': topT,\n",
    "#                 },\n",
    "#                 'vid':{\n",
    "#                     'vidTS':    worldT,\n",
    "#                     'vid_sm':   world_vid_sm,\n",
    "#                 },\n",
    "#                 }\n",
    "\n",
    "#     return raw_data, goodcells\n",
    "\n",
    "\n",
    "# def interp_raw_data(raw_data, align_t, model_dt=0.05, goodcells=None):\n",
    "#     \"\"\"Interpolates raw data based on nested dictionary. \n",
    "\n",
    "#     Args:\n",
    "#         raw_data (dict): nested dictionary where first level (raw_data[key0]) represents data type\n",
    "#                          second level contains data and timestamps assuming the following format respectively: \n",
    "#                          raw_data[key0]['datatype'] or raw_data[key0]['datatypeTS'] \n",
    "#                          where datatype is the variabel name of the data.\n",
    "#         align_t (np.array): timestamps which to align data to.\n",
    "#         model_dt (float, optional): model bin size to align data. Defaults to 0.05.\n",
    "#         goodcells (pd.DataFrame, optional): If processing ephys data input DataFrame with size (units x features) with a column named \n",
    "#                                             spikeT. spikeT containsa list spike times (row). Defaults to None.\n",
    "\n",
    "#     Returns:\n",
    "#         model_data (dict): dictionary containing interpolated time-aligned model data with naming convention 'model_[datatype]'\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     ##### Set up model interpolated time #####\n",
    "#     model_t = np.arange(0,np.max(align_t), model_dt)\n",
    "\n",
    "#     ##### Interpolate raw data #####\n",
    "#     model_data = {}\n",
    "#     for key0 in raw_data.keys():\n",
    "#             for key1 in raw_data[key0].keys():\n",
    "#                 if 'TS' not in key1:\n",
    "#                     if 'vid' in key0: # Z score video then interpolate\n",
    "#                         std_im = np.std(raw_data[key0][key1], axis=0, dtype=float)\n",
    "#                         img_norm = ((raw_data[key0][key1]-np.mean(raw_data[key0][key1],axis=0,dtype=float))/std_im).astype(float)\n",
    "#                         std_im[std_im<20] = 0 # zero out extreme values\n",
    "#                         img_norm = (img_norm * (std_im>0)).astype(float)\n",
    "#                         interp = interp1d(raw_data[key0][key0+'TS'], img_norm,'nearest', axis=0,bounds_error = False) \n",
    "#                         testimg = interp(model_t[0])\n",
    "#                         model_vid_sm = np.zeros((len(model_t),int(np.shape(testimg)[0]),int(np.shape(testimg)[1])),dtype=float)\n",
    "#                         for i in tqdm(range(len(model_t))):\n",
    "#                             model_vid = interp(model_t[i] + model_dt/2)\n",
    "#                             model_vid_sm[i,:] = model_vid\n",
    "#                         model_vid_sm[np.isnan(model_vid_sm)]=0\n",
    "#                         model_data['model_'+ key1] = model_vid_sm\n",
    "#                     else:\n",
    "#                         interp = interp1d(raw_data[key0][key0+'TS'],raw_data[key0][key1],axis=0, bounds_error=False)\n",
    "#                         model_data['model_'+ key1] = interp(model_t+model_dt/2)\n",
    "#     model_data['model_t'] = model_t\n",
    "#     if 'acc' in raw_data.keys():\n",
    "#         model_data['model_active'] = np.convolve(np.abs(model_data['model_gz']), np.ones(int(1/model_dt)), 'same') / len(np.ones(int(1/model_dt)))\n",
    "\n",
    "#     # get spikes / rate\n",
    "#     if goodcells is not None:\n",
    "#         n_units = len(goodcells)\n",
    "#         model_nsp = np.zeros((len(model_t),n_units))\n",
    "#         bins = np.append(model_t,model_t[-1]+model_dt)\n",
    "#         for i,ind in enumerate(goodcells.index):\n",
    "#             model_nsp[:,i],bins = np.histogram(goodcells.at[ind,'spikeT'],bins)\n",
    "#         model_data['model_nsp'] = model_nsp\n",
    "#         model_data['unit_nums'] = goodcells.index.values\n",
    "\n",
    "#     return model_data\n",
    "\n",
    "\n",
    "# def load_aligned_data(file_dict, params, reprocess=False):\n",
    "#     \"\"\" Load time aligned data from file or process raw data and return formatted data\n",
    "\n",
    "#     Args:\n",
    "#         file_dict (dict): file dictionary containing raw data paths.\n",
    "#         params (dict): parameter dictionary holding key parameters for formatting.\n",
    "#         reprocess (bool, optional): reprocess raw data. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         model_data (dict): returns dictionary with time aligned model data\n",
    "#     \"\"\"\n",
    "\n",
    "#     model_file = params['save_dir'] / 'ModelData_{}_dt{:03d}_rawWorldCam_{:d}ds.h5'.format(params['data_name'],int(params['model_dt']*1000),int(params['downsamp_vid']))\n",
    "#     if (model_file.exists()) & (reprocess==False):\n",
    "#         model_data = ioh5.load(model_file)\n",
    "#     else:\n",
    "#         raw_data, goodcells = format_raw_data(file_dict,params)\n",
    "#         model_data = interp_raw_data(raw_data,raw_data['vid']['vidTS'],model_dt=params['model_dt'],goodcells=goodcells)\n",
    "#         if params['free_move']:\n",
    "#             ##### Saving average and std of parameters for centering and scoring across conditions #####\n",
    "#             FM_move_avg = np.zeros((2,6))\n",
    "#             FM_move_avg[:,0] = np.array([np.nanmean(model_data['model_th']),np.nanstd(model_data['model_th'])])\n",
    "#             FM_move_avg[:,1] = np.array([np.nanmean(model_data['model_phi']),np.nanstd(model_data['model_phi'])])\n",
    "#             FM_move_avg[:,2] = np.array([np.nanmean(model_data['model_roll']),np.nanstd(model_data['model_roll'])])\n",
    "#             FM_move_avg[:,3] = np.array([np.nanmean(model_data['model_pitch']),np.nanstd(model_data['model_pitch'])])\n",
    "#             FM_move_avg[:,4] = np.array([np.nanmean(model_data['model_speed']),np.nanmax(model_data['model_speed'])])\n",
    "#             FM_move_avg[:,5] = np.array([np.nanmean(model_data['model_eyerad']),np.nanmax(model_data['model_eyerad'])])\n",
    "#             np.save(params['save_dir_fm']/'FM_MovAvg_{}_dt{:03d}.npy'.format(params['data_name'],int(params['model_dt']*1000)),FM_move_avg)\n",
    "#         ephys_file = params['save_dir'] / 'RawEphysData_{}.h5'.format(params['data_name'])\n",
    "#         goodcells.to_hdf(ephys_file,key='goodcells', mode='w')\n",
    "#         ioh5.save(model_file, model_data)\n",
    "#     return model_data\n",
    "\n",
    "\n",
    "\n",
    "# def load_aligned_data(file_dict, params, reprocess=False):\n",
    "#     \"\"\" Load time aligned data from file or process raw data and return formatted data\n",
    "\n",
    "#     Args:\n",
    "#         file_dict (dict): file dictionary containing raw data paths.\n",
    "#         params (dict): parameter dictionary holding key parameters for formatting.\n",
    "#         reprocess (bool, optional): reprocess raw data. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         model_data (dict): returns dictionary with time aligned model data\n",
    "#     \"\"\"\n",
    "\n",
    "#     model_file = params['save_dir'] / 'ModelData_{}_dt{:03d}_rawWorldCam_{:d}ds.h5'.format(params['data_name'],int(params['model_dt']*1000),int(params['downsamp_vid']))\n",
    "#     if (model_file.exists()) & (reprocess==False):\n",
    "#         model_data = ioh5.load(model_file)\n",
    "#     else:\n",
    "#         raw_data, goodcells = format_raw_data(file_dict,params)\n",
    "#         model_data = interp_raw_data(raw_data,raw_data['vid']['vidTS'],model_dt=params['model_dt'],goodcells=goodcells)\n",
    "#         if params['free_move']:\n",
    "#             ##### Saving average and std of parameters for centering and scoring across conditions #####\n",
    "#             FM_move_avg = np.zeros((2,6))\n",
    "#             FM_move_avg[:,0] = np.array([np.nanmean(model_data['model_th']),np.nanstd(model_data['model_th'])])\n",
    "#             FM_move_avg[:,1] = np.array([np.nanmean(model_data['model_phi']),np.nanstd(model_data['model_phi'])])\n",
    "#             FM_move_avg[:,2] = np.array([np.nanmean(model_data['model_roll']),np.nanstd(model_data['model_roll'])])\n",
    "#             FM_move_avg[:,3] = np.array([np.nanmean(model_data['model_pitch']),np.nanstd(model_data['model_pitch'])])\n",
    "#             FM_move_avg[:,4] = np.array([np.nanmean(model_data['model_speed']),np.nanmax(model_data['model_speed'])])\n",
    "#             FM_move_avg[:,5] = np.array([np.nanmean(model_data['model_eyerad']),np.nanmax(model_data['model_eyerad'])])\n",
    "#             np.save(params['save_dir_fm']/'FM_MovAvg_{}_dt{:03d}.npy'.format(params['data_name'],int(params['model_dt']*1000)),FM_move_avg)\n",
    "#         ephys_file = params['save_dir'] / 'RawEphysData_{}.h5'.format(params['data_name'])\n",
    "#         goodcells.to_hdf(ephys_file,key='goodcells', mode='w')\n",
    "#         ioh5.save(model_file, model_data)\n",
    "#     return model_data\n",
    "\n",
    "\n",
    "\n",
    "# def format_data(data, params, frac=.1, shifter_train_size=.5, test_train_size=.75, do_shuffle=False, do_norm=False, NKfold=1, thresh_cells=False, cut_inactive=True, move_medwin=11,**kwargs):\n",
    "#     \"\"\" Fully format data for model training\n",
    "\n",
    "#     Args:\n",
    "#         data (dict): data dictionary containing time aligned data. get as ouput from load_aligned_data\n",
    "#         params (dict): parameter dictionary holding key parameters for formatting.\n",
    "#         frac (float, optional): from of total length, size of groups for train/test split. Defaults to .1.\n",
    "#         shifter_train_size (float, optional): shifter train/test split fraction. Defaults to .5.\n",
    "#         test_train_size (float, optional): train/test size for random group shuffle. Defaults to .75.\n",
    "#         do_shuffle (bool, optional): shuffle spikes. Defaults to False.\n",
    "#         do_norm (bool, optional): normalize data. Defaults to False.\n",
    "#         NKfold (int, optional): How many Kfolds to make. Defaults to 1.\n",
    "#         thresh_cells (bool, optional): threshold out bad cells. Defaults to False.\n",
    "#         cut_inactive (bool, optional): cut inactive timepoints. Defaults to True.\n",
    "#         move_medwin (int, optional): window to smooth roll/pitch. Defaults to 11.\n",
    "\n",
    "#     Returns:\n",
    "#         _type_: _description_\n",
    "#     \"\"\"\n",
    "#     ##### Load in preprocessed data #####\n",
    "#     if params['free_move']:\n",
    "#         ##### Find 'good' timepoints when mouse is active #####\n",
    "#         nan_idxs = []\n",
    "#         for key in data.keys():\n",
    "#             nan_idxs.append(np.where(np.isnan(data[key]))[0])\n",
    "#         good_idxs = np.ones(len(data['model_active']),dtype=bool)\n",
    "#         good_idxs[data['model_active']<0.5] = False # .5 based on histogram, determined emperically \n",
    "#         good_idxs[np.unique(np.hstack(nan_idxs))] = False\n",
    "#     else:\n",
    "#         good_idxs = np.where((np.abs(data['model_th'])<50) & (np.abs(data['model_phi'])<50))[0].astype(int)\n",
    "\n",
    "\n",
    "#     data['raw_nsp'] = data['model_nsp'].copy()\n",
    "#     if cut_inactive:\n",
    "#         ##### return only active data #####\n",
    "#         for key in data.keys():\n",
    "#             if (key != 'model_nsp') & (key != 'model_active') & (key != 'unit_nums') & (key != 'model_vis_sm_shift'):\n",
    "#                 if len(data[key])>0:\n",
    "#                     data[key] = data[key][good_idxs] # interp_nans(data[key]).astype(float)\n",
    "#             elif (key == 'model_nsp'):\n",
    "#                 data[key] = data[key][good_idxs]\n",
    "#             elif (key == 'unit_nums') | (key == 'model_vis_sm_shift'):\n",
    "#                 pass\n",
    "    \n",
    "    \n",
    "#     ##### Splitting data for shifter, then split for model training #####\n",
    "#     if params['shifter_5050']==False:\n",
    "#         gss = GroupShuffleSplit(n_splits=NKfold, train_size=test_train_size, random_state=42)\n",
    "#         nT = data['model_nsp'].shape[0]\n",
    "#         groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "\n",
    "#         train_idx_list=[]\n",
    "#         test_idx_list = []\n",
    "#         for train_idx, test_idx in gss.split(np.arange(data['model_nsp'].shape[0]), groups=groups):\n",
    "#             train_idx_list.append(train_idx)\n",
    "#             test_idx_list.append(test_idx)\n",
    "#     else:\n",
    "#         ##### Need to fix 50/50 splitting ###################\n",
    "#         # gss = GroupShuffleSplit(n_splits=NKfold, train_size=shifter_train_size, random_state=42)\n",
    "#         np.random.seed(42)\n",
    "#         nT = data['model_nsp'].shape[0]\n",
    "        \n",
    "#         shifter_train_size = .5\n",
    "#         groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "#         train_idx_list_shifter=[]\n",
    "#         test_idx_list_shifter=[]\n",
    "#         train_idx_list=[]\n",
    "#         test_idx_list = []\n",
    "#         for Kfold in np.arange(NKfold):\n",
    "#             glist = np.arange(1,1/frac+1)\n",
    "#             shifter_groups = np.random.choice(glist,size=int((1/frac)*shifter_train_size),replace=False)\n",
    "#             idx = np.arange(nT)\n",
    "#             sampled_inds = np.any(np.array([groups==shifter_groups[n] for n in np.arange(len(shifter_groups))]),axis=0)\n",
    "#             train_idx_list_shifter.append(idx[sampled_inds])\n",
    "#             test_idx_list_shifter.append(idx[~sampled_inds])\n",
    "#             glist=glist[~np.any(np.array([glist==shifter_groups[n] for n in np.arange(len(shifter_groups))]),axis=0)]\n",
    "            \n",
    "#             if params['train_shifter']==False:\n",
    "#                 if params['shifter_5050_run']:\n",
    "#                     train_idx_list = test_idx_list_shifter\n",
    "#                     test_idx_list  = train_idx_list_shifter\n",
    "#                 else:\n",
    "#                     train_idx_list = train_idx_list_shifter\n",
    "#                     test_idx_list  = test_idx_list_shifter\n",
    "#             else:\n",
    "#                 if params['shifter_5050_run']:\n",
    "#                     train_idx_list = test_idx_list_shifter\n",
    "#                     test_idx_list  = train_idx_list_shifter\n",
    "#                 else:\n",
    "#                     train_idx_list = train_idx_list_shifter\n",
    "#                     test_idx_list  = test_idx_list_shifter\n",
    "\n",
    "#     if thresh_cells:\n",
    "#         print('Tot_units: {}'.format(data['unit_nums'].shape))\n",
    "#         if params['free_move']:\n",
    "#             if (params['save_dir_fm']/'bad_cells.npy').exists():\n",
    "#                 bad_cells = np.load(params['save_dir_fm']/'bad_cells.npy')\n",
    "#             else:\n",
    "#                 mean_thresh = np.nanmean(data['model_nsp']/params['model_dt'],axis=0) < 1 # Thresholding out units under 1 Hz\n",
    "#                 f25,l75=int((data['model_nsp'].shape[0])*.5),int((data['model_nsp'].shape[0])*.5) # Checking first 25% and last 25% firing rate for drift\n",
    "#                 scaled_fr = (np.nanmean(data['model_nsp'][:f25], axis=0)/np.nanstd(data['model_nsp'][:f25], axis=0) - np.nanmean(data['model_nsp'][l75:], axis=0)/np.nanstd(data['model_nsp'][l75:], axis=0))/model_dt\n",
    "#                 bad_cells = np.where((mean_thresh | (np.abs(scaled_fr)>4)))[0] # Locating bad units      \n",
    "#                 np.save(params['save_dir_fm']/'bad_cells.npy',bad_cells)\n",
    "#         else:\n",
    "#             bad_cells = np.load(params['save_dir_fm']/'bad_cells.npy')\n",
    "\n",
    "#         data['model_nsp'] = np.delete(data['model_nsp'],bad_cells,axis=1) # removing bad units\n",
    "#         data['unit_nums'] = np.delete(data['unit_nums'],bad_cells,axis=0) # removing bad units\n",
    "        \n",
    "#     data['model_dth'] = np.diff(data['model_th'],append=0)\n",
    "#     data['model_dphi'] = np.diff(data['model_phi'],append=0)\n",
    "#     FM_move_avg = np.load(params['save_dir_fm']/'FM_MovAvg_{}_dt{:03d}.npy'.format(params['data_name'],int(params['model_dt']*1000)))\n",
    "#     data['model_th'] = data['model_th'] - FM_move_avg[0,0]\n",
    "#     data['model_phi'] = (data['model_phi'] - FM_move_avg[0,1])\n",
    "#     if do_norm:\n",
    "#         data['model_vid_sm'] = (data['model_vid_sm'] - np.mean(data['model_vid_sm'],axis=0))/np.nanstd(data['model_vid_sm'],axis=0)\n",
    "#         data['model_vid_sm'][np.isnan(data['model_vid_sm'])]=0\n",
    "#         data['model_th'] = (data['model_th'])/FM_move_avg[1,0] # np.nanstd(data['model_th'],axis=0) \n",
    "#         data['model_phi'] = (data['model_phi'])/FM_move_avg[1,1] # np.nanstd(data['model_phi'],axis=0) \n",
    "#         if params['free_move']:\n",
    "#             data['model_roll'] = (data['model_roll'] - FM_move_avg[0,2])/FM_move_avg[1,2]\n",
    "#             data['model_pitch'] = (data['model_pitch'] - FM_move_avg[0,3])/FM_move_avg[1,3]\n",
    "#             data['model_roll'] = medfilt(data['model_roll'],move_medwin)\n",
    "#             data['model_pitch'] = medfilt(data['model_pitch'],move_medwin)\n",
    "#             if params['use_spdpup']:\n",
    "#                 data['model_speed'] = (data['model_speed']-FM_move_avg[0,4])/FM_move_avg[1,4]\n",
    "#                 data['model_eyerad'] = (data['model_eyerad']-FM_move_avg[0,5])/FM_move_avg[1,5]\n",
    "#         else:\n",
    "#             # data['model_roll'] = (0 - FM_move_avg[0,2])/FM_move_avg[1,2])\n",
    "#             data['model_pitch'] = (np.zeros(data['model_phi'].shape) - FM_move_avg[0,3])/FM_move_avg[1,3]\n",
    "#     else:\n",
    "#         if params['free_move']:\n",
    "#             data['model_roll']   = (data['model_roll'] - FM_move_avg[0,2])\n",
    "#             data['model_pitch']  = (data['model_pitch'] - FM_move_avg[0,3])\n",
    "#             data['model_roll']   = medfilt(data['model_roll'],move_medwin)\n",
    "#             data['model_pitch']  = medfilt(data['model_pitch'],move_medwin)\n",
    "#             if params['use_spdpup']:\n",
    "#                 data['model_speed']  = (data['model_speed'])\n",
    "#                 data['model_eyerad'] = (data['model_eyerad'])\n",
    "#         else:\n",
    "#             data['model_pitch']  = (np.zeros(data['model_phi'].shape) - FM_move_avg[0,3])\n",
    "#     return data,train_idx_list,test_idx_list\n",
    "\n",
    "\n",
    "# ##### Load in Kfold Data #####\n",
    "# def load_Kfold_data(data,params,train_idx,test_idx):\n",
    "#     \"\"\" Create Train/Test splits \n",
    "\n",
    "#     Args:\n",
    "#         data (dict): dictionary of formatted data\n",
    "#         params (dict): parameter dictionary holding key parameters for formatting.\n",
    "#         train_idx (array): training indecies\n",
    "#         test_idx (array): testing indecies\n",
    "\n",
    "#     Returns:\n",
    "#         data (dict): data dictionary with train/test splits\n",
    "#     \"\"\"\n",
    "#     data['train_vid'] = data['model_vid_sm'][train_idx]\n",
    "#     data['test_vid'] = data['model_vid_sm'][test_idx]\n",
    "#     data['train_nsp'] = shuffle(data['model_nsp'][train_idx],random_state=42) if params['do_shuffle'] else data['model_nsp'][train_idx]\n",
    "#     data['test_nsp'] = shuffle(data['model_nsp'][test_idx],random_state=42) if params['do_shuffle'] else data['model_nsp'][test_idx]\n",
    "#     data['train_th'] = data['model_th'][train_idx]\n",
    "#     data['test_th'] = data['model_th'][test_idx]\n",
    "#     data['train_phi'] = data['model_phi'][train_idx]\n",
    "#     data['test_phi'] = data['model_phi'][test_idx]\n",
    "#     data['train_roll'] = data['model_roll'][train_idx] if params['free_move'] else []\n",
    "#     data['test_roll'] = data['model_roll'][test_idx] if params['free_move'] else []\n",
    "#     data['train_pitch'] = data['model_pitch'][train_idx]\n",
    "#     data['test_pitch'] = data['model_pitch'][test_idx]\n",
    "#     data['train_t'] = data['model_t'][train_idx]\n",
    "#     data['test_t'] = data['model_t'][test_idx]\n",
    "#     data['train_dth'] = data['model_dth'][train_idx]\n",
    "#     data['test_dth'] = data['model_dth'][test_idx]\n",
    "#     data['train_dphi'] = data['model_dphi'][train_idx]\n",
    "#     data['test_dphi'] = data['model_dphi'][test_idx]\n",
    "#     data['train_gz'] = data['model_gz'][train_idx] if params['free_move'] else []\n",
    "#     data['test_gz'] = data['model_gz'][test_idx] if params['free_move'] else []\n",
    "#     data['train_speed'] = data['model_speed'][train_idx] if ((params['use_spdpup'])&params['free_move']) else []\n",
    "#     data['test_speed'] = data['model_speed'][test_idx] if ((params['use_spdpup'])&params['free_move']) else []\n",
    "#     data['train_eyerad'] = data['model_eyerad'][train_idx] if ((params['use_spdpup'])&params['free_move']) else []\n",
    "#     data['test_eyerad'] = data['model_eyerad'][test_idx] if ((params['use_spdpup'])&params['free_move']) else []\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data, goodcells = format_raw_data(file_dict,params)\n",
    "# for key0 in raw_data.keys():\n",
    "#     for key1 in raw_data[key0].keys():\n",
    "#         print(key0, key1, raw_data[key0][key1].shape,raw_data[key0][key1].dtype)\n",
    "# model_data = interp_raw_data(raw_data,raw_data['vid']['vidTS'],model_dt=0.05,goodcells=goodcells)\n",
    "# for key0 in model_data.keys():\n",
    "#     print(key0,model_data[key0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot_units: (128,)\n"
     ]
    }
   ],
   "source": [
    "data = load_aligned_data(file_dict, params, reprocess=False)\n",
    "data,train_idx_list,test_idx_list = format_data(data, params,do_norm=True,thresh_cells=True,cut_inactive=True)\n",
    "data = load_Kfold_data(data,params,train_idx_list[0],test_idx_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_modeltype(params,load_for_training=False):\n",
    "    \"\"\"Creates model name based on params configuation\n",
    "\n",
    "    Args:\n",
    "        params (dict): parameter dictionary holding key parameters\n",
    "        load_for_training (bool, optional): If loading for shifter. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        params or model_type: params with model_type key or model_type string\n",
    "    \"\"\"\n",
    "    if load_for_training==False:\n",
    "        if params['ModelID'] == 0:\n",
    "            model_type = 'Pytorch_Mot'\n",
    "        elif params['ModelID'] == 1:\n",
    "            model_type = 'Pytorch_Vis'\n",
    "        elif params['ModelID'] == 2:\n",
    "            model_type = 'Pytorch_Add'\n",
    "        elif params['ModelID'] == 3:\n",
    "            model_type = 'Pytorch_Mul'\n",
    "    else:\n",
    "        model_type = 'Pytorch_Vis'\n",
    "\n",
    "    if params['train_shifter']:\n",
    "        params['save_model_shift'] = params['save_model'].parent.parent / 'Shifter'\n",
    "        params['save_model_shift'].mkdir(parents=True, exist_ok=True)\n",
    "        params['NoL1'] = True\n",
    "        params['do_norm']=True\n",
    "        model_type = model_type + 'Shifter'\n",
    "        if params['shifter_5050']:\n",
    "            if params['shifter_5050_run']:\n",
    "                model_type = model_type + 'Train_1'\n",
    "            else: \n",
    "                model_type = model_type + 'Train_0'\n",
    "    else:\n",
    "        if params['shifter_5050']:\n",
    "            if params['shifter_5050_run']:\n",
    "                model_type = model_type + '1'\n",
    "            else: \n",
    "                model_type = model_type + '0'\n",
    "\n",
    "    if params['EyeHead_only']:\n",
    "        if params['EyeHead_only_run']==True:\n",
    "            model_type = model_type + '_EyeOnly'\n",
    "        else:\n",
    "            model_type = model_type + '_HeadOnly'\n",
    "    if params['NoShifter']:\n",
    "        model_type = model_type + 'NoShifter'\n",
    "\n",
    "    if params['only_spdpup']:\n",
    "        model_type = model_type + '_onlySpdPup'\n",
    "    elif params['use_spdpup']:\n",
    "        model_type = model_type + '_SpdPup'\n",
    "    if params['NoL1']:\n",
    "        model_type = model_type + '_NoL1'\n",
    "    if params['NoL2']:\n",
    "        model_type = model_type + '_NoL2'\n",
    "    if params['SimRF']:\n",
    "        model_type = model_type + '_SimRF'\n",
    "    if load_for_training==False:\n",
    "        params['model_type'] = model_type\n",
    "        return params\n",
    "    else: \n",
    "        return model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_pytorch_data(data,params,train_idx,test_idx, device='cuda'):\n",
    "    if params['free_move']:\n",
    "        if params['train_shifter']:\n",
    "            pos_train = np.hstack((data['train_th'][:, np.newaxis], data['train_phi'][:, np.newaxis],data['train_pitch'][:, np.newaxis]))\n",
    "            pos_test  = np.hstack((data['test_th'][:, np.newaxis], data['test_phi'][:, np.newaxis],data['test_pitch'][:, np.newaxis]))\n",
    "            model_pos = np.hstack((data['model_th'][:, np.newaxis], data['model_phi'][:, np.newaxis],data['model_pitch'][:, np.newaxis]))\n",
    "            params['shift_in'] = model_pos.shape[-1]\n",
    "            params['shift_out'] = model_pos.shape[-1]\n",
    "        else:\n",
    "            pos_train,pos_test,model_pos = [],[],[]\n",
    "            for key in params['position_vars']:\n",
    "                pos_train.append(data['train_'+key][:,np.newaxis])\n",
    "                pos_test.append(data['test_'+key][:,np.newaxis])\n",
    "                model_pos.append(data['model_'+key][:,np.newaxis])\n",
    "            pos_train = np.hstack(pos_train)\n",
    "            pos_test  = np.hstack(pos_test)\n",
    "            model_pos = np.hstack(model_pos)\n",
    "    else: \n",
    "        pos_train = np.hstack((data['train_th'][:, np.newaxis], data['train_phi'][:, np.newaxis], data['train_pitch'][:, np.newaxis], np.zeros(data['train_phi'].shape)[:, np.newaxis]))\n",
    "        pos_test  = np.hstack((data['test_th'][:, np.newaxis], data['test_phi'][:, np.newaxis], data['test_pitch'][:, np.newaxis], np.zeros(data['test_phi'].shape)[:, np.newaxis]))\n",
    "        model_pos = np.hstack((data['model_th'][:, np.newaxis], data['model_phi'][:, np.newaxis], data['model_pitch'][:, np.newaxis], np.zeros(data['model_phi'].shape)[:, np.newaxis]))\n",
    "\n",
    "    ##### Save dimensions #####\n",
    "    params['nks'] = np.shape(data['train_vid'])[1:]\n",
    "    params['nk'] = params['nks'][0]*params['nks'][1]*params['nt_glm_lag']\n",
    "    if params['train_shifter']:\n",
    "        rolled_vid = np.hstack([np.roll(data['model_vid_sm'], nframes, axis=0) for nframes in params['lag_list']])\n",
    "        move_quantiles = np.quantile(model_pos,params['quantiles'],axis=0)\n",
    "        train_range = np.all(((pos_train>move_quantiles[0]) & (pos_train<move_quantiles[1])),axis=1)\n",
    "        test_range = np.all(((pos_test>move_quantiles[0]) & (pos_test<move_quantiles[1])),axis=1)\n",
    "        x_train = rolled_vid[train_idx].reshape((len(train_idx), params['nt_glm_lag'])+params['nks']).astype(np.float32)[train_range]\n",
    "        x_test = rolled_vid[test_idx].reshape((len(test_idx), params['nt_glm_lag'])+params['nks']).astype(np.float32)[test_range]\n",
    "        pos_train = pos_train[train_range]\n",
    "        pos_test = pos_test[test_range]\n",
    "        ytr = torch.from_numpy(data['train_nsp'][train_range].astype(np.float32))\n",
    "        yte = torch.from_numpy(data['test_nsp'][test_range].astype(np.float32))\n",
    "    elif params['NoShifter']:\n",
    "        if params['crop_input'] != 0:\n",
    "            model_vid_sm = data['model_vid_sm'][:,params['crop_input']:-params['crop_input'],params['crop_input']:-params['crop_input']]\n",
    "        rolled_vid = np.hstack([np.roll(model_vid_sm, nframes, axis=0) for nframes in params['lag_list']])\n",
    "        x_train = rolled_vid[train_idx].reshape(len(train_idx), -1).astype(np.float32)\n",
    "        x_test = rolled_vid[test_idx].reshape(len(test_idx), -1).astype(np.float32)\n",
    "        ytr = torch.from_numpy(data['train_nsp'].astype(np.float32))\n",
    "        yte = torch.from_numpy(data['test_nsp'].astype(np.float32))\n",
    "        params['nks'] = np.shape(model_vid_sm)[1:]\n",
    "        params['nk'] = params['nks'][0]*params['nks'][1]*params['nt_glm_lag']\n",
    "    else: \n",
    "        ##### Rework for new model format ######\n",
    "        model_vid_sm_shift = ioh5.load(params['save_dir']/params['exp_name']/'ModelWC_shifted_dt{:03d}_ModelID{:d}.h5'.format(int(params['model_dt']*1000), 1))['model_vid_sm_shift']  # [:,5:-5,5:-5]\n",
    "        if params['crop_input'] != 0:\n",
    "            model_vid_sm_shift = model_vid_sm_shift[:,params['crop_input']:-params['crop_input'],params['crop_input']:-params['crop_input']]\n",
    "        params['nks'] = np.shape(model_vid_sm_shift)[1:]\n",
    "        params['nk'] = params['nks'][0]*params['nks'][1]*params['nt_glm_lag']\n",
    "        rolled_vid = np.hstack([np.roll(model_vid_sm_shift, nframes, axis=0) for nframes in params['lag_list']]) \n",
    "        x_train = rolled_vid[train_idx].reshape(len(train_idx), -1).astype(np.float32)\n",
    "        x_test = rolled_vid[test_idx].reshape(len(test_idx), -1).astype(np.float32)\n",
    "\n",
    "        ytr = torch.from_numpy(data['train_nsp'].astype(np.float32))\n",
    "        yte = torch.from_numpy(data['test_nsp'].astype(np.float32))\n",
    "\n",
    "    ##### Convert to Tensors #####\n",
    "    if params['ModelID']==0:\n",
    "        xtr = torch.from_numpy(pos_train.astype(np.float32))\n",
    "        xte = torch.from_numpy(pos_test.astype(np.float32))\n",
    "    else:\n",
    "        xtr = torch.from_numpy(x_train.astype(np.float32))\n",
    "        xte = torch.from_numpy(x_test.astype(np.float32))\n",
    "    xtr_pos = torch.from_numpy(pos_train.astype(np.float32))\n",
    "    xte_pos = torch.from_numpy(pos_test.astype(np.float32))\n",
    "    params['pos_features'] = xtr_pos.shape[-1]\n",
    "    params['Ncells'] = ytr.shape[-1]\n",
    "    \n",
    "    if params['SimRF']:\n",
    "        SimRF_file = params['save_dir'].parent.parent.parent/'021522/SimRF/fm1/SimRF_withL1_dt050_T01_Model1_NB10000_Kfold00_best.h5'\n",
    "        SimRF_data = ioh5.load(SimRF_file)\n",
    "        ytr = torch.from_numpy(SimRF_data['ytr'].astype(np.float32)).to(device)\n",
    "        yte = torch.from_numpy(SimRF_data['yte'].astype(np.float32)).to(device)\n",
    "        params['save_model'] = params['save_model'] / 'SimRF'\n",
    "        params['save_model'].mkdir(parents=True, exist_ok=True)\n",
    "        meanbias = torch.from_numpy(SimRF_data['bias_sim'].astype(np.float32)).to(device)\n",
    "    else:\n",
    "        meanbias = torch.mean(torch.tensor(data['model_nsp'], dtype=torch.float32), axis=0)\n",
    "    return xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FreeMovingEphysDataset(Dataset):\n",
    "    def __init__(self, input0, input1, target):\n",
    "        \"\"\"Pytorch dataset class for 2 inputs and 1 output\n",
    "\n",
    "        Args:\n",
    "            input0 (Tensor): tensor for model input\n",
    "            input1 (Tensor): tensor for model input\n",
    "            target (Tensor): Target data\n",
    "        \"\"\"\n",
    "        self.input0 = input0\n",
    "        self.input1 = input1\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.input0[idx]\n",
    "        Y = self.target[idx]\n",
    "        X2 = self.input1[idx]\n",
    "        return X, X2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['ModelID']=1\n",
    "params['position_vars'] = ['th','phi','pitch','roll']#,'speed','eyerad']\n",
    "params['train_shifter']=True\n",
    "train_idx = train_idx_list[0]\n",
    "test_idx = test_idx_list[0]\n",
    "\n",
    "xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias = format_pytorch_data(data,params,train_idx,test_idx)\n",
    "train_dataset = FreeMovingEphysDataset(xtr,xtr_pos,ytr)\n",
    "test_dataset  = FreeMovingEphysDataset(xte,xte_pos,yte)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=xtr.shape[0],num_workers=2,pin_memory=True,)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=xte.shape[0],num_workers=2,pin_memory=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,pos,Y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18972, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr_pos.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                    in_features, \n",
    "                    N_cells,\n",
    "                    config,\n",
    "                    device='cuda'):\n",
    "        super(BaseModel, self).__init__()\n",
    "        r''' Base GLM Network\n",
    "        Args: \n",
    "            in_feature: size of the input dimension\n",
    "            N_cells: the number of cells to fit\n",
    "            config: network configuration file with hyperparameters\n",
    "            device: which device to run network on\n",
    "        '''\n",
    "        self.config = config\n",
    "        self.in_features = in_features\n",
    "        self.N_cells = N_cells\n",
    "        \n",
    "        self.Cell_NN = nn.Sequential(nn.Linear(self.in_features, self.N_cells,bias=True))\n",
    "        self.activations = nn.ModuleDict({'SoftPlus':nn.Softplus(),\n",
    "                                          'ReLU': nn.ReLU(),})\n",
    "        torch.nn.init.uniform_(self.Cell_NN[0].weight, a=-1e-6, b=1e-6)\n",
    "        \n",
    "        # Initialize Regularization parameters\n",
    "        self.L1_alpha = config['L1_alpha']\n",
    "        if self.L1_alpha != None:\n",
    "            self.alpha = config['L1_alpha']*torch.ones(1).to(device)\n",
    "\n",
    "      \n",
    "    def init_weights(self,m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.uniform_(m.weight,a=-1e-6,b=1e-6)\n",
    "            m.bias.data.fill_(1e-6)\n",
    "        \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x, y = inputs.shape\n",
    "        output = self.Cell_NN(inputs)\n",
    "        ret = self.activations['ReLU'](output)\n",
    "\n",
    "    def loss(self,Yhat, Y): \n",
    "        loss_vec = torch.mean((Yhat-Y)**2,axis=0)\n",
    "        if self.L1_alpha != None:\n",
    "            l1_reg0 = torch.stack([torch.linalg.vector_norm(NN_params,ord=1) for name, NN_params in self.Cell_NN.named_parameters() if '0.weight' in name])\n",
    "            loss_vec = loss_vec + self.alpha*(l1_reg0)\n",
    "        return loss_vec\n",
    "\n",
    "\n",
    "class ShifterNetwork(BaseModel):\n",
    "    def __init__(self, \n",
    "                    in_features, \n",
    "                    N_cells, \n",
    "                    config,\n",
    "                    device='cuda'):\n",
    "        super(ShifterNetwork, self).__init__(in_features, N_cells, config)\n",
    "        r''' Shifter GLM Network\n",
    "        Args: \n",
    "            in_feature: size of the input dimension\n",
    "            N_cells: the number of cells to fit\n",
    "            shift_in: size of the input to shifter network. \n",
    "            shift_hidden: size of the hidden layer in the shifter network\n",
    "            shift_out: output dimension of shifter network\n",
    "            L1_alpha: L1 regularization value for visual network\n",
    "            train_shifter: Bool for whether training shifter network\n",
    "            meanbias: can set bias to mean firing rate of each neurons\n",
    "            device: which device to run network on\n",
    "        \n",
    "        '''\n",
    "        self.config = config\n",
    "        ##### shifter network initialization #####\n",
    "        self.shift_in = config['shift_in']\n",
    "        self.shift_hidden = config['shift_hidden']\n",
    "        self.shift_out = config['shift_out']\n",
    "        self.shifter_nn = nn.Sequential(\n",
    "                                        nn.Linear(config['shift_in'],config['shift_hidden']),\n",
    "                                        nn.Softplus(),\n",
    "                                        nn.Linear(config['shift_hidden'], config['shift_out'])\n",
    "                                    )\n",
    "\n",
    "\n",
    "    def forward(self, inputs, shifter_input=None, **kwargs):\n",
    "        ##### Forward Pass of Shifter #####\n",
    "        batchsize, timesize, x, y = inputs.shape\n",
    "        dxy = self.shifter_nn(shifter_input)\n",
    "        shift = Affine(angle=torch.clamp(dxy[:,-1],min=-45,max=45),translation=torch.clamp(dxy[:,:2],min=-15,max=15))\n",
    "        inputs = shift(inputs)\n",
    "        inputs = inputs.reshape(batchsize,-1).contiguous()\n",
    "        ##### fowrad pass of GLM #####\n",
    "        x, y = inputs.shape\n",
    "        if y != self.in_features:\n",
    "            print(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')\n",
    "            return 0\n",
    "        output = self.Cell_NN(inputs)\n",
    "        ret = self.activations['ReLU'](output)\n",
    "        return ret\n",
    "\n",
    "\n",
    "\n",
    "class MixedNetwork(BaseModel):\n",
    "    def __init__(self, \n",
    "                in_features, \n",
    "                N_cells, \n",
    "                config,\n",
    "                device='cuda'):\n",
    "        super(MixedNetwork, self).__init__(in_features, N_cells, config)\n",
    "        r''' Mixed GLM Network\n",
    "        Args: \n",
    "            in_feature: size of the input dimension\n",
    "            N_cells: the number of cells to fit\n",
    "            L1_alpha: L1 regularization value for visual network\n",
    "            L1_alpham: L1 regularization value for position network \n",
    "            move_feature: the number of position features \n",
    "            LinMix: Additive or Multiplicative mixing. LinMix=True for additive, LinMix=False for multiplicative\n",
    "            device: which device to run network on\n",
    "        \n",
    "        '''\n",
    "        self.config = config\n",
    "        self.LinMix = config['LinMix']\n",
    "        ##### Position Network Initialization #####\n",
    "        if config['L1_alpham'] != None:\n",
    "            self.alpha_m = config['L1_alpham']*torch.ones(1).to(device)\n",
    "        self.posNN = nn.Sequential(nn.Linear(config['pos_features'], N_cells))\n",
    "        torch.nn.init.uniform_(self.posNN[0].weight,a=-1e-6,b=1e-6) \n",
    "        if self.LinMix==False:\n",
    "            torch.nn.init.ones_(self.posNN[0].bias) # Bias = 1 for mult bias=0 for add\n",
    "        else:    \n",
    "            torch.nn.init.zeros_(self.posNN[0].bias) # Bias = 1 for mult bias=0 for add\n",
    "\n",
    "    def forward(self, inputs, pos_inputs, **kwargs):\n",
    "        x, y = inputs.shape\n",
    "        if y != self.in_features:\n",
    "            print(f'Wrong Input Features. Please use tensor with {self.in_features} Input Features')\n",
    "            return 0\n",
    "        output = self.Cell_NN(inputs)\n",
    "        if self.LinMix==True:\n",
    "            output = output + self.posNN(pos_inputs)\n",
    "        else:\n",
    "            move_out = torch.abs(self.posNN(pos_inputs))\n",
    "            output = output*move_out\n",
    "        ret = self.activations['ReLU'](output)\n",
    "\n",
    "    def loss(self,Yhat, Y): \n",
    "        loss_vec = torch.mean((Yhat-Y)**2,axis=0)\n",
    "        if self.L1_alpha != None:\n",
    "            l1_reg0 = self.alpha*(torch.linalg.norm(self.Cell_NN[0].weight,axis=1,ord=1))\n",
    "        else: \n",
    "            l1_reg0 = 0\n",
    "            l1_reg1 = 0\n",
    "        if self.L1_alpham != None:\n",
    "            l1_regm = self.alpha_m*(torch.linalg.norm(self.posNN[0].weight,axis=1,ord=1))\n",
    "        else: \n",
    "            l1_regm = 0\n",
    "        loss_vec = loss_vec + l1_reg0 + l1_reg1 + l1_regm\n",
    "        return loss_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network_config(params,single_trial=False):\n",
    "    \"\"\" Create Network Config dictionary for hyperparameter search\n",
    "\n",
    "    Args:\n",
    "        params (dict): key parameters dictionary\n",
    "        device (string): cuda\n",
    "\n",
    "    Returns:\n",
    "        network_config (dict): dictionary with hyperparameters\n",
    "    \"\"\"\n",
    "    network_config = {}\n",
    "    network_config['in_features']   = params['nk']\n",
    "    network_config['Ncells']        = params['Ncells']\n",
    "    network_config['shift_in']      = params['shift_in']\n",
    "    network_config['shift_hidden']  = params['shift_hidden']\n",
    "    network_config['shift_out']     = params['shift_out']\n",
    "    network_config['LinMix']        = params['LinMix']\n",
    "    network_config['pos_features']  = params['pos_features']\n",
    "    network_config['lr_shift']      = [1e-2]\n",
    "    network_config['lr_w']          = [1e-3]\n",
    "    network_config['lr_b']          = [1e-3]\n",
    "    network_config['lr_m']          = [1e-3]\n",
    "    if params['NoL1']:\n",
    "        network_config['L1_alpha']  = None\n",
    "        network_config['L1_alpham'] = None\n",
    "    else:\n",
    "        network_config['L1_alpha']  = .0001\n",
    "        network_config['L1_alpham'] = None\n",
    "\n",
    "    if params['NoL2']:\n",
    "        network_config['L2_lambda']   = 0\n",
    "        network_config['L2_lambda_m'] = 0\n",
    "    else:\n",
    "        if single_trial:\n",
    "            network_config['L2_lambda_m'] = np.logspace(-2, 3, 20)[1]\n",
    "            network_config['L2_lambda']   = np.logspace(-2, 3, 20)[1]\n",
    "        else:\n",
    "            network_config['L2_lambda']   = tune.grid_search(np.logspace(-2, 3, 20))\n",
    "            network_config['L2_lambda_m'] = tune.grid_search(np.logspace(-2, 3, 20))\n",
    "    return network_config\n",
    "\n",
    "def model_wrapper(ARGS,**kwargs):\n",
    "    \"\"\"Model Wrapper\n",
    "\n",
    "    Args:\n",
    "        ARGS (tuple): tuple containing the config dictionary and model class\n",
    "\n",
    "    Returns:\n",
    "        model : returns instantiated model of input class. \n",
    "    \"\"\"\n",
    "    config = ARGS[0]\n",
    "    Model = ARGS[1]\n",
    "    model = Model(config['in_features'],config['Ncells'],config,device=config['device'],**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config = make_network_config(params,single_trial=True,device=device)\n",
    "model = model_wrapper((network_config,ShifterNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShifterNetwork(\n",
       "  (Cell_NN): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=108, bias=True)\n",
       "  )\n",
       "  (activations): ModuleDict(\n",
       "    (SoftPlus): Softplus(beta=1, threshold=20)\n",
       "    (ReLU): ReLU()\n",
       "  )\n",
       "  (shifter_nn): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=20, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "    (2): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = next(iter(train_dataloader))\n",
    "vid,pos,y = minibatch\n",
    "vid,pos,y = vid.to(device),pos.to(device),y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(vid,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(list(params['save_dir'].glob('GLM_Pytorch_BestShift*'))[0])\n",
    "filename = list(params['save_dir'].glob('GLM_Pytorch_BestShift*'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model,params,filename,meanbias=None):\n",
    "    \"\"\" Load model parameters\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): _description_\n",
    "        params (dict): _description_\n",
    "        filename (str): _description_\n",
    "        meanbias (Tensor, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    state_dict = model.state_dict()\n",
    "    for key in state_dict.keys():\n",
    "        if ('posNN' not in key) & ('shifter_nn' not in key):\n",
    "            if 'weight' in key:\n",
    "                state_dict[key] = checkpoint['model_state_dict'][key].repeat(1,params['nt_glm_lag'])\n",
    "            else:\n",
    "                state_dict[key] = checkpoint['model_state_dict'][key]\n",
    "    if (params['SimRF']==True):\n",
    "        SimRF_file = params['save_dir'].parent.parent.parent/'121521/SimRF/fm1/SimRF_withmodel_dt050_T01_Modemodel_NB10000_Kfold00_best.h5'\n",
    "        SimRF_data = ioh5.load(SimRF_file)\n",
    "        model.Cell_NN[0].weight.data = torch.from_numpy(SimRF_data['sta'].astype(np.float32).T)\n",
    "        model.Cell_NN[0].bias.data = torch.from_numpy(SimRF_data['bias_sim'].astype(np.float32))\n",
    "    if meanbias is not None:\n",
    "        state_dict = model.state_dict()\n",
    "        state_dict['Cell_NN.0.bias']=meanbias\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def setup_model_training(model,params,network_config):\n",
    "    \"\"\"Set up optimizer and scheduler for training\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Network model to train\n",
    "        params (dict): key parameters \n",
    "        network_config (diot): dictionary of hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        optimizer: pytorch optimizer\n",
    "        scheduler: learning rate scheduler\n",
    "    \"\"\"\n",
    "    param_list = []\n",
    "    if params['train_shifter']:\n",
    "        param_list.append({'params': list(model.shifter_nn.parameters()),'lr': network_config['lr_shift'],'weight_decay':.0001})\n",
    "    for name,p in model.named_parameters():\n",
    "        if params['ModelID']<2:\n",
    "            if ('Cell_NN' in name):\n",
    "                if ('weight' in name):\n",
    "                    param_list.append({'params':[p],'lr':network_config['lr_w'],'weight_decay':network_config['L2_lambda']})\n",
    "                elif ('bias' in name):\n",
    "                    param_list.append({'params':[p],'lr':network_config['lr_b']})\n",
    "        elif params['ModelID']>1:\n",
    "            if ('posNN' in name):\n",
    "                if ('weight' in name):\n",
    "                    param_list.append({'params':[p],'lr':network_config['lr_w'],'weight_decay':network_config['L2_lambda_m']})\n",
    "                elif ('bias' in name):\n",
    "                    param_list.append({'params':[p],'lr':network_config['lr_b']})\n",
    "\n",
    "    optimizer = optim.Adam(params=param_list)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(params['Nepochs']/5))\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell_NN.0.weight torch.Size([108, 1200])\n",
      "Cell_NN.0.bias torch.Size([108])\n",
      "shifter_nn.0.weight torch.Size([20, 3])\n",
      "shifter_nn.0.bias torch.Size([20])\n",
      "shifter_nn.2.weight torch.Size([3, 20])\n",
      "shifter_nn.2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "network_config = make_network_config(params,single_trial=True)\n",
    "model = model_wrapper((network_config,ShifterNetwork))\n",
    "model = load_model(model,params,filename,meanbias=meanbias)\n",
    "optimizer, scheduler = setup_model_training(model,params,network_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(params, checkpoint_dir=None, data_dir=None):\n",
    "    network_config = make_network_config(params,single_trial=True)\n",
    "    model = model_wrapper((network_config,ShifterNetwork))\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer, scheduler = setup_model_training(model,params,network_config)\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model = load_model(model,params,filename,meanbias=meanbias)\n",
    "\n",
    "    xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias = format_pytorch_data(data,params,train_idx,test_idx)\n",
    "    train_dataset = FreeMovingEphysDataset(xtr,xtr_pos,ytr)\n",
    "    test_dataset  = FreeMovingEphysDataset(xte,xte_pos,yte)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=xtr.shape[0],num_workers=2,pin_memory=True,)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=xte.shape[0],num_workers=2,pin_memory=True,)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            vid,pos,y = data\n",
    "            vid,pos,y = vid.to(device),pos.to(device),y.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = model.loss(outputs, y)\n",
    "            loss.backward(torch.ones_like(y))\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = model.loss(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "vloss_trace = np.zeros((Nepochs, ytr.shape[-1]), dtype=np.float32)\n",
    "tloss_trace = np.zeros((Nepochs, ytr.shape[-1]), dtype=np.float32)\n",
    "Epoch_GLM = {}\n",
    "if track_all:\n",
    "    for name, p in l1.named_parameters():\n",
    "        Epoch_GLM[name] = np.zeros((Nepochs,) + p.shape, dtype=np.float32)\n",
    "\n",
    "if pbar is None:\n",
    "    pbar = pbar2 = tqdm(np.arange(Nepochs))\n",
    "else:\n",
    "    pbar2 = np.arange(Nepochs)\n",
    "for batchn in pbar2:\n",
    "    out = l1(xtr, xtrm, shift_in_tr)\n",
    "    loss = l1.loss(out, ytr)\n",
    "    pred = l1(xte, xtem, shift_in_te)\n",
    "    val_loss = l1.loss(pred, yte)\n",
    "    vloss_trace[batchn] = val_loss.clone().cpu().detach().numpy()\n",
    "    tloss_trace[batchn] = loss.clone().cpu().detach().numpy()\n",
    "    pbar.set_description('Loss: {:.03f}'.format(np.nanmean(val_loss.clone().cpu().detach().numpy())))\n",
    "    pbar.refresh()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(torch.ones_like(loss))\n",
    "    optimizer.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    if track_all:\n",
    "        for name, p in l1.named_parameters():\n",
    "            Epoch_GLM[name][batchn] = p.clone().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18972, 108])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid,pos,Y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3496b118a4d22d6a6f5485f441160c3e17db178642e3548badf1988c7bf383fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
