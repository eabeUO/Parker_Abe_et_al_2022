{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ray\n",
    "import gc\n",
    "import cv2\n",
    "import time\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.signal import medfilt\n",
    "from pathlib import Path\n",
    "from scipy.stats import binned_statistic\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from kornia.geometry.transform import Affine\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from Utils.utils import *\n",
    "import Utils.io_dict_to_hdf5 as ioh5\n",
    "from Utils.format_data import *\n",
    "from Utils.plotting import *\n",
    "from main.models import *\n",
    "from main.fit_GLM import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load/Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pparams={\n",
    "    'mod_titles':   ['Mot','Vis','Add','Mul','HF','VisNoShifter'],\n",
    "    'mod_clrs':     [\"#f1c40f\",\"#06d6a0\",\"#118ab2\",\"#ef476f\",\"#073b4c\"],\n",
    "    'dates_all':    ['070921/J553RT','101521/J559NC','102821/J570LT','110421/J569LT'],\n",
    "}\n",
    "pparams['date_ani2'] = [pparams['dates_all'][n].replace('/','_') for n in range(len(pparams['dates_all']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find Shuffle data files #####\n",
    "params['do_shuffle']=True\n",
    "params['complex']     = False\n",
    "params['nt_glm_lag']=1\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "Shuff_List = []\n",
    "for date_ani in dates_all:\n",
    "    date_ani_dir = params['save_dir'].parent.parent.parent/date_ani\n",
    "    params = get_modeltype(params)\n",
    "    model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "    mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_shuffled_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "    Shuff_List.append(sorted([path for path in date_ani_dir.rglob(mod_name+'.h5') if ((('CropInputs' in path.parts) | ('complex' in path.parts)) &('VisNoShifter' not in path.stem) & ('SimRF' not in path.stem))]))\n",
    "Shuff_List = np.stack(Shuff_List)\n",
    "# Shuff_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load data and find all files #####\n",
    "Kfold = 0\n",
    "args['free_move']=True\n",
    "ModelList_all,test_std,tuning_sig_all,tuning_sig_all2,NCells_all,bad_cells_all,nsp_raw_all,hf_nsp_all,model_move_FM,model_move_HF,ax_lims_all,move_test_all,test_nsp_all = [], [], [],[], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for d,date_ani in enumerate(pparams['dates_all']):\n",
    "    args['free_move']   = True\n",
    "    args['date_ani']    = date_ani\n",
    "    args['NoL1']        = False\n",
    "    args['NoL2']        = False\n",
    "    args['reg_lap']     = False\n",
    "    args['do_shuffle']  = False\n",
    "    args['use_spdpup']  = False\n",
    "    args['only_spdpup'] = False\n",
    "    args['complex']     = False\n",
    "    args['crop_input']  = True\n",
    "    args['Nepochs']     = 10000\n",
    "    params,_,_ = load_params(1,Kfold,args,debug=True)\n",
    "    params['nt_glm_lag']=1\n",
    "    date_ani_dir = params['save_dir'].parent.parent.parent/date_ani\n",
    "    params = get_modeltype(params)\n",
    "    model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "    mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "    # mod_name = '*{}_dt{:03d}_T*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['Nepochs'],Kfold)\n",
    "    ModelList = np.array(sorted([path for path in date_ani_dir.rglob(mod_name+'.h5') if  (('CropInputs' in path.parts) |('EyeHead_only' in path.parts) | ('complex' in path.parts) | ('complex_onoff' in path.parts)| ('OnlySpdPupil' in path.parts))  & ('Laplace' not in path.stem) & ('VisShifter' not in path.stem)])) # (params['exp_name'] in path.as_posix()) &\n",
    "    ModelList_all.append(ModelList)\n",
    "    bad_cells = np.load(params['save_dir_hf']/'bad_cells.npy')\n",
    "    bad_cells_all.append(bad_cells)\n",
    "    params['use_spdpup']=True\n",
    "    data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "    model_move_FM.append(model_move)\n",
    "    move_test_all.append(move_test)\n",
    "    ax_lims_all.append(ax_ylims)\n",
    "    nsp_raw_all.append(nsp_raw)\n",
    "    test_nsp_all.append(data['test_nsp'])\n",
    "    test_std.append(np.var(data['test_nsp'],axis=0))\n",
    "    tuning_sig_all.append(tuning_sig)\n",
    "    tuning_sig_all2.append(tuning_sig2)\n",
    "    NCells_all.append(len(tuning_sig2))\n",
    "    args['free_move'] = False\n",
    "    params,_,_ = load_params(0,Kfold,args,debug=True)\n",
    "    data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "    hf_nsp_all.append(data['test_nsp'])\n",
    "    model_move_HF.append(model_move)\n",
    "\n",
    "ModelList_all = np.stack(ModelList_all)\n",
    "test_std=np.hstack(test_std)\n",
    "ModelList_all = np.concatenate((ModelList_all,Shuff_List),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Save all data into nested dictionary #####\n",
    "sf=4\n",
    "params['nt_glm_lag']=1\n",
    "date_ani2 = [pparams['dates_all'][n].replace('/','_') for n in range(len(pparams['dates_all']))]\n",
    "All_data = {date_ani2[n]:{'complex':{},'CropInputs':{},'complex_onoff':{},'EyeHead_only':{}, 'complex_SpdPup':{},'CropInputs_SpdPup':{},'OnlySpdPupil':{}, } for n in range(len(date_ani2))}\n",
    "for da, date_ani in enumerate(date_ani2):\n",
    "    All_data[date_ani]['model_move_FM'] = model_move_FM[da]\n",
    "    All_data[date_ani]['tuning_sig'] = tuning_sig_all[da]\n",
    "    All_data[date_ani]['model_move_FM'] = model_move_FM[da]\n",
    "    All_data[date_ani]['model_move_HF'] = model_move_HF[da]\n",
    "    All_data[date_ani]['move_test'] = move_test_all[da]\n",
    "    All_data[date_ani]['nsp_raw'] = nsp_raw_all[da]\n",
    "    All_data[date_ani]['ax_lims'] = ax_lims_all[da]\n",
    "    All_data[date_ani]['actual_meanfr'] = np.mean(test_nsp_all[da],axis=0)/params['model_dt']\n",
    "    All_data[date_ani]['actual_smooth'] = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=0, arr=test_nsp_all[da])[params['bin_length']:-params['bin_length']]\n",
    "    All_data[date_ani]['actual_HF_smooth'] = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=0, arr=hf_nsp_all[da])[params['bin_length']:-params['bin_length']]\n",
    "    for _,fmod in enumerate(ModelList_all[da]):\n",
    "        if 'hf1_wn' in fmod.parts:\n",
    "            ModelRun = 4\n",
    "        else: \n",
    "            mtype = fmod.name.split('_')[2]\n",
    "            ModelRun = [n for n,m in enumerate(pparams['mod_titles']) if m == mtype][0]\n",
    "        if ('complex_onoff' in fmod.parts):\n",
    "            exp_type = 'complex_onoff'\n",
    "        elif ('EyeHead_only' in fmod.parts):\n",
    "            exp_type = 'EyeHead_only'\n",
    "        elif ('complex' in fmod.parts):\n",
    "            exp_type = 'complex'\n",
    "        elif ('CropInputs' in fmod.parts):\n",
    "            exp_type = 'CropInputs'\n",
    "        elif ('OnlySpdPupil' in fmod.parts):\n",
    "            exp_type = 'OnlySpdPupil' \n",
    "        if 'SpdPup' == fmod.name.split('_')[3]:\n",
    "            exp_type = exp_type + '_SpdPup'\n",
    "\n",
    "        VisName = pparams['mod_titles'][1]\n",
    "        ModelName = pparams['mod_titles'][ModelRun]\n",
    "        if 'shuffled'==fmod.name.split('_')[-2]:\n",
    "            ModelName = ModelName + '_shuffled'\n",
    "            VisName = VisName + '_shuffled'\n",
    "\n",
    "        if ('EyeOnly'==fmod.name.split('_')[3]):\n",
    "            ModelName = ModelName + '_EyeOnly'\n",
    "        elif ('HeadOnly'==fmod.name.split('_')[3]):\n",
    "            ModelName = ModelName + '_HeadOnly'\n",
    "\n",
    "        GLM_Data = ioh5.load(fmod)\n",
    "        All_data[date_ani][exp_type][ModelName +'_pred_smooth'] = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=1, arr=GLM_Data['pred_cv'].copy())[:,params['bin_length']:-params['bin_length']].T\n",
    "        All_data[date_ani][exp_type][ModelName +'_vloss_all'] = GLM_Data['vloss_trace_all'].copy()\n",
    "        All_data[date_ani][exp_type][ModelName +'_vloss_trace'] = np.nanmean(GLM_Data['vloss_trace_all'][:,-10:],axis=1)/np.nanvar(test_nsp_all[da],axis=0)\n",
    "        if (ModelRun == 0):\n",
    "            All_data[date_ani][exp_type][ModelName+'_moveW'] = GLM_Data['Cell_NN.0.weight'].copy()\n",
    "        if (ModelRun == 1) | (ModelRun == 4) | (ModelRun == 5):\n",
    "            All_data[date_ani][exp_type][ModelName+'_loss_regcv'] = GLM_Data['loss_regcv'].copy()\n",
    "            if (exp_type =='complex')|(exp_type =='complex_SpdPup')|(exp_type =='complex_onoff'):\n",
    "                RF_all = GLM_Data['Cell_NN.0.weight'].reshape((GLM_Data['Cell_NN.0.weight'].shape[0],2*params['nt_glm_lag'],)+params['nks'])\n",
    "            else:\n",
    "                RF_all = GLM_Data['Cell_NN.0.weight'].reshape((GLM_Data['Cell_NN.0.weight'].shape[0],params['nt_glm_lag'],)+params['nks'])\n",
    "            RF_all_up = np.zeros((RF_all.shape[:2] + ( sf*(RF_all.shape[-2]), sf*(RF_all.shape[-1]))))\n",
    "            for celln in range(RF_all.shape[0]):\n",
    "                for t in range(RF_all.shape[1]):\n",
    "                    RF_all_up[celln,t] = cv2.resize(RF_all[celln,t], (sf*(RF_all.shape[-1]), sf*(RF_all.shape[-2])))\n",
    "            All_data[date_ani][exp_type][ModelName+'_rf_up'] = RF_all_up\n",
    "            All_data[date_ani][exp_type][ModelName+'_rf_all'] = RF_all\n",
    "        if (ModelRun == 2) | (ModelRun==3):\n",
    "            All_data[date_ani][exp_type][ModelName+'_moveW'] = GLM_Data['posNN.Layer0.weight'].copy()\n",
    "            All_data[date_ani][exp_type][ModelName+'_moveBias'] = GLM_Data['posNN.Layer0.bias'].copy()\n",
    "        if (ModelRun != 4):\n",
    "            if 'shuffled'==fmod.name.split('_')[-2]:\n",
    "                All_data[date_ani][exp_type][ModelName+'_cc_test'] = GLM_Data['cc_test'].copy()\n",
    "                All_data[date_ani][exp_type][ModelName+'_r2_test'] = GLM_Data['r2_test'].copy()\n",
    "            else:\n",
    "                All_data[date_ani][exp_type][ModelName+'_cc_test'] = np.array([(np.corrcoef(All_data[date_ani][exp_type][ModelName+'_pred_smooth'][:,celln],All_data[date_ani]['actual_smooth'][:,celln])[0, 1]) for celln in range(All_data[date_ani]['actual_smooth'].shape[1])])\n",
    "                All_data[date_ani][exp_type][ModelName+'_r2_test'] = np.array([(np.corrcoef(All_data[date_ani][exp_type][ModelName+'_pred_smooth'][:,celln],All_data[date_ani]['actual_smooth'][:,celln])[0, 1]**2) for celln in range(All_data[date_ani]['actual_smooth'].shape[1])])\n",
    "        else: \n",
    "            if 'shuffled'==fmod.name.split('_')[-2]:\n",
    "                All_data[date_ani][exp_type][ModelName+'_cc_test'] = GLM_Data['cc_test'].copy()\n",
    "                All_data[date_ani][exp_type][ModelName+'_r2_test'] = GLM_Data['r2_test'].copy()\n",
    "            else:\n",
    "                All_data[date_ani][exp_type][ModelName+'_cc_test'] = np.array([(np.corrcoef(All_data[date_ani][exp_type][ModelName+'_pred_smooth'][:,celln],All_data[date_ani]['actual_HF_smooth'][:,celln])[0, 1]) for celln in range(All_data[date_ani]['actual_HF_smooth'].shape[1])])\n",
    "                All_data[date_ani][exp_type][ModelName+'_r2_test'] = np.array([(np.corrcoef(All_data[date_ani][exp_type][ModelName+'_pred_smooth'][:,celln],All_data[date_ani]['actual_HF_smooth'][:,celln])[0, 1]**2) for celln in range(All_data[date_ani]['actual_HF_smooth'].shape[1])])\n",
    "        if (ModelRun == 4):\n",
    "            All_data[date_ani][exp_type]['HF_meanfr'] = np.mean(hf_nsp_all[da],axis=0)/params['model_dt']\n",
    "            if exp_type == 'complex':\n",
    "                r2_FMHF_RF_on = np.zeros(All_data[date_ani][exp_type][VisName+'_rf_all'].shape[0])\n",
    "                r2_FMHF_RF_off = np.zeros(All_data[date_ani][exp_type][VisName+'_rf_all'].shape[0])\n",
    "            else:\n",
    "                r2_FMHF_RF = np.zeros(All_data[date_ani][exp_type][VisName+'_rf_all'].shape[0])\n",
    "            for celln in np.arange(All_data[date_ani][exp_type][VisName+'_rf_all'].shape[0]):\n",
    "                if exp_type == 'complex':\n",
    "                    r2_FMHF_RF_on[celln] = np.corrcoef(All_data[date_ani][exp_type][VisName+'_rf_all'][celln,params['nt_glm_lag']//2].flatten(),All_data[date_ani][exp_type][ModelName+'_rf_all'][celln,(params['nt_glm_lag']//2)].flatten())[0,1]\n",
    "                    r2_FMHF_RF_off[celln] = np.corrcoef(All_data[date_ani][exp_type][VisName+'_rf_all'][celln,(params['nt_glm_lag']//2 + params['nt_glm_lag'])].flatten(),All_data[date_ani][exp_type][ModelName+'_rf_all'][celln,(params['nt_glm_lag']//2 + params['nt_glm_lag'])].flatten())[0,1]\n",
    "                else:\n",
    "                    r2_FMHF_RF[celln] = np.corrcoef(All_data[date_ani][exp_type][VisName+'_rf_all'][celln,params['nt_glm_lag']//2].flatten(),All_data[date_ani][exp_type][ModelName+'_rf_all'][celln,params['nt_glm_lag']//2].flatten())[0,1]\n",
    "            if exp_type == 'complex':\n",
    "                All_data[date_ani][exp_type][ModelName+'_FMHF_on_cc'] = r2_FMHF_RF_on\n",
    "                All_data[date_ani][exp_type][ModelName+'_FMHF_off_cc'] = r2_FMHF_RF_off\n",
    "            else:\n",
    "                All_data[date_ani][exp_type][ModelName+'_FMHF_cc'] = r2_FMHF_RF\n",
    "            \n",
    "SimRF_file = params['base_dir']/'121521/SimRF/fm1/SimRF_{}_dt{:03d}_T{:02d}_Model1_NB{}_Kfold{:02d}_best.h5'.format('withL1',int(params['model_dt']*1000), 1, params['Nepochs'],Kfold)\n",
    "SimRFfit_file = params['base_dir']/'121521/SimRF/fm1/GLM_Network/MovModel1/version_0/SimRF/GLM_Pytorch_Vis_withL1_SimRF_dt050_T01_MovModel1_NB10000_Kfold00_best.h5'\n",
    "SimFit_Data = ioh5.load(SimRFfit_file)\n",
    "SimRF_Data = ioh5.load(SimRF_file)\n",
    "RF_SimFit = SimFit_Data['Cell_NN.0.weight'].reshape((SimFit_Data['Cell_NN.0.weight'].shape[0],1,)+(30,40))\n",
    "SimFit_smooth = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=1, arr=SimFit_Data['pred_cv'].copy())[:,params['bin_length']:-params['bin_length']].T\n",
    "Sim_act_smooth = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=0, arr=SimRF_Data['yte'].copy())[params['bin_length']:-params['bin_length']].T\n",
    "Sim_r2 = np.array([(np.corrcoef(SimFit_smooth[:,celln],Sim_act_smooth[celln])[0, 1])**2 for celln in range(Sim_act_smooth.shape[0])])\n",
    "Sim_cc = np.array([(np.corrcoef(SimFit_smooth[:,celln],Sim_act_smooth[celln])[0, 1]) for celln in range(Sim_act_smooth.shape[0])])\n",
    "Sim_RFcc = np.array([(np.corrcoef(SimRF_Data['RF_actual'][celln].flatten(),RF_SimFit[celln,0].flatten())[0, 1]) for celln in range(Sim_act_smooth.shape[0])])\n",
    "Sim_dict = {'SimData': {'Simfit_smooth': SimFit_smooth,\n",
    "                        'Simact_smooth': Sim_act_smooth,\n",
    "                        'Sim_r2': Sim_r2,\n",
    "                        'Sim_cc': Sim_cc,\n",
    "                        'Sim_RFcc': Sim_RFcc,\n",
    "                        'Simact_RF':SimRF_Data['RF_actual'],\n",
    "                        'Simfit_RF':RF_SimFit, \n",
    "                        'Sim_yte': SimRF_Data['yte'],}}\n",
    "All_data.update(Sim_dict)\n",
    "\n",
    "All_data['tuning_sig_all'] = tuning_sig_all\n",
    "\n",
    "celltypes_all = pd.read_hdf(params['base_dir']/'Data4_dataset.h5')\n",
    "\n",
    "da = 0\n",
    "args['free_move']=True\n",
    "args['date_ani'] = pparams['dates_all'][da]\n",
    "params,_,_ = load_params(1,Kfold,args,debug=True)\n",
    "data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "\n",
    "# Remove files\n",
    "# rm_list = list(params['base_dir'].parents[1].rglob('*onlySpdPup_SpdPup*.h5'))\n",
    "# rm_list = np.array(sorted([path for path in params['base_dir'].parents[1].rglob('*T01*.h5') if  (('CropInputs' in path.parts) | ('complex' in path.parts) | ('OnlySpdPupil' in path.parts))  & ('VisShifter' not in path.stem) & ('complex_onoff' not in path.parts)])) # (params['exp_name'] in path.as_posix()) &\n",
    "# for f in rm_list:\n",
    "#     f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### STA Data #####\n",
    "sf=4\n",
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "args['free_move'] = True\n",
    "if args['free_move']:\n",
    "    stim_type = 'fm1'\n",
    "else:\n",
    "    stim_type = 'hf1_wn'  # 'fm1' #\n",
    "exp_type = 'CropInputs'\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "for da, date_ani in enumerate(dates_all):\n",
    "    args['date_ani'] = dates_all[da]\n",
    "    args['train_shifter']   = False\n",
    "    args['NoL1']            = False\n",
    "    args['NoL2']            = False\n",
    "    args['reg_lap']         = False\n",
    "    args['complex']         = False\n",
    "    args['do_shuffle']      = False\n",
    "    args['Nepochs']         = 10000\n",
    "    params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "\n",
    "    params = get_modeltype(params)\n",
    "    model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "    mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), 5, params['Nepochs'],Kfold)\n",
    "    ModelList = sorted(list(exp.save_dir.rglob(mod_name+'.h5')))\n",
    "    args['train_shifter']   = False\n",
    "    params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "    # params['lag_list']     = [0]\n",
    "    params['nt_glm_lag'] = len(params['lag_list'])\n",
    "    if args['train_shifter']:\n",
    "        params['lag_list']     = [0]\n",
    "        params['nt_glm_lag']   = len(params['lag_list'])\n",
    "        params['Nepochs']      = 5000\n",
    "    data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "\n",
    "    ##### Set Train Test Splits #####\n",
    "    Kfold = 0\n",
    "    train_idx = train_idx_list[Kfold]\n",
    "    test_idx = test_idx_list[Kfold]\n",
    "    data = load_Kfold_data(data,train_idx,test_idx,params)\n",
    "\n",
    "    params, xtr, xtrm, xte, xtem, ytr, yte, shift_in_tr, shift_in_te, input_size, output_size, meanbias, model_move = load_GLM_data(data, params, train_idx, test_idx)\n",
    "    print('Model: {}, LinMix: {}, move_features: {}, Ncells: {}, train_shifter: {}, NoL1: {}, NoL2: {}, reg_lap: {}, complex: {}'.format(\n",
    "        params['MovModel'],params['LinMix'],params['move_features'],params['Ncells'],params['train_shifter'],params['NoL1'],params['NoL2'],params['reg_lap'],params['complex']))\n",
    "    \n",
    "    xtr = xtr.cpu().detach().numpy()\n",
    "    ytr = ytr.cpu().detach().numpy()\n",
    "    xte = xte.cpu().detach().numpy()\n",
    "    yte = yte.cpu().detach().numpy()\n",
    "    sta = (xtr.T @ ytr)/np.sum(ytr,0,keepdims=True)\n",
    "    yte_pred = xte@sta\n",
    "    sta_im = sta.T.reshape((ytr.shape[1],params['nt_glm_lag'],)+params['nks'])\n",
    "    sp_smooth = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=0, arr=yte)[params['bin_length']:-params['bin_length']]\n",
    "    pred_smooth = np.apply_along_axis(lambda m: np.convolve(m, np.ones(params['bin_length']), mode='same')/(params['bin_length'] * params['model_dt']), axis=0, arr=yte_pred)[params['bin_length']:-params['bin_length']]\n",
    "    cc_all = np.array([(np.corrcoef(yte[:,celln],yte_pred[:,celln])[0, 1]) for celln in range(yte.shape[1])])\n",
    "\n",
    "    # sta_im,sp_smooth,pred_smooth,cc_all = compute_STA(xtr,xte,ytr,yte,params)\n",
    "    All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_im'] = sta_im\n",
    "    RF_all_up = np.zeros((sta_im.shape[:2] + ( sf*(sta_im.shape[-2]), sf*(sta_im.shape[-1]))))\n",
    "    for celln in range(sta_im.shape[0]):\n",
    "        for t in range(sta_im.shape[1]):\n",
    "            RF_all_up[celln,t] = cv2.resize(sta_im[celln,t], (sf*(sta_im.shape[-1]), sf*(sta_im.shape[-2])))\n",
    "    All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_up'] = RF_all_up\n",
    "    All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_sp_smooth'] = sp_smooth\n",
    "    All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_pred_smooth'] = pred_smooth\n",
    "    All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_cc_all'] = cc_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioh5.save(params['save_dir'].parents[2]/'All_data_v2_T01.h5',All_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "args['free_move'] = True\n",
    "if args['free_move']:\n",
    "    stim_type = 'fm1'\n",
    "else:\n",
    "    stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['date_ani']        = dates_all[0]\n",
    "args['train_shifter']   = False\n",
    "args['NoL1']            = False\n",
    "args['NoL2']            = False\n",
    "args['reg_lap']         = False\n",
    "args['complex']         = False\n",
    "args['do_shuffle']      = False\n",
    "args['use_spdpup']      = False\n",
    "args['Nepochs'] = 10000\n",
    "\n",
    "params,file_dict,exp = load_params(MovModel,Kfold,args,{},debug=True)\n",
    "params['lag_list']     = [-2,-1,-0,1,2]\n",
    "params['nt_glm_lag']   = len(params['lag_list'])\n",
    "\n",
    "pparams={\n",
    "    'mod_titles':   ['Mot','Vis','Add','Mul','HF','VisNoShifter'],\n",
    "    'mod_clrs':     [\"#B541FF\",\"#00A14B\",\"#118ab2\",\"#ef476f\",\"#073b4c\"],\n",
    "    'dates_all':    ['070921/J553RT','101521/J559NC','102821/J570LT','110421/J569LT'],\n",
    "}\n",
    "pparams['date_ani2'] = [pparams['dates_all'][n].replace('/','_') for n in range(len(pparams['dates_all']))]\n",
    "locals().update(pparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltypes_all = pd.read_hdf(params['base_dir']/'Data4_dataset.h5')\n",
    "All_data_T1 = ioh5.load(params['save_dir'].parents[2]/'All_data_v2_T01.h5')\n",
    "All_data = ioh5.load(params['save_dir'].parents[2]/'All_data_v2.h5')\n",
    "exp_type = 'CropInputs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['5050split', 'CropInputs', 'CropInputs_SpdPup', 'EyeHead_only', 'OnlySpdPupil', 'actual_HF_smooth', 'actual_meanfr', 'actual_smooth', 'ax_lims', 'complex', 'complex_SpdPup', 'complex_onoff', 'model_move_FM', 'model_move_HF', 'move_test', 'nsp_raw', 'tuning_sig'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_data[pparams['date_ani2'][0]].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs'\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_VisNoSh = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['VisNoShifter_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "print('With Shifter:',np.nanmean(vals_Vis),'Without Shifter:', np.nanmean(vals_VisNoSh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_name = paper_fig_dir/'Figure2.pdf'\n",
    "# figure2(All_data,pparams)\n",
    "figure2(All_data,pparams,figname=fig2_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3_name = paper_fig_dir/'Figure3.pdf'\n",
    "# figure3(All_data,pparams)\n",
    "figure3(All_data,pparams,figname=fig3_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure4(All_data,pparams,exp_type='CropInputs')\n",
    "# figure4(All_data,pparams,exp_type='OnlySpdPupil')\n",
    "# figure4(All_data,pparams,exp_type='CropInputs_SpdPup')\n",
    "# figure4(All_data,pparams,exp_type='complex_SpdPup')\n",
    "# figure4(All_data,pparams,exp_type='complex_onoff')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4_name = paper_fig_dir/'Figure4.pdf'\n",
    "fig4_name_complex = paper_fig_dir/'Figure4_complex.pdf'\n",
    "fig4_name_complex_onoff = paper_fig_dir/'Figure4_complex_onoff.pdf'\n",
    "figure4(All_data,pparams,exp_type='CropInputs',figname=fig4_name)\n",
    "# figure4(All_data,pparams,exp_type='complex',figname=fig4_name_complex)\n",
    "# figure4(All_data,pparams,exp_type='complex_onoff',figname=fig4_name_complex_onoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_shuffled_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "shuff_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_shuffled_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "shuff_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_shuffled_cc_test'] for da in range(len(pparams['dates_all']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b901d",
   "metadata": {},
   "source": [
    "## % stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### % time active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 0\n",
    "args['free_move']=True\n",
    "args['do_shuffle']=False\n",
    "active_all = []\n",
    "for da in range(len(dates_all)):\n",
    "    args['date_ani'] = dates_all[da]\n",
    "    params,_,_ = load_params(2,Kfold,args,debug=True)\n",
    "    data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "    active_all.append(np.sum(data['model_active']>.5)/data['model_active'].shape[0])\n",
    "\n",
    "active_all = np.stack(active_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for da in range(len(dates_all)):\n",
    "    print('{} - % time active: {:.02}'.format(dates_all[da],active_all[da]))\n",
    "print('{} - % time active: {:.02}'.format('mean',np.mean(active_all)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg move speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 0\n",
    "args['free_move']=True\n",
    "args['do_shuffle']=False\n",
    "args['use_spdpup'] = True\n",
    "active_all = []\n",
    "for da in range(len(dates_all)):\n",
    "    args['date_ani'] = dates_all[da]\n",
    "    args['do_norm'] = False\n",
    "    params,_,_ = load_params(2,Kfold,args,debug=True)\n",
    "    params['do_norm'] = False\n",
    "    data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "    active_all.append(np.mean(data['model_speed']))\n",
    "\n",
    "for da in range(len(dates_all)):\n",
    "    print('{} - avg Move speed: {:.02}'.format(dates_all[da],np.mean(active_all[da])))\n",
    "print('{} - avg Move speed: {:.02}'.format('mean',np.mean(active_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7016d3ce",
   "metadata": {},
   "source": [
    "### Paired T test for Shifter on vs off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### across all cells\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_VisNoSh = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['VisNoShifter_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "r2_all = np.stack((vals_VisNoSh,vals_Vis))\n",
    "vis_th = r2_all[1] > .05\n",
    "ttest_rel(r2_all[0,vis_th],r2_all[1,vis_th])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Average within session then t-test across sessions\n",
    "vals_Vis=[]\n",
    "vals_VisNoSh=[]\n",
    "for da in range(len(pparams['dates_all'])):\n",
    "    vals_Vis.append(np.nanmean(All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test']))\n",
    "    vals_VisNoSh.append(np.nanmean(All_data[pparams['date_ani2'][da]][exp_type]['VisNoShifter_cc_test']))\n",
    "\n",
    "vals_Vis=np.array(vals_Vis)\n",
    "vals_VisNoSh=np.array(vals_VisNoSh)\n",
    "\n",
    "ttest_rel(vals_Vis,vals_VisNoSh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired t-test GLM vs. STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_sta = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_cc_all'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "cc_all2 = np.stack((cc_sta,vals_Vis))\n",
    "RF_all = np.stack((np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_sta_up'][:,0] for da in range(len(pparams['dates_all']))]),np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_rf_up'][:,2,:,:] for da in range(len(pparams['dates_all']))])))\n",
    "\n",
    "ttest_rel(vals_Vis,cc_sta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d86cf9",
   "metadata": {},
   "source": [
    "### Standard Deviation of positional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.nanstd(model_move_FM,axis=0),decimals=1),np.round(np.nanstd(model_move_HF,axis=0),decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a369a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(model_move_FM[:,0])<15)/(model_move_FM.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb4fd2",
   "metadata": {},
   "source": [
    "### number of units increased with joint fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0da35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "np.sum(vals_Mot>vals_Vis),vals_Vis.shape[0],np.sum(vals_Mot<vals_Vis),vals_Vis.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(vals_Vis[vals_Vis>.05]), np.nanmean(vals_Vis), np.nanmax(vals_Vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "move_r2_th = (vals_Vis>.05) # & (vals_Mul>vals_Vis)\n",
    "vals_Mot = vals_Mot[move_r2_th]\n",
    "vals_Vis = vals_Vis[move_r2_th]\n",
    "vals_Add = vals_Add[move_r2_th]\n",
    "vals_Mul = vals_Mul[move_r2_th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71aa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_joint = np.stack((vals_Add,vals_Mul))\n",
    "vals_increased = np.max(vals_joint,axis=0)[np.any(vals_joint>vals_Vis,axis=0)]-vals_Vis[np.any(vals_joint>vals_Vis,axis=0)]\n",
    "np.max(vals_joint,axis=0)[np.any(vals_joint>vals_Vis,axis=0)]-vals_Vis[np.any(vals_joint>vals_Vis,axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b16fc",
   "metadata": {},
   "source": [
    "T-test: Add vs Mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "move_r2_th = ((vals_Add>vals_Vis) | (vals_Mul>vals_Vis)) # & (vals_Mul>vals_Vis)\n",
    "vals_Mot = vals_Mot[move_r2_th]\n",
    "vals_Vis = vals_Vis[move_r2_th]\n",
    "vals_Add = vals_Add[move_r2_th]\n",
    "vals_Mul = vals_Mul[move_r2_th]\n",
    "increase = vals_Vis\n",
    "th1 = ((increase>0) & (vals_Vis>.22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2632699",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(vals_Mul[th1],vals_Add[th1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paired t-test between mul and add and show the distributions are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff = []\n",
    "avg_add=[]\n",
    "avg_mul=[]\n",
    "for da in range(len(pparams['dates_all'])):\n",
    "    vM = All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test']\n",
    "    vV = All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test']\n",
    "    vA = All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test']\n",
    "    v_th = ((vA>vV) | (vM>vV)) & (vV>.05) # & (vals_Mul>vals_Vis)\n",
    "    diff = vM[v_th]-vA[v_th]\n",
    "    avg_add.append(np.nanmean(vA[v_th]))\n",
    "    avg_mul.append(np.nanmean(vM[v_th]))\n",
    "    avg_diff.append(np.nanmean(diff))\n",
    "    print(np.nanmean(diff))\n",
    "ttest_1samp(avg_diff,popmean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da320286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(vals_Add,vals_Mul),ttest_rel(avg_add,avg_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d27520",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(vals_Add[vals_Add<vals_Mul]),vals_Add[vals_Add<vals_Mul][np.argmax(vals_Add[vals_Add<vals_Mul])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanargmax(vals_Add-vals_Mul),vals_Add[92],vals_Mul[92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(vals_Add==vals_Add[vals_Add<vals_Mul][np.argmax(vals_Add[vals_Add<vals_Mul])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf80b0f",
   "metadata": {},
   "source": [
    "# Supplementary Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['free_move']=True\n",
    "scale_factor = 0.195 # multiply by raw trace to get microvolts\n",
    "date_ani2 = [dates_all[n].replace('/','_') for n in range(len(dates_all))]\n",
    "waveform_data = {date_ani2[n]:{} for n in range(len(date_ani2))}\n",
    "for da in range(len(dates_all)):\n",
    "    args['date_ani'] = dates_all[da]\n",
    "    params,file_dict,_ = load_params(1,Kfold,args,debug=True)\n",
    "    waveform_FM = ioh5.load(params['save_dir_fm']/'waveforms.h5')\n",
    "    waveform_HF = ioh5.load(params['save_dir_hf']/'waveforms.h5')\n",
    "    waveform_data[date_ani2[da]]['waveform_FM'] = waveform_FM\n",
    "    waveform_data[date_ani2[da]]['waveform_HF'] = waveform_HF\n",
    "    waveform_data[date_ani2[da]]['mean_wf_FM']  = scale_factor*np.stack([np.mean(waveform_FM['waveform']['{}'.format(n)],axis=0) for n in range(len(waveform_FM['waveform']))])\n",
    "    waveform_data[date_ani2[da]]['std_wf_FM']   = scale_factor*np.stack([np.std(waveform_FM['waveform']['{}'.format(n)],axis=0) for n in range(len(waveform_FM['waveform']))])\n",
    "    waveform_data[date_ani2[da]]['mean_wf_HF']  = scale_factor*np.stack([np.mean(waveform_HF['waveform']['{}'.format(n)],axis=0) for n in range(len(waveform_HF['waveform']))])\n",
    "    waveform_data[date_ani2[da]]['std_wf_HF']   = scale_factor*np.stack([np.std(waveform_HF['waveform']['{}'.format(n)],axis=0) for n in range(len(waveform_HF['waveform']))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "celln = 1\n",
    "samp_rate = 30000\n",
    "mean_wf_FM = np.concatenate([waveform_data[date_ani2[da]]['mean_wf_FM']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "std_wf_FM = np.concatenate([waveform_data[date_ani2[da]]['std_wf_FM']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "mean_wf_HF = np.concatenate([waveform_data[date_ani2[da]]['mean_wf_HF']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "std_wf_HF = np.concatenate([waveform_data[date_ani2[da]]['std_wf_HF']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "\n",
    "time_waveform = np.arange(0,2,1000/samp_rate)\n",
    "# fig, axs = plt.subplots(2,2,figsize=(8,3))\n",
    "\n",
    "fig = plt.figure(constrained_layout=False, figsize=(10,6))\n",
    "gs0 = gridspec.GridSpec(nrows=2,ncols=2, figure=fig, wspace=.3,hspace=.4)\n",
    "\n",
    "# gs00 = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=gs0[0,1:],wspace=.8,hspace=.7)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(3,1, subplot_spec=gs0[0:,:1],wspace=.05,hspace=.8)\n",
    "axs = np.array([fig.add_subplot(gs0[0,0]),fig.add_subplot(gs0[1,0])])\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(time_waveform,mean_wf_FM[celln], 'k',label='waveform start')\n",
    "ax.fill_between(time_waveform,mean_wf_FM[celln]+std_wf_FM[celln],mean_wf_FM[celln]-std_wf_FM[celln],color='k',alpha=.5)\n",
    "ax.set_xlabel('time (ms)',fontsize=fontsize)\n",
    "ax.set_title('Freely moving',fontsize=fontsize+2)\n",
    "# ax.legend(labelcolor='linecolor', fontsize=fontsize, handlelength=0, handletextpad=0,loc='lower right',ncol=1)\n",
    "ax.set_ylim([-150,100])\n",
    "ax.set_ylabel(r'$\\mu V$',fontsize=fontsize)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(time_waveform,mean_wf_HF[celln], 'k',label='waveform start')\n",
    "ax.fill_between(time_waveform,mean_wf_HF[celln]+std_wf_HF[celln],mean_wf_HF[celln]-std_wf_HF[celln],color='k',alpha=.5)\n",
    "ax.set_xlabel('time (ms)',fontsize=fontsize)\n",
    "ax.set_title('Head-fixed',fontsize=fontsize+2)\n",
    "ax.set_ylim([-150,100])\n",
    "ax.set_ylabel(r'$\\mu V$',fontsize=fontsize)\n",
    "\n",
    "\n",
    "axs = np.array([fig.add_subplot(gs0[:1,1:]),fig.add_subplot(gs0[1:,1:])])\n",
    "ax = axs[0]\n",
    "std_wf_all = np.stack([std_wf_FM,std_wf_HF])\n",
    "ax.bar(np.arange(2), np.mean(std_wf_all,axis=(1,2)),yerr= np.std(std_wf_all,axis=(1,2))/np.sqrt(std_wf_all.shape[0]), color='k',error_kw=dict(ecolor='#6D6E71', lw=2, capsize=5, capthick=2))\n",
    "ax.set_xticks(np.arange(2))\n",
    "ax.set_xticklabels(['Freely moving','Head-fixed'],fontsize=fontsize)\n",
    "ax.set_ylabel(r'standard deviation ($\\mu V$)', fontsize=fontsize)\n",
    "ax.set_yticks([0,10,20,30])\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "r2 = r2_score(mean_wf_FM.T,mean_wf_HF.T,multioutput='raw_values')\n",
    "hbins=.01\n",
    "lim0 = .8\n",
    "lim1 = 1.01\n",
    "dlim = .2\n",
    "ylim = .4\n",
    "dylim = .15\n",
    "xticks = [.8,.9,1] \n",
    "count,edges = np.histogram(r2,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(r2),color='k',width=hbins, alpha=1,zorder=1) \n",
    "\n",
    "\n",
    "ax.set_xlabel('waveform $R^2$',fontsize=fontsize)\n",
    "ax.set_ylabel('fraction of units',fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(0,ylim,dylim))\n",
    "ax.set_yticklabels(np.round(np.arange(0,ylim,dylim),decimals=3),fontsize=fontsize)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(np.round(xticks,decimals=1), fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/'Waveform.pdf', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celltype Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs'\n",
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_All = np.stack([vals_Mot,vals_Vis,vals_Mul-vals_Add])\n",
    "fig, axs = plt.subplots(1,4,figsize=(8.5,1.8))\n",
    "c_cell_EI   = celltypes_all['exc_or_inh'].factorize()[0]\n",
    "vals_Mot2 = [vals_Mot[c_cell_EI==0],vals_Mot[c_cell_EI==1]]\n",
    "vals_Vis2 = [vals_Vis[c_cell_EI==0],vals_Vis[c_cell_EI==1]]\n",
    "ste_all = [(np.nanstd(vals_Mot2[0],axis=0)/np.sqrt(vals_Mot2[0].shape[0])),\n",
    "            (np.nanstd(vals_Vis2[0],axis=0)/np.sqrt(vals_Vis2[0].shape[0])),\n",
    "            (np.nanstd(vals_Mot2[1],axis=0)/np.sqrt(vals_Mot2[1].shape[0])),\n",
    "            (np.nanstd(vals_Vis2[1],axis=0)/np.sqrt(vals_Vis2[1].shape[0]))]\n",
    "\n",
    "ax = axs[0]\n",
    "ax.bar([0],np.nanmean(vals_Mot2[0],axis=0),yerr=ste_all[0],color=\"#B541FF\",error_kw=dict(ecolor='#6D6E71', lw=2, capsize=2, capthick=2))\n",
    "ax.bar([1],np.nanmean(vals_Vis2[0],axis=0),yerr=ste_all[1],color=\"#00A14B\",error_kw=dict(ecolor='#6D6E71', lw=2, capsize=2, capthick=2))\n",
    "ax.bar([2],np.nanmean(vals_Mot2[1],axis=0),yerr=ste_all[2],color=\"#B541FF\",error_kw=dict(ecolor='#6D6E71', lw=2, capsize=2, capthick=2))\n",
    "ax.bar([3],np.nanmean(vals_Vis2[1],axis=0),yerr=ste_all[3],color=\"#00A14B\",error_kw=dict(ecolor='#6D6E71', lw=2, capsize=2, capthick=2))\n",
    "ax.plot([0,0, 1, 1], [.31, .32, .32, .31], linewidth=1, color='k')\n",
    "ax.set_yticks([0,.15,.3])\n",
    "ax.set_xticks([.5,2.5])\n",
    "ax.set_xticklabels(['exc','inh'],fontsize=fontsize)\n",
    "ax.set_ylabel('cc',fontsize=fontsize)\n",
    "legend1 = ax.legend(['Vis','Pos'],labelcolor=[\"#00A14B\",\"#B541FF\"],fontsize=fontsize,ncol=1, markerscale=0, handlelength=0, handletextpad=-1.5,loc=\"upper left\",frameon=False, bbox_to_anchor=(1.1, 1.1))\n",
    "\n",
    "\n",
    "\n",
    "lims = (-.05, .85)\n",
    "ticks = [0,.4,.8]\n",
    "ln_max = .8\n",
    "crange=500\n",
    "c_cell_depth =celltypes_all['Wn_depth_from_layer5']#[move_r2_th]\n",
    "labels = [r'$cc_{pos}$',r'$cc_{vis}$',r'$cc_{mul}-cc_{add}$']\n",
    "for l,n in enumerate(range(1,3)):\n",
    "    ax = axs[n]\n",
    "    im=ax.scatter(c_cell_depth,vals_All[l],s=5,color='k',edgecolors='none')\n",
    "    ax.set_xlabel(r'depth from L5 ($\\mu$m)',fontsize=fontsize)\n",
    "    ax.set_ylabel(labels[l], fontsize=fontsize)\n",
    "    ax.set_xlim([-400,700])\n",
    "    ax.set_xticks([-400,0,400])\n",
    "    ax.set_xticklabels([-400,0,400])\n",
    "    if l<2:\n",
    "        ax.set_ylim([0,.7])\n",
    "    else:\n",
    "        ax.set_ylim(-.1,.1)\n",
    "\n",
    "pparams['titles'] = [r'$\\theta$',r'$\\phi$',r'$\\rho$',r'$\\omega$']\n",
    "MotW = np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_moveW'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "x_sc = np.stack([np.random.normal(n, 0.1, MotW.shape[0]) for n in range(len(pparams['titles']))])\n",
    "ax = axs[3]\n",
    "for modeln in np.arange(0,len(pparams['titles'])):\n",
    "    ax.scatter(x_sc[modeln], MotW[:,modeln],s=2,c='k',edgecolors='none')\n",
    "ax.set_xticks(np.arange(len(pparams['titles'])))\n",
    "ax.set_xticklabels(pparams['titles'],fontsize=fontsize)\n",
    "ax.set_ylim([-.5,.5])\n",
    "ax.set_yticks([-.5,0,.5])\n",
    "ax.set_yticklabels([-.5,0,.5],fontsize=fontsize)\n",
    "ax.set_ylabel('GLM Weights',fontsize=fontsize)\n",
    "ax.axhline(y=0,ls='--',color='#6D6E71')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/'CellType_Scatter.pdf', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs'\n",
    "All_data[pparams['date_ani2'][0]][exp_type]['Mot_moveW'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MotW = np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_moveW'] for da in range(len(pparams['dates_all']))])\n",
    "pparams['titles'] = [r'$(\\theta)$',r'$(\\phi)$',r'$(\\rho)$',r'$(\\omega)$']\n",
    "x_sc = np.stack([np.random.normal(n, 0.1, MotW.shape[0]) for n in range(len(pparams['titles']))])\n",
    "fig, axs = plt.subplots(figsize=(4,2))\n",
    "ax = axs\n",
    "for modeln in np.arange(0,len(pparams['titles'])):\n",
    "    ax.scatter(x_sc[modeln], MotW[:,modeln],s=5,c='k',edgecolors='none')\n",
    "ax.set_xticks(np.arange(len(pparams['titles'])))\n",
    "ax.set_xticklabels(pparams['titles'],fontsize=fontsize)\n",
    "ax.set_ylim([-.5,.5])\n",
    "ax.set_yticks([-.5,0,.5])\n",
    "ax.set_yticklabels([-.5,0,.5],fontsize=fontsize)\n",
    "ax.set_ylabel('GLM Weights',fontsize=fontsize)\n",
    "ax.axhline(y=0,ls='--',color='#6D6E71')\n",
    "fig.savefig(paper_fig_dir/'PosW.jpg', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest for excitatory cells \n",
    "ttest_rel(vals_Mot2[0],vals_Vis2[0]),ttest_rel(vals_Mot2[1],vals_Vis2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeedPup Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs'\n",
    "vals_Add_CP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul_CP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mot_CP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "exp_type = 'OnlySpdPupil'\n",
    "vals_Add_OSP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul_OSP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mot_OSP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "exp_type = 'CropInputs_SpdPup'\n",
    "vals_Vis_SP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add_SP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul_SP = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'] for da in range(len(pparams['dates_all']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs_SpdPup'\n",
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_r2_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_r2_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Add = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_r2_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mul = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_r2_test'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "AddW = np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Add_moveW'] for da in range(len(pparams['dates_all']))])\n",
    "MulW = np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Mul_moveW'] for da in range(len(pparams['dates_all']))])\n",
    "MotW = np.vstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_moveW'] for da in range(len(pparams['dates_all']))])\n",
    "\n",
    "Mot_thresh = vals_Mot>.05\n",
    "vals_Add2 = (np.abs(AddW)/np.sum(np.abs(AddW),axis=1,keepdims=True))[Mot_thresh]\n",
    "vals_Mul2 = (np.abs(MulW)/np.sum(np.abs(MulW),axis=1,keepdims=True))[Mot_thresh]\n",
    "vals_Mot2 = (np.abs(MotW)/np.sum(np.abs(MotW),axis=1,keepdims=True))[Mot_thresh]\n",
    "\n",
    "np.nanmean(vals_Add2,axis=0),np.nanmean(vals_Mul2,axis=0),np.nanmean(vals_Mot2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_nonVis = (np.sum(np.abs(MulW)[:,:4],axis=1)[:,np.newaxis]/np.sum(np.abs(MulW),axis=1,keepdims=True))[Mot_thresh]\n",
    "vals_nonVis2 = (np.sum(np.abs(MulW)[:,4:5],axis=1)[:,np.newaxis]/np.sum(np.abs(MulW),axis=1,keepdims=True))[Mot_thresh]\n",
    "vals_nonVis3 = (np.sum(np.abs(MulW)[:,5:],axis=1)[:,np.newaxis]/np.sum(np.abs(MulW),axis=1,keepdims=True))[Mot_thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['theta','phi','pitch','roll','spd','pup']\n",
    "r2_all = [np.nanmean(vals_Mot_CP)**2,np.nanmean(vals_Mot_OSP)**2,np.nanmean(vals_Vis_SP)**2,np.nanmean(vals_Mul_CP)**2,np.nanmean(vals_Mul_OSP)**2,np.nanmean(vals_Mul_SP)**2] #\n",
    "fig, axs = plt.subplots(1,3,figsize=(8.4,2))\n",
    "ax=axs[0]\n",
    "ax.bar(np.arange(len(r2_all)),r2_all,color='k')\n",
    "ax.set_xticks(np.arange(len(r2_all)))\n",
    "ax.set_xticklabels(['pos','sp','vis','mul_pos','mul_sp','mul_all'],fontsize=fontsize,ha='center',rotation=45)\n",
    "ax.set_yticks([0,.05,.1,.15])\n",
    "ax.set_ylabel(r'$r^2$',fontsize=fontsize)\n",
    "\n",
    "ax = axs[1]\n",
    "for modeln in range(len(titles)):\n",
    "    ax.bar(modeln, np.nanmean(vals_Mul2,axis=0)[modeln], color='k')\n",
    "    ax.set_xticks(np.arange(0,len(titles)))\n",
    "    ax.set_xticklabels(titles,fontsize=fontsize)\n",
    "    ax.set_ylabel('Fraction of contribution',fontsize=fontsize)\n",
    "    ax.set_title('Mul. Weights',fontsize=fontsize+2)\n",
    "ax.set_yticks([0,.2,.4])\n",
    "\n",
    "ax = axs[2]\n",
    "ax.bar([0], np.nanmean(vals_nonVis,axis=0), color='k')\n",
    "ax.bar([1], np.nanmean(vals_nonVis2,axis=0), color='k')\n",
    "ax.bar([2], np.nanmean(vals_nonVis3,axis=0), color='k')\n",
    "ax.set_xticks(np.arange(0,3))\n",
    "ax.set_xticklabels(['position','speed','pupil'],fontsize=fontsize)\n",
    "ax.set_ylabel('Fraction of contribution',fontsize=fontsize)\n",
    "ax.set_title('Mul. Weights',fontsize=fontsize+2)\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/'Mul_SpdPup_Contribution.pdf', dpi=300, transparent=True, bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'CropInputs_SpdPup'\n",
    "t = 0\n",
    "dt = int(100/params['model_dt'])\n",
    "# dt = 1000\n",
    "cell = 265 # np.nanargmax(GLM_Data['cc_test'])\n",
    "CellNum_Tot = get_cellnums(All_data,pparams,exp_type=exp_type)\n",
    "da,celln = ([[da,np.where(CellNum_Tot[key]==cell)[0][0]] for da,key in enumerate(CellNum_Tot.keys()) if len(np.where(CellNum_Tot[key]==cell)[0])>0])[0]\n",
    "\n",
    "titles = [r'$\\theta$',r'$\\phi$',r'$\\rho$',r'$\\omega$','speed','eyerad']\n",
    "fig, axs = plt.subplots(1,3,figsize=(20,5))\n",
    "ax = axs[0]\n",
    "ax.plot(All_data[pparams['date_ani2'][da]]['actual_smooth'][t:t+dt,celln],label='data')\n",
    "ax.plot(All_data[pparams['date_ani2'][da]][exp_type]['Vis_pred_smooth'][t:t+dt,celln],label='vis')\n",
    "ax.plot(All_data[pparams['date_ani2'][da]][exp_type]['Add_pred_smooth'][t:t+dt,celln],label='add')\n",
    "ax.plot(All_data[pparams['date_ani2'][da]][exp_type]['Mul_pred_smooth'][t:t+dt,celln],label='mul')\n",
    "ax.legend(labelcolor='linecolor', fontsize=fontsize, handlelength=0, handletextpad=0,)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar(titles,All_data[pparams['date_ani2'][da]][exp_type]['Add_moveW'][celln]+All_data[pparams['date_ani2'][da]][exp_type]['Add_moveBias'][celln])\n",
    "\n",
    "ax = axs[2]\n",
    "ax.bar(titles,All_data[pparams['date_ani2'][da]][exp_type]['Mul_moveW'][celln]+All_data[pparams['date_ani2'][da]][exp_type]['Mul_moveBias'][celln])\n",
    "\n",
    "plt.suptitle('vis_cc: {:.03f}, add_cc: {:.03f}, mul_cc: {:.03f}'.format(All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'][celln],All_data[pparams['date_ani2'][da]][exp_type]['Add_cc_test'][celln],All_data[pparams['date_ani2'][da]][exp_type]['Mul_cc_test'][celln]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106aaed",
   "metadata": {},
   "source": [
    "## Tuning half/half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ca1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN', ,'122021/J581RT'\n",
    "ModelList_all,test_std,tuning_sig_all,tuning_sig_all2,NCells_all,bad_cells_all,test_nsp_all,hf_nsp_all,model_move_FM,model_move_HF = [],[], [], [], [], [], [], [], [], []\n",
    "for d,date_ani in enumerate(dates_all):\n",
    "    args['free_move']   = True\n",
    "    args['date_ani']    = date_ani\n",
    "    args['NoL1']        = False\n",
    "    args['NoL2']        = False\n",
    "    args['reg_lap']     = False\n",
    "    args['do_shuffle']  = False\n",
    "    args['use_spdpup']  = False\n",
    "    args['only_spdpup'] = False\n",
    "    args['complex']     = False\n",
    "    args['crop_input']  = True\n",
    "    args['Nepochs']     = 10000\n",
    "    params,_,_ = load_params(1,Kfold,args,debug=True)\n",
    "    params['nt_glm_lag']=5\n",
    "    date_ani_dir = params['save_dir'].parent.parent.parent/date_ani\n",
    "    params = get_modeltype(params)\n",
    "    model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "    mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "    ModelList = np.array(sorted([path for path in date_ani_dir.rglob(mod_name+'.h5') if (params['exp_name'] in path.as_posix()) & ('SpdPup' not in path.stem) & ('VisNoShifter' not in path.stem) & ('SimRF' not in path.stem) & ('complex' not in path.stem)]))\n",
    "    ModelList_all.append(ModelList)\n",
    "    bad_cells = np.load(params['save_dir_hf']/'bad_cells.npy')\n",
    "    bad_cells_all.append(bad_cells)\n",
    "    data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "    tuning_thresh = .2\n",
    "    thresh_fr = 1\n",
    "    spk_percentile2 = np.arange(.125,1.125,.25)\n",
    "    quartiles = np.arange(0,1.25,.25)\n",
    "\n",
    "    nsp_half0 = data['model_nsp'][:data['model_nsp'].shape[0]//2]\n",
    "    tuning_curves0 = np.zeros((nsp_half0.shape[1],model_move.shape[-1],len(quartiles)-1))\n",
    "    tuning_stds0 = np.zeros((nsp_half0.shape[1],model_move.shape[-1],1))\n",
    "    tuning_curve_edges0 = np.zeros((nsp_half0.shape[1],model_move.shape[-1],len(quartiles)-1))\n",
    "    ax_ylims0 = np.zeros((nsp_half0.shape[-1],model_move.shape[-1]))\n",
    "\n",
    "    nsp_half1 = data['model_nsp'][data['model_nsp'].shape[0]//2:]\n",
    "    tuning_curves1 = np.zeros((nsp_half1.shape[1],model_move.shape[-1],len(quartiles)-1))\n",
    "    tuning_stds1 = np.zeros((nsp_half1.shape[1],model_move.shape[-1],1))\n",
    "    tuning_curve_edges1 = np.zeros((nsp_half1.shape[1],model_move.shape[-1],len(quartiles)-1))\n",
    "    ax_ylims1 = np.zeros((nsp_half1.shape[-1],model_move.shape[-1]))\n",
    "    for i,modeln in enumerate(range(model_move.shape[-1])):\n",
    "        for celln in np.arange(nsp_half0.shape[1]):\n",
    "            metric0 = model_move[:data['model_nsp'].shape[0]//2,modeln]\n",
    "            nranges0 = np.quantile(metric0,quartiles)\n",
    "            stat_range0, edges0, _ = binned_statistic(metric0,nsp_half0[:,celln],statistic='mean',bins=nranges0)\n",
    "            stat_std0, _, _ = binned_statistic(metric0,nsp_half0[:,celln],statistic='std',bins=nranges0)\n",
    "            tuning_curves0[celln,modeln] = stat_range0/params['model_dt']\n",
    "            edge_mids0 = np.quantile(metric0,spk_percentile2)\n",
    "            tuning_curve_edges0[celln,modeln] = edge_mids0\n",
    "            tuning_stds0[celln,modeln] = stat_std0.max()\n",
    "\n",
    "            metric1 = model_move[data['model_nsp'].shape[0]//2:,modeln]\n",
    "            nranges1 = np.quantile(metric1,quartiles)\n",
    "            stat_range1, edges1, _ = binned_statistic(metric1,nsp_half1[:,celln],statistic='mean',bins=nranges1)\n",
    "            stat_std1, _, _ = binned_statistic(metric1,nsp_half1[:,celln],statistic='std',bins=nranges1)\n",
    "            tuning_curves1[celln,modeln] = stat_range1/params['model_dt']\n",
    "            edge_mids1 = np.quantile(metric1,spk_percentile2)\n",
    "            tuning_curve_edges1[celln,modeln] = edge_mids1\n",
    "            tuning_stds1[celln,modeln] = stat_std1.max()\n",
    "        ax_ylims0[:,modeln] = np.nanmax(tuning_curves0[:,modeln],axis=-1)\n",
    "        ax_ylims1[:,modeln] = np.nanmax(tuning_curves1[:,modeln],axis=-1)\n",
    "    tc_mod0 = (np.max(tuning_curves0,axis=-1,keepdims=True)-np.min(tuning_curves0,axis=-1,keepdims=True))/(np.max(tuning_curves0,axis=-1,keepdims=True)+np.min(tuning_curves0,axis=-1,keepdims=True))\n",
    "    avg_fr0 = np.mean(tuning_curves0,axis=(-1,-2)).squeeze()\n",
    "    tc_mod1 = (np.max(tuning_curves1,axis=-1,keepdims=True)-np.min(tuning_curves1,axis=-1,keepdims=True))/(np.max(tuning_curves1,axis=-1,keepdims=True)+np.min(tuning_curves1,axis=-1,keepdims=True))\n",
    "    avg_fr1 = np.mean(tuning_curves1,axis=(-1,-2)).squeeze()\n",
    "\n",
    "    tuning_sig0 = tc_mod.copy()\n",
    "    tuning_sig0[avg_fr0<thresh_fr,:,0] = np.nan\n",
    "    tuning_sig20 = np.any(tuning_sig0>tuning_thresh,axis=1).squeeze()\n",
    "    tuning_idx0 = np.where(tuning_sig20)[0]\n",
    "    tuning_sig1 = tc_mod1.copy()\n",
    "    tuning_sig1[avg_fr1<thresh_fr,:,0] = np.nan\n",
    "    tuning_sig21 = np.any(tuning_sig1>tuning_thresh,axis=1).squeeze()\n",
    "    tuning_idx1 = np.where(tuning_sig21)[0]\n",
    "    tuning_sig = np.concatenate((tuning_sig0,tuning_sig1),axis=2)\n",
    "\n",
    "    tuning_sig_all.append(tuning_sig)\n",
    "tuning_sig_all = np.concatenate(tuning_sig_all,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [r'$\\theta$',r'$\\phi$',r'$\\rho$',r'$\\omega$']\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(8.5,1.5))\n",
    "for modeln in range(len(titles)):\n",
    "    ax = axs[modeln]\n",
    "    ax.scatter(tuning_sig_all[:,modeln,0],tuning_sig_all[:,modeln,1],s=5,c='k',edgecolors='none')\n",
    "    ax.plot(np.linspace(0,1.05,100),np.linspace(0,1.05,100),c='k',lw=1,ls='--')\n",
    "    ax.set_xticks([0,.5,1])\n",
    "    ax.set_yticks([0,.5,1])\n",
    "    ax.set_xlabel('first half',fontsize=fontsize)\n",
    "    ax.set_ylabel('second half',fontsize=fontsize)\n",
    "    ax.set_title(titles[modeln],fontsize=fontsize)\n",
    "    ax.axis('square')\n",
    "fig.savefig(paper_fig_dir/'5050_EyeHeadTuning.pdf', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5050 splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "ModelList_5050 = []\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT', '020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "for date_ani in dates_all:\n",
    "    for is_fm in [False,True]:\n",
    "        args['free_move'] = is_fm\n",
    "        if args['free_move']:\n",
    "            stim_type = 'fm1'\n",
    "        else:\n",
    "            stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "        args['save_dir']         = '~/Research/SensoryMotorPred_Data/data4/'\n",
    "        args['date_ani']         = date_ani\n",
    "        args['train_shifter']    = False\n",
    "        args['NoL1']             = False\n",
    "        args['NoL2']             = False\n",
    "        args['reg_lap']          = False\n",
    "        args['complex']          = False\n",
    "        args['complex_onoff']    = False\n",
    "        args['do_shuffle']       = False\n",
    "        args['shifter_5050']     = True\n",
    "        args['shifter_5050_run'] = False\n",
    "        args['Nepochs']          = 10000\n",
    "\n",
    "\n",
    "        params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "        params['nt_glm_lag']=5\n",
    "        if args['train_shifter']:\n",
    "            params['lag_list']     = [0]\n",
    "            params['nt_glm_lag']   = len(params['lag_list'])\n",
    "            params['Nepochs']      = 5000\n",
    "        data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "        params = get_modeltype(params)\n",
    "        model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "        mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best.h5'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "        ModelList = sorted(list(exp.save_dir.rglob(mod_name)))\n",
    "        model_info = params['date_ani2']+'_'+mod_name.replace('*', '')\n",
    "        ModelList_5050.append(ModelList)\n",
    "##### Set Train Test Splits #####\n",
    "Kfold = 0\n",
    "train_idx = train_idx_list[Kfold]\n",
    "test_idx = test_idx_list[Kfold]\n",
    "data = load_Kfold_data(data,train_idx,test_idx,params)\n",
    "locals().update(data)\n",
    "\n",
    "\n",
    "params, xtr, xtrm, xte, xtem, ytr, yte, shift_in_tr, shift_in_te, input_size, output_size, meanbias, model_move = load_GLM_data(data, params, train_idx, test_idx)\n",
    "print('Model: {}, LinMix: {}, move_features: {}, Ncells: {}, train_shifter: {}, NoL1: {}, NoL2: {}, reg_lap: {}, complex: {}'.format(\n",
    "    params['MovModel'],params['LinMix'],params['move_features'],params['Ncells'],params['train_shifter'],params['NoL1'],params['NoL2'],params['reg_lap'],params['complex']))\n",
    "ModelList_5050 = np.stack(ModelList_5050).reshape(4,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=4\n",
    "params['nt_glm_lag']=5\n",
    "date_ani2 = [dates_all[n].replace('/','_') for n in range(len(dates_all))]\n",
    "split_5050 = {date_ani2[n]:{'5050split':{}} for n in range(len(date_ani2))}\n",
    "for da, date_ani in enumerate(date_ani2):\n",
    "    GLM_Data0_HF = ioh5.load(ModelList_5050[da,0])\n",
    "    GLM_Data1_HF = ioh5.load(ModelList_5050[da,1])\n",
    "    GLM_Data0_FM = ioh5.load(ModelList_5050[da,2])\n",
    "    GLM_Data1_FM = ioh5.load(ModelList_5050[da,3])\n",
    "    RF0_HF = GLM_Data0_HF['Cell_NN.0.weight'].reshape((GLM_Data0_HF['Cell_NN.0.weight'].shape[0],params['nt_glm_lag'],)+params['nks'])\n",
    "    RF1_HF = GLM_Data1_HF['Cell_NN.0.weight'].reshape((GLM_Data1_HF['Cell_NN.0.weight'].shape[0],params['nt_glm_lag'],)+params['nks'])\n",
    "    RF0_FM = GLM_Data0_FM['Cell_NN.0.weight'].reshape((GLM_Data0_FM['Cell_NN.0.weight'].shape[0],params['nt_glm_lag'],)+params['nks'])\n",
    "    RF1_FM = GLM_Data1_FM['Cell_NN.0.weight'].reshape((GLM_Data1_FM['Cell_NN.0.weight'].shape[0],params['nt_glm_lag'],)+params['nks'])\n",
    "    RF_all_up0 = np.zeros(((2,) + RF0_HF.shape[:2] + ( sf*(RF0_HF.shape[-2]), sf*(RF0_HF.shape[-1]))))\n",
    "    RF_all_up1 = np.zeros(((2,) + RF1_HF.shape[:2] + ( sf*(RF1_HF.shape[-2]), sf*(RF1_HF.shape[-1]))))\n",
    "\n",
    "    for celln in range(RF0_HF.shape[0]):\n",
    "        for t in range(RF0_HF.shape[1]):\n",
    "            RF_all_up0[0,celln,t] = cv2.resize(RF0_HF[celln,t], (sf*(RF0_HF.shape[-1]), sf*(RF0_HF.shape[-2])))\n",
    "            RF_all_up1[0,celln,t] = cv2.resize(RF1_HF[celln,t], (sf*(RF1_HF.shape[-1]), sf*(RF1_HF.shape[-2])))\n",
    "            RF_all_up0[1,celln,t] = cv2.resize(RF0_FM[celln,t], (sf*(RF0_FM.shape[-1]), sf*(RF0_FM.shape[-2])))\n",
    "            RF_all_up1[1,celln,t] = cv2.resize(RF1_FM[celln,t], (sf*(RF1_FM.shape[-1]), sf*(RF1_FM.shape[-2])))\n",
    "    RF_CC_HF = np.zeros(RF0_HF.shape[0])\n",
    "    RF_CC_FM = np.zeros(RF0_HF.shape[0])\n",
    "    for celln in np.arange(RF0_HF.shape[0]):\n",
    "        RF_CC_HF[celln] = np.corrcoef(RF0_HF[celln,2].flatten(),RF1_HF[celln,2].flatten())[0,1]\n",
    "        RF_CC_FM[celln] = np.corrcoef(RF0_FM[celln,2].flatten(),RF1_FM[celln,2].flatten())[0,1]\n",
    "    split_5050[date_ani]['5050split']['RF0_HF'] = RF0_HF\n",
    "    split_5050[date_ani]['5050split']['RF1_HF'] = RF1_HF\n",
    "    split_5050[date_ani]['5050split']['RF0_FM'] = RF0_FM\n",
    "    split_5050[date_ani]['5050split']['RF1_FM'] = RF1_FM\n",
    "    split_5050[date_ani]['5050split']['RF_all_up0'] = RF_all_up0\n",
    "    split_5050[date_ani]['5050split']['RF_all_up1'] = RF_all_up1\n",
    "    split_5050[date_ani]['5050split']['RF_CC_HF'] = RF_CC_HF\n",
    "    split_5050[date_ani]['5050split']['RF_CC_FM'] = RF_CC_FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for da in range(len(pparams['date_ani2'])): \n",
    "    All_data[pparams['date_ani2'][da]]['5050split'] = split_5050[pparams['date_ani2'][da]]['5050split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = '5050split'\n",
    "RF_CC_HF_all = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "RF_CC_FM_all = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "RF_all_up0 = np.concatenate([split_5050[date_ani2[da]][exp_type]['RF_all_up0'] for da in range(len(dates_all))],axis=1)\n",
    "RF_all_up1 = np.concatenate([split_5050[date_ani2[da]][exp_type]['RF_all_up1'] for da in range(len(dates_all))],axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresh = .5\n",
    "frac_HF = np.sum(RF_CC_HF_all>thresh)/RF_CC_HF_all.shape[0]\n",
    "frac_FM = np.sum(RF_CC_FM_all>thresh)/RF_CC_FM_all.shape[0]\n",
    "frac_both = np.sum(((RF_CC_HF_all>thresh)&(RF_CC_FM_all>thresh)))/RF_CC_HF_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "lag_list = [0,0] #params['lag_list']\n",
    "# for da in tqdm(np.arange(len(date_ani2))):\n",
    "da = 0\n",
    "cells = [22,101,61]#[22,42,101] \n",
    "\n",
    "# fig1,axs = plt.subplots(len(cells),2*len(lag_list),figsize=(len(cells)*len(lag_list),1.5*len(cells)))\n",
    "# axs = axs.reshape(len(cells),2*len(lag_list))\n",
    "fig1 = plt.figure(constrained_layout=True, figsize=(8.2,4.25))\n",
    "gs0 = fig1.add_gridspec(nrows=3, ncols=6, wspace=.8, hspace=.9)\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=len(cells),ncols=2*len(lag_list), subplot_spec=gs0[:,:3], wspace=.05, hspace=.08)\n",
    "axs = np.array([fig1.add_subplot(gs00[n,m]) for n in range(len(cells)) for m in range(2*len(lag_list))]).reshape(len(cells),2*len(lag_list))\n",
    "\n",
    "\n",
    "for n, cell in enumerate(cells):\n",
    "    for m,lag in enumerate(lag_list):\n",
    "        if m == 0:\n",
    "            crange0_HF = np.max(np.abs(RF_all_up0[0,cell,2]))\n",
    "            crange0_FM = np.max(np.abs(RF_all_up0[1,cell,2]))\n",
    "            ax = axs[n,m]\n",
    "            im = ax.imshow(RF_all_up0[0,cell,2],'RdBu_r',vmin=-crange0_HF,vmax=crange0_HF)\n",
    "            ax.set_title('RF cc={:.03f}'.format(RF_CC_HF_all[cell]),fontsize=fontsize)\n",
    "            ax = axs[n,m+2]\n",
    "            im = ax.imshow(RF_all_up0[1,cell,2],'RdBu_r',vmin=-crange0_FM,vmax=crange0_FM)\n",
    "            ax.set_title('RF cc={:.03f}'.format(RF_CC_FM_all[cell]),fontsize=fontsize)\n",
    "        else:\n",
    "            crange1_HF = np.max(np.abs(RF_all_up1[0,cell,2]))\n",
    "            crange1_FM = np.max(np.abs(RF_all_up1[1,cell,2]))\n",
    "            ax = axs[n,m]\n",
    "            im = ax.imshow(RF_all_up1[0,cell,2],'RdBu_r',vmin=-crange1_HF,vmax=crange1_HF)\n",
    "            ax = axs[n,m+2]\n",
    "            im = ax.imshow(RF_all_up1[1,cell,2],'RdBu_r',vmin=-crange1_FM,vmax=crange1_FM)\n",
    "        # add_colorbar(im)\n",
    "    axs[n,0].set_ylabel('unit {}'.format(n+1),fontsize=fontsize)\n",
    "scalebar = AnchoredSizeBar(axs[0,0].transData,\n",
    "                        20, '10 deg', 'lower left', \n",
    "                        pad=0.1,\n",
    "                        color='black',\n",
    "                        frameon=False,\n",
    "                        size_vertical=1,\n",
    "                        )\n",
    "axs[0,0].add_artist(scalebar)\n",
    "# scale1 = ScaleBar(dx=.5, width_fraction=.05,location='lower left',scale_formatter=lambda value, unit: f'{value} deg')\n",
    "# axs[0,0].add_artist(scale1)\n",
    "\n",
    "ext = []\n",
    "for m in range(axs.shape[1]):\n",
    "    ext.append([axs[0,m].get_window_extent().x0, axs[0,m].get_window_extent().width])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "        ax.spines[axis].set_visible(True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "# axs[0,0].set_title('First half',fontsize=fontsize+2)\n",
    "# axs[0,1].set_title('Second half',fontsize=fontsize+2)\n",
    "inv = fig1.transFigure.inverted()\n",
    "width_left = ext[0][0]+(ext[1][0]+ext[1][1]-ext[0][0])/2.\n",
    "left_center = inv.transform( (width_left, 1) )\n",
    "width_right = ext[2][0]+(ext[3][0]+ext[3][1]-ext[2][0])/2.\n",
    "right_center = inv.transform( (width_right, 1) )\n",
    "\n",
    "# set column spanning title \n",
    "# the first two arguments to figtext are x and y coordinates in the figure system (0 to 1)\n",
    "# plt.figtext(left_center[0],.99,'Head-fixed', va=\"center\", ha=\"center\", size=fontsize+2)\n",
    "# plt.figtext(right_center[0],.99,'Freely moving', va=\"center\", ha=\"center\", size=fontsize+2)\n",
    "plt.figtext(.12,.99,'Head-fixed', va=\"center\", ha=\"center\", size=fontsize+2)\n",
    "plt.figtext(.34,.99,'Freely moving', va=\"center\", ha=\"center\", size=fontsize+2)\n",
    "\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=2,ncols=1, subplot_spec=gs0[:,3:], wspace=.05, hspace=.08)\n",
    "axs = np.array([fig1.add_subplot(gs01[0,0]),fig1.add_subplot(gs01[1,0])])\n",
    "\n",
    "\n",
    "thresh = .5\n",
    "frac_HF = np.sum(RF_CC_HF_all>thresh)/RF_CC_HF_all.shape[0]\n",
    "frac_FM = np.sum(RF_CC_FM_all>thresh)/RF_CC_FM_all.shape[0]\n",
    "frac_both = np.sum(((RF_CC_HF_all>thresh)&(RF_CC_FM_all>thresh)))/RF_CC_HF_all.shape[0]\n",
    "\n",
    "hbins = .02\n",
    "lim0 = -.1\n",
    "lim1 = 1\n",
    "dlim = .2\n",
    "xlab = 'cc'\n",
    "ax = axs[0]\n",
    "count_HF,edges_HF = np.histogram(RF_CC_HF_all,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_HF = np.array([(edges_HF[i]+edges_HF[i+1])/2 for i in range(len(edges_HF)-1)])\n",
    "count_FM,edges_FM = np.histogram(RF_CC_FM_all,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_FM = np.array([(edges_FM[i]+edges_FM[i+1])/2 for i in range(len(edges_FM)-1)])\n",
    "ax.bar(edges_mid_HF, count_HF/len(RF_CC_HF_all),color='k',width=hbins, alpha=.75)\n",
    "ax.bar(edges_mid_FM, count_FM/len(RF_CC_FM_all),color='r',width=hbins, alpha=.75)\n",
    "ax.set_xticks([0,.25,.5,.75,1])\n",
    "ax.set_xticklabels([0,.25,.5,.75,1],fontsize=fontsize-2)\n",
    "ax.set_xlabel(xlab,fontsize=fontsize)\n",
    "ax.axvline(x=np.nanmean(RF_CC_HF_all),lw=2, c='k',ls='--',zorder=1)\n",
    "ax.axvline(x=np.nanmean(RF_CC_FM_all),lw=2, c='r',ls='--',zorder=1)\n",
    "ax.set_ylabel('fraction of units',fontsize=fontsize)\n",
    "ax.set_yticks([0,.03,.06])\n",
    "ax.set_yticklabels([0,.03,.06],fontsize=fontsize-2)    \n",
    "legend1 = ax.legend(['HF','FM'],labelcolor=['k','r'],fontsize=fontsize,ncol=2, markerscale=0, handlelength=0, handletextpad=-1.5,loc=\"upper left\",frameon=False, bbox_to_anchor=(.1, 1.1))\n",
    "\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar([0,1,2], [frac_HF, frac_FM, frac_both],color='k')\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(['HF', 'FM', 'Both'],fontsize=fontsize)\n",
    "ax.set_ylabel('fraction of neurons',fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(0,.81,.4))\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig1.savefig(paper_fig_dir/('5050_RF_Comparison.pdf'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of HF and FM cc distributions\n",
    "np.nanmean(RF_CC_HF_all),np.nanmean(RF_CC_FM_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .5\n",
    "frac_HF = np.sum(RF_CC_HF_all>thresh)/RF_CC_HF_all.shape[0]\n",
    "frac_FM = np.sum(RF_CC_FM_all>thresh)/RF_CC_FM_all.shape[0]\n",
    "frac_both = np.sum(((RF_CC_HF_all>thresh)&(RF_CC_FM_all>thresh)))/RF_CC_HF_all.shape[0]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(2,2))\n",
    "ax = axs\n",
    "ax.bar([0,1,2], [frac_HF, frac_FM, frac_both],color='k')\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(['HF', 'FM', 'Both'],fontsize=fontsize)\n",
    "ax.set_ylabel('fraction of neurons',fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(0,.81,.4))\n",
    "\n",
    "fig1.savefig(paper_fig_dir/('fraction_significant.pdf'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hfFM_complex_lin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_on_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_complex_nlin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_off_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_simple =  np.hstack([All_data[pparams['date_ani2'][da]]['CropInputs']['HF_FMHF_cc'] for da in range(len(dates_all))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_name = paper_fig_dir/ ('ComplexRFs_onoff_T01.pdf')\n",
    "exp_type = 'complex'\n",
    "FM_RF_all = np.concatenate([All_data[pparams['date_ani2'][da]][exp_type]['Vis_rf_all']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "HF_RF_all = np.concatenate([All_data[pparams['date_ani2'][da]][exp_type]['HF_rf_all']  for da in range(len(pparams['dates_all']))],axis=0)\n",
    "vals_Vis = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Vis_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_HF = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['HF_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "# cell = 101\n",
    "params['nt_glm_lag'] = 1\n",
    "lag_list = 2*[0] #2*[-100,-50,0,50,100] \n",
    "pdf = PdfPages(pdf_name)\n",
    "# cell_list = [22,117,101,64]\n",
    "for n, cell in enumerate(range(FM_RF_all.shape[0])):\n",
    "# for n, cell in enumerate(tqdm(cell_list)):#range(FM_RF_all.shape[0]))):\n",
    "    fig, axs = plt.subplots(2,params['nt_glm_lag']*2,figsize=(2*params['nt_glm_lag'],2))\n",
    "    for m in range(2*params['nt_glm_lag']):\n",
    "        crange2 = np.max(np.abs(FM_RF_all[cell])) \n",
    "        ax = axs[0,m]\n",
    "        im2 = ax.imshow(FM_RF_all[cell, m],'RdBu_r', vmin=-crange2, vmax=crange2)\n",
    "        ax.set_title('{} ms'.format(lag_list[m]),fontsize=fontsize)\n",
    "        crange1 = np.max(np.abs(HF_RF_all[cell])) \n",
    "        ax = axs[1,m]\n",
    "        im1 = ax.imshow(HF_RF_all[cell, m], 'RdBu_r', vmin=-crange1, vmax=crange1) \n",
    "        ax.set_title('{} ms'.format(lag_list[m]),fontsize=fontsize)\n",
    "        if m == 9:\n",
    "            cbar2 = add_colorbar(im2)\n",
    "            cbar1 = add_colorbar(im1)\n",
    "            cbar2.outline.set_linewidth(1)\n",
    "            cbar1.outline.set_linewidth(1)\n",
    "\n",
    "\n",
    "    plt.suptitle('unit: {} FMcc={:.03f} HFcc={:.03f} '.format(cell,vals_Vis[cell],vals_HF[cell]), fontsize=fontsize).set_multialignment('center')\n",
    "    \n",
    "    axs[1,0].set_ylabel('head- \\n fixed', fontsize=fontsize)\n",
    "    axs[0,0].set_ylabel('freely \\n moving', fontsize=fontsize)\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(0.5)\n",
    "            ax.spines[axis].set_visible(True)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])   \n",
    "\n",
    "    plt.tight_layout()\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex 5050 Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "ModelList_5050 = []\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT', '020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "for date_ani in dates_all:\n",
    "    for is_fm in [False,True]:\n",
    "        args['free_move'] = is_fm\n",
    "        if args['free_move']:\n",
    "            stim_type = 'fm1'\n",
    "        else:\n",
    "            stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "        args['save_dir']         = '~/Research/SensoryMotorPred_Data/data4/'\n",
    "        args['date_ani']         = date_ani\n",
    "        args['train_shifter']    = False\n",
    "        args['NoL1']             = False\n",
    "        args['NoL2']             = False\n",
    "        args['reg_lap']          = False\n",
    "        args['complex']          = False\n",
    "        args['complex_onoff']    = True\n",
    "        args['do_shuffle']       = False\n",
    "        args['shifter_5050']     = True\n",
    "        args['shifter_5050_run'] = False\n",
    "        args['Nepochs']          = 10000\n",
    "\n",
    "\n",
    "        params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "        params['nt_glm_lag']=1\n",
    "        if args['train_shifter']:\n",
    "            params['lag_list']     = [0]\n",
    "            params['nt_glm_lag']   = len(params['lag_list'])\n",
    "            params['Nepochs']      = 5000\n",
    "        data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "        params = get_modeltype(params)\n",
    "        model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "        mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best.h5'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "        ModelList = sorted(list(exp.save_dir.rglob(mod_name)))\n",
    "        model_info = params['date_ani2']+'_'+mod_name.replace('*', '')\n",
    "        ModelList_5050.append(ModelList)\n",
    "##### Set Train Test Splits #####\n",
    "Kfold = 0\n",
    "train_idx = train_idx_list[Kfold]\n",
    "test_idx = test_idx_list[Kfold]\n",
    "data = load_Kfold_data(data,train_idx,test_idx,params)\n",
    "locals().update(data)\n",
    "\n",
    "\n",
    "params, xtr, xtrm, xte, xtem, ytr, yte, shift_in_tr, shift_in_te, input_size, output_size, meanbias, model_move = load_GLM_data(data, params, train_idx, test_idx)\n",
    "print('Model: {}, LinMix: {}, move_features: {}, Ncells: {}, train_shifter: {}, NoL1: {}, NoL2: {}, reg_lap: {}, complex: {}'.format(\n",
    "    params['MovModel'],params['LinMix'],params['move_features'],params['Ncells'],params['train_shifter'],params['NoL1'],params['NoL2'],params['reg_lap'],params['complex']))\n",
    "ModelList_5050 = np.stack(ModelList_5050).reshape(4,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf=4\n",
    "params['nt_glm_lag']=1\n",
    "date_ani2 = [dates_all[n].replace('/','_') for n in range(len(dates_all))]\n",
    "split_5050 = {date_ani2[n]:{'5050split':{}} for n in range(len(date_ani2))}\n",
    "for da, date_ani in enumerate(date_ani2):\n",
    "    GLM_Data0_HF = ioh5.load(ModelList_5050[da,0])\n",
    "    GLM_Data1_HF = ioh5.load(ModelList_5050[da,1])\n",
    "    GLM_Data0_FM = ioh5.load(ModelList_5050[da,2])\n",
    "    GLM_Data1_FM = ioh5.load(ModelList_5050[da,3])\n",
    "    RF0_HF = GLM_Data0_HF['Cell_NN.0.weight'].reshape((GLM_Data0_HF['Cell_NN.0.weight'].shape[0],2*params['nt_glm_lag'],)+params['nks'])\n",
    "    RF1_HF = GLM_Data1_HF['Cell_NN.0.weight'].reshape((GLM_Data1_HF['Cell_NN.0.weight'].shape[0],2*params['nt_glm_lag'],)+params['nks'])\n",
    "    RF0_FM = GLM_Data0_FM['Cell_NN.0.weight'].reshape((GLM_Data0_FM['Cell_NN.0.weight'].shape[0],2*params['nt_glm_lag'],)+params['nks'])\n",
    "    RF1_FM = GLM_Data1_FM['Cell_NN.0.weight'].reshape((GLM_Data1_FM['Cell_NN.0.weight'].shape[0],2*params['nt_glm_lag'],)+params['nks'])\n",
    "    RF_HF = np.concatenate(([RF0_HF[:,:1],RF1_HF[:,:1],RF0_HF[:,1:],RF1_HF[:,1:]]),axis=1)\n",
    "    RF_FM = np.concatenate(([RF0_FM[:,:1],RF1_FM[:,:1],RF0_FM[:,1:],RF1_FM[:,1:]]),axis=1)\n",
    "    RF_all_up = np.zeros(((2,) + RF_HF.shape[:2] + ( sf*(RF_HF.shape[-2]), sf*(RF_HF.shape[-1]))))\n",
    "    for celln in range(RF_FM.shape[0]):\n",
    "        for t in range(RF_FM.shape[1]):\n",
    "            RF_all_up[0,celln,t] = cv2.resize(RF_HF[celln,t], (sf*(RF_HF.shape[-1]), sf*(RF_HF.shape[-2])))\n",
    "            RF_all_up[1,celln,t] = cv2.resize(RF_FM[celln,t], (sf*(RF_FM.shape[-1]), sf*(RF_FM.shape[-2])))\n",
    "\n",
    "    RF_CC_HF = np.zeros((RF0_HF.shape[0]))\n",
    "    RF_CC_FM = np.zeros((RF0_HF.shape[0]))\n",
    "    \n",
    "    for celln in np.arange(RF0_HF.shape[0]):\n",
    "        RF_CC_HF[celln] = np.corrcoef(RF0_HF[celln].flatten(),RF1_HF[celln].flatten())[0,1]\n",
    "        RF_CC_FM[celln] = np.corrcoef(RF0_FM[celln].flatten(),RF1_FM[celln].flatten())[0,1]\n",
    "        # RF_CC_HF[celln,1] = np.corrcoef(RF0_HF[celln,1].flatten(),RF1_HF[celln,1].flatten())[0,1]\n",
    "        # RF_CC_FM[celln,0] = np.corrcoef(RF0_FM[celln,0].flatten(),RF1_FM[celln,0].flatten())[0,1]\n",
    "    split_5050[date_ani]['5050split']['RF_HF'] = RF_HF\n",
    "    split_5050[date_ani]['5050split']['RF_FM'] = RF_FM\n",
    "    split_5050[date_ani]['5050split']['RF_CC_HF'] = RF_CC_HF\n",
    "    split_5050[date_ani]['5050split']['RF_CC_FM'] = RF_CC_FM\n",
    "    split_5050[date_ani]['5050split']['RF_all_up'] = RF_all_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = '5050split'\n",
    "RF_CC_HF_all = np.hstack([All_data_T1[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "RF_CC_FM_all = np.hstack([All_data_T1[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "RF_all_up = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['RF_all_up'] for da in range(len(dates_all))],axis=1)\n",
    "\n",
    "\n",
    "thresh = .5\n",
    "frac_HF = np.sum(RF_CC_HF_all>thresh,axis=0)/RF_CC_HF_all.shape[0]\n",
    "frac_FM = np.sum(RF_CC_FM_all>thresh,axis=0)/RF_CC_FM_all.shape[0]\n",
    "frac_both = np.sum(((RF_CC_HF_all>thresh)&(RF_CC_FM_all>thresh)),axis=0)/RF_CC_HF_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "lag_list = RF_all_up.shape[2]# [0,0] #params['lag_list']\n",
    "# for da in tqdm(np.arange(len(date_ani2))):\n",
    "da = 0\n",
    "cells = [247,3,41]#,101,61]# ,101,61]#[22,42,101] \n",
    "\n",
    "# fig1,axs = plt.subplots(len(cells),2*len(lag_list),figsize=(len(cells)*len(lag_list),1.5*len(cells)))\n",
    "# axs = axs.reshape(len(cells),2*len(lag_list))\n",
    "fig1 = plt.figure(constrained_layout=False, figsize=(12,6))\n",
    "gs0 = fig1.add_gridspec(nrows=3, ncols=9, wspace=2.5, hspace=.9)\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=2*len(cells),ncols=lag_list, subplot_spec=gs0[:,:3], wspace=.05, hspace=.08)\n",
    "axs = np.array([fig1.add_subplot(gs00[n,m]) for n in range(2*len(cells)) for m in range(lag_list)]).reshape((2*len(cells)//2),2,lag_list)\n",
    "\n",
    "HF_FM = ['HF','FM']\n",
    "on_off = ['L0','L1','NL0','NL1']\n",
    "for n, cell in enumerate(cells):\n",
    "    for j, cond in enumerate(HF_FM):\n",
    "        for m,lag in enumerate(range(lag_list)):\n",
    "            crange0_HF = np.max(np.abs(RF_all_up[j,cell]))\n",
    "            ax = axs[n,j,m]\n",
    "            im = ax.imshow(RF_all_up[j,cell,m],'RdBu_r',vmin=-crange0_HF,vmax=crange0_HF)\n",
    "            ax.set_title(on_off[m],fontsize=fontsize)\n",
    "\n",
    "\n",
    "        axs[n,j,0].set_ylabel('{}'.format(HF_FM[j]),fontsize=fontsize)\n",
    "\n",
    "scalebar = AnchoredSizeBar(axs[0,0,0].transData,\n",
    "                        20, '10 deg', 'lower left', \n",
    "                        pad=0.1,\n",
    "                        color='black',\n",
    "                        frameon=False,\n",
    "                        size_vertical=1,\n",
    "                        )\n",
    "axs[0,0,0].add_artist(scalebar)\n",
    "\n",
    "ext = []\n",
    "for m in range(axs.shape[1]):\n",
    "    ext.append([axs[0,0,m].get_window_extent().x0, axs[0,0,m].get_window_extent().width])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "        ax.spines[axis].set_visible(True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(nrows=2,ncols=1, subplot_spec=gs0[:,3:6], wspace=.05, hspace=.4)\n",
    "axs = np.array([fig1.add_subplot(gs01[0,0]),fig1.add_subplot(gs01[1,0])])\n",
    "\n",
    "\n",
    "thresh = .5\n",
    "frac_HF = np.sum(RF_CC_HF_all>thresh,axis=0)/RF_CC_HF_all.shape[0]\n",
    "frac_FM = np.sum(RF_CC_FM_all>thresh,axis=0)/RF_CC_FM_all.shape[0]\n",
    "frac_both = np.sum(((RF_CC_HF_all>thresh)&(RF_CC_FM_all>thresh)),axis=0)/RF_CC_HF_all.shape[0]\n",
    "\n",
    "hbins = .05\n",
    "lim0 = -.1\n",
    "lim1 = 1\n",
    "dlim = .2\n",
    "xlab = 'cc'\n",
    "ax = axs[0]\n",
    "count_HF,edges_HF = np.histogram(RF_CC_HF_all,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_HF = np.array([(edges_HF[i]+edges_HF[i+1])/2 for i in range(len(edges_HF)-1)])\n",
    "count_FM,edges_FM = np.histogram(RF_CC_FM_all,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_FM = np.array([(edges_FM[i]+edges_FM[i+1])/2 for i in range(len(edges_FM)-1)])\n",
    "ax.bar(edges_mid_HF, count_HF/len(RF_CC_HF_all),color='k',width=hbins, alpha=.75)\n",
    "ax.bar(edges_mid_FM, count_FM/len(RF_CC_FM_all),color='r',width=hbins, alpha=.75)\n",
    "\n",
    "# ax.set_xticks([0,.25,.5,.75,1])\n",
    "# ax.set_xticklabels([0,.25,.5,.75,1],fontsize=fontsize-2)\n",
    "ax.set_xlabel(xlab,fontsize=fontsize)\n",
    "ax.axvline(x=np.nanmean(RF_CC_HF_all),lw=2, c='k',ls='--',zorder=1)\n",
    "ax.axvline(x=np.nanmean(RF_CC_FM_all),lw=2, c='r',ls='--',zorder=1)\n",
    "ax.set_ylabel('fraction of units',fontsize=fontsize)\n",
    "# ax.set_yticks([0,.03,.06])\n",
    "# ax.set_yticklabels([0,.03,.06],fontsize=fontsize-2)    \n",
    "legend1 = ax.legend(['HF','FM'],labelcolor=['k','r'],fontsize=fontsize,ncol=2, markerscale=0, handlelength=0, handletextpad=-1.5,loc=\"upper left\",frameon=False, bbox_to_anchor=(.1, 1.1))\n",
    "\n",
    "\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar(np.arange(3),np.stack([frac_HF, frac_FM, frac_both]),color='k')\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(['HF', 'FM', 'Both'],fontsize=fontsize)\n",
    "ax.set_ylabel('fraction of neurons',fontsize=fontsize)\n",
    "ax.set_yticks(np.arange(0,.81,.4))\n",
    "\n",
    "\n",
    "thresh=.5\n",
    "exp_type = '5050split'\n",
    "RF_CC_HF_all_c = np.hstack([All_data_T1[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "RF_CC_FM_all_c = np.hstack([All_data_T1[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "RF_CC_HF_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_HF'] for da in range(len(pparams['dates_all']))])\n",
    "RF_CC_FM_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_FM'] for da in range(len(pparams['dates_all']))])\n",
    "all_hfFM_complex_lin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_on_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_complex_nlin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_off_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_simple =  np.hstack([All_data[pparams['date_ani2'][da]]['CropInputs']['HF_FMHF_cc'] for da in range(len(dates_all))])\n",
    "\n",
    "gs02 = gridspec.GridSpecFromSubplotSpec(nrows=2,ncols=1, subplot_spec=gs0[:,6:], wspace=.05, hspace=.4)\n",
    "axs = np.array([fig1.add_subplot(gs02[0,0]),fig1.add_subplot(gs02[1,0])])\n",
    "ax = axs[0]\n",
    "lim0=-.5\n",
    "lim1=.75\n",
    "hbins=.05\n",
    "thresh_complex = ((RF_CC_HF_all_c>thresh)&(RF_CC_FM_all_c>thresh))\n",
    "thresh_simple = ((RF_CC_HF_all_s>thresh)&(RF_CC_FM_all_s>thresh))\n",
    "count_S,edges_S = np.histogram(all_hfFM_simple[thresh_simple],bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_S = np.array([(edges_S[i]+edges_S[i+1])/2 for i in range(len(edges_S)-1)])\n",
    "count_CL,edges_CL = np.histogram(all_hfFM_complex_lin[thresh_complex],bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_CL = np.array([(edges_CL[i]+edges_CL[i+1])/2 for i in range(len(edges_CL)-1)])\n",
    "count_CNL,edges_CNL = np.histogram(all_hfFM_complex_nlin[thresh_complex],bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_CNL = np.array([(edges_CNL[i]+edges_CNL[i+1])/2 for i in range(len(edges_CNL)-1)])\n",
    "ax.bar(edges_mid_S, count_S/len(all_hfFM_simple[thresh_simple]),color='k',width=hbins, alpha=.75,label='Simple')\n",
    "ax.bar(edges_mid_CL, count_CL/len(all_hfFM_complex_lin[thresh_complex]),color='r',width=hbins, alpha=.75,label='Complex Lin')\n",
    "ax.bar(edges_mid_CNL, count_CNL/len(all_hfFM_complex_nlin[thresh_complex]),color='b',width=hbins, alpha=.75,label='Complex NLin')\n",
    "\n",
    "ax.axvline(np.nanmean(all_hfFM_simple[thresh_simple]),ls='--',color='k')\n",
    "ax.axvline(np.nanmean(all_hfFM_complex_lin[thresh_complex]),ls='--',color='r')\n",
    "ax.axvline(np.nanmean(all_hfFM_complex_nlin[thresh_complex]),ls='--',color='b')\n",
    "ax.set_xlabel('HF_FM cc',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize,loc='upper right',labelcolor='linecolor', handlelength=0, handletextpad=0,bbox_to_anchor=(1.2, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig1.savefig(paper_fig_dir/('5050_RF_Comparison_Complex.jpg'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(all_hfFM_complex_nlin>.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = '5050split'\n",
    "RF_CC_HF_all_c = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "RF_CC_FM_all_c = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "RF_all_up = np.concatenate([split_5050[date_ani2[da]][exp_type]['RF_all_up'] for da in range(len(dates_all))],axis=1)\n",
    "\n",
    "RF_CC_HF_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_HF'] for da in range(len(pparams['dates_all']))])\n",
    "RF_CC_FM_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_FM'] for da in range(len(pparams['dates_all']))])\n",
    "thresh=.5\n",
    "thresh_simple = ((RF_CC_HF_all_s>thresh)&(RF_CC_FM_all_s>thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'complex'\n",
    "thresh=.5\n",
    "thresh_complex = ((RF_CC_HF_all_c>thresh)&(RF_CC_FM_all_c>thresh))\n",
    "thresh_simple = ((RF_CC_HF_all_s>thresh)&(RF_CC_FM_all_s>thresh))\n",
    "all_hfFM_complex_lin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_on_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_complex_nlin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex']['HF_FMHF_off_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_simple =  np.hstack([All_data[pparams['date_ani2'][da]]['CropInputs']['HF_FMHF_cc'] for da in range(len(dates_all))])\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "ax = axs\n",
    "ax.hist(all_hfFM_simple[thresh_simple],bins=20,color='k',alpha=.75,label='Simple')\n",
    "ax.hist(all_hfFM_complex_lin[thresh_complex],bins=20,color='r',alpha=.75,label='Complex Lin')\n",
    "ax.hist(all_hfFM_complex_nlin[thresh_complex],bins=20,color='b',alpha=.75,label='Complex NLin')\n",
    "ax.axvline(np.nanmean(all_hfFM_simple[thresh_simple]),ls='--',color='k')\n",
    "ax.axvline(np.nanmean(all_hfFM_complex_lin[thresh_complex]),ls='--',color='r')\n",
    "ax.axvline(np.nanmean(all_hfFM_complex_nlin[thresh_complex]),ls='--',color='b')\n",
    "ax.set_xlabel('HF_FM cc',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize)\n",
    "fig.savefig(paper_fig_dir/('HF_FM_CC_with_TRT.jpg'), transparent=True, bbox_inches='tight',dpi=300)\n",
    "print('simple:',np.nanmean(all_hfFM_simple[thresh_simple]),'complex lin:',np.nanmean(all_hfFM_complex_lin[thresh_complex]),'complex Nlin:',np.nanmean(all_hfFM_complex_nlin[thresh_complex]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Reviewer Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_CC_HF_all = np.hstack([All_data[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "# RF_CC_FM_all = np.hstack([All_data[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "exp_type='CropInputs'\n",
    "Vis_RF_all_up_simp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['Vis_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "HF_RF_all_up_simp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['HF_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "exp_type = 'complex_onoff'\n",
    "Vis_RF_all_up_comp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['Vis_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "HF_RF_all_up_comp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['HF_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "Vis_RF_all_up =  np.concatenate((Vis_RF_all_up_simp,Vis_RF_all_up_comp),axis=1)\n",
    "HF_RF_all_up =  np.concatenate((HF_RF_all_up_simp,HF_RF_all_up_comp),axis=1)\n",
    "RF_all_up = np.stack((HF_RF_all_up,Vis_RF_all_up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_all_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data[pparams['date_ani2'][da]]['complex_onoff'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_list = 3\n",
    "da = 0\n",
    "cells = [247,3,41]#,101,61]# ,101,61]#[22,42,101] \n",
    "\n",
    "\n",
    "fig1 = plt.figure(constrained_layout=False, figsize=(8,6))\n",
    "gs0 = fig1.add_gridspec(nrows=3, ncols=6, wspace=2.5, hspace=.5)\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=2*len(cells),ncols=lag_list, subplot_spec=gs0[:,:3], wspace=.1, hspace=.02)\n",
    "axs = np.array([fig1.add_subplot(gs00[n,m]) for n in range(2*len(cells)) for m in range(lag_list)]).reshape((2*len(cells)),lag_list)\n",
    "\n",
    "HF_FM = ['HF','FM']\n",
    "# on_off = ['Lin Only','C_lin','C_Nlin']\n",
    "on_off = ['Lin Only','C_On','C_Off']\n",
    "n = 0\n",
    "for _, cell in enumerate(cells):\n",
    "    for m,lag in enumerate(range(lag_list)):\n",
    "        if m == 0:\n",
    "            crange0_HF = np.max(np.abs(RF_all_up[0,cell,m]))\n",
    "            crange0_FM = np.max(np.abs(RF_all_up[1,cell,m]))\n",
    "        else: \n",
    "            crange0_HF = np.max(np.abs(RF_all_up[0,cell,1:]))\n",
    "            crange0_FM = np.max(np.abs(RF_all_up[1,cell,1:]))\n",
    "        ax = axs[n,m]\n",
    "        im = ax.imshow(RF_all_up[0,cell,m],'RdBu_r',vmin=-crange0_HF,vmax=crange0_HF)\n",
    "        ax.set_title(on_off[m],fontsize=fontsize)\n",
    "        axs[n,0].set_ylabel('{}'.format(HF_FM[n%2]),fontsize=fontsize)\n",
    "        crange0_HF = np.max(np.abs(RF_all_up[1,cell]))\n",
    "        ax = axs[n+1,m]\n",
    "        im = ax.imshow(RF_all_up[1,cell,m],'RdBu_r',vmin=-crange0_FM,vmax=crange0_FM)\n",
    "        axs[n+1,0].set_ylabel('{}'.format(HF_FM[(n+1)%2]),fontsize=fontsize)\n",
    "    n = n+2\n",
    "\n",
    "scalebar = AnchoredSizeBar(axs[0,0].transData,\n",
    "                        20, '10 deg', 'lower left', \n",
    "                        pad=0.1,\n",
    "                        color='black',\n",
    "                        frameon=False,\n",
    "                        size_vertical=1,\n",
    "                        )\n",
    "axs[0,0].add_artist(scalebar)\n",
    "\n",
    "ext = []\n",
    "for m in range(axs.shape[1]):\n",
    "    ext.append([axs[0,m].get_window_extent().x0, axs[0,m].get_window_extent().width])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "        ax.spines[axis].set_visible(True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "gs02 = gridspec.GridSpecFromSubplotSpec(nrows=1,ncols=1, subplot_spec=gs0[:1,3:], wspace=.05, hspace=.4)\n",
    "axs = np.array([fig1.add_subplot(gs02[0,0])])\n",
    "\n",
    "thresh=.5\n",
    "exp_type = '5050split'\n",
    "RF_CC_HF_all_c = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_HF'] for da in range(len(dates_all))])\n",
    "RF_CC_FM_all_c = np.hstack([split_5050[date_ani2[da]][exp_type]['RF_CC_FM'] for da in range(len(dates_all))])\n",
    "RF_all_up = np.concatenate([split_5050[date_ani2[da]][exp_type]['RF_all_up'] for da in range(len(dates_all))],axis=1)\n",
    "\n",
    "RF_CC_HF_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_HF'] for da in range(len(pparams['dates_all']))])\n",
    "RF_CC_FM_all_s = np.hstack([All_data[pparams['date_ani2'][da]]['5050split']['RF_CC_FM'] for da in range(len(pparams['dates_all']))])\n",
    "thresh_complex = ((RF_CC_HF_all_c>thresh)&(RF_CC_FM_all_c>thresh))\n",
    "thresh_simple = ((RF_CC_HF_all_s>thresh)&(RF_CC_FM_all_s>thresh))\n",
    "# all_hfFM_complex_lin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex_onoff']['HF_FMHF_on_cc'] for da in range(len(dates_all))])\n",
    "# all_hfFM_complex_nlin =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex_onoff']['HF_FMHF_off_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_complex_onoff =  np.hstack([All_data_T1[pparams['date_ani2'][da]]['complex_onoff']['HF_FMHF_cc'] for da in range(len(dates_all))])\n",
    "all_hfFM_simple =  np.hstack([All_data[pparams['date_ani2'][da]]['CropInputs']['HF_FMHF_cc'] for da in range(len(dates_all))])\n",
    "\n",
    "ax = axs[0]\n",
    "lim0=-.5\n",
    "lim1=.75\n",
    "hbins=.1\n",
    "thresh_complex = ((RF_CC_HF_all_c>thresh)&(RF_CC_FM_all_c>thresh))\n",
    "thresh_simple = ((RF_CC_HF_all_s>thresh)&(RF_CC_FM_all_s>thresh))\n",
    "count_S,edges_S = np.histogram(all_hfFM_simple[thresh_simple],bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_S = np.array([(edges_S[i]+edges_S[i+1])/2 for i in range(len(edges_S)-1)])\n",
    "# count_CL,edges_CL = np.histogram(all_hfFM_complex_lin[thresh_complex],bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid_CL = np.array([(edges_CL[i]+edges_CL[i+1])/2 for i in range(len(edges_CL)-1)])\n",
    "# count_CNL,edges_CNL = np.histogram(all_hfFM_complex_nlin[thresh_complex],bins=np.arange(lim0,lim1,hbins))\n",
    "# edges_mid_CNL = np.array([(edges_CNL[i]+edges_CNL[i+1])/2 for i in range(len(edges_CNL)-1)])\n",
    "count_CNL,edges_CNL = np.histogram(all_hfFM_complex_onoff[thresh_complex],bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid_CNL = np.array([(edges_CNL[i]+edges_CNL[i+1])/2 for i in range(len(edges_CNL)-1)])\n",
    "ax.bar(edges_mid_S, count_S/len(all_hfFM_simple[thresh_simple]),color='k',width=hbins, alpha=.75,label='Simple')\n",
    "# ax.bar(edges_mid_CL, count_CL/len(all_hfFM_complex_lin[thresh_complex]),color='r',width=hbins, alpha=.75,label='Complex Lin')\n",
    "ax.bar(edges_mid_CNL, count_CNL/len(all_hfFM_complex_onoff[thresh_complex]),color='r',width=hbins, alpha=.75,label='Complex NLin')\n",
    "\n",
    "ax.axvline(np.nanmean(all_hfFM_simple[thresh_simple]),ls='--',color='k')\n",
    "# ax.axvline(np.nanmean(all_hfFM_complex_lin[thresh_complex]),ls='--',color='r')\n",
    "ax.axvline(np.nanmean(all_hfFM_complex_onoff[thresh_complex]),ls='--',color='#6D0000')\n",
    "ax.set_xlabel('HF_FM cc',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize,loc='upper right',labelcolor='linecolor', handlelength=0, handletextpad=0,bbox_to_anchor=(1.2, 1))\n",
    "ax.set_ylabel('fraction of units',fontsize=fontsize)\n",
    "\n",
    "cells = [101,61]\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=2*len(cells),ncols=lag_list, subplot_spec=gs0[1:,3:], wspace=.1, hspace=.02)\n",
    "axs = np.array([fig1.add_subplot(gs00[n,m]) for n in range(2*len(cells)) for m in range(lag_list)]).reshape((2*len(cells)),lag_list)\n",
    "\n",
    "exp_type='CropInputs'\n",
    "Vis_RF_all_up_simp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['Vis_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "HF_RF_all_up_simp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['HF_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "exp_type = 'complex_onoff'\n",
    "Vis_RF_all_up_comp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['Vis_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "HF_RF_all_up_comp = np.concatenate([All_data_T1[date_ani2[da]][exp_type]['HF_rf_up'] for da in range(len(dates_all))],axis=0)\n",
    "Vis_RF_all_up =  np.concatenate((Vis_RF_all_up_simp,Vis_RF_all_up_comp),axis=1)\n",
    "HF_RF_all_up =  np.concatenate((HF_RF_all_up_simp,HF_RF_all_up_comp),axis=1)\n",
    "RF_all_up = np.stack((HF_RF_all_up,Vis_RF_all_up))\n",
    "\n",
    "HF_FM = ['HF','FM']\n",
    "n = 0\n",
    "for _, cell in enumerate(cells):\n",
    "    for m,lag in enumerate(range(lag_list)):\n",
    "        if m == 0:\n",
    "            crange0_HF = np.max(np.abs(RF_all_up[0,cell,m]))\n",
    "            crange0_FM = np.max(np.abs(RF_all_up[1,cell,m]))\n",
    "        else: \n",
    "            crange0_HF = np.max(np.abs(RF_all_up[0,cell,1:]))\n",
    "            crange0_FM = np.max(np.abs(RF_all_up[1,cell,1:]))\n",
    "        ax = axs[n,m]\n",
    "        im = ax.imshow(RF_all_up[0,cell,m],'RdBu_r',vmin=-crange0_HF,vmax=crange0_HF)\n",
    "        ax.set_title(on_off[m],fontsize=fontsize)\n",
    "        axs[n,0].set_ylabel('{}'.format(HF_FM[n%2]),fontsize=fontsize)\n",
    "        crange0_HF = np.max(np.abs(RF_all_up[1,cell]))\n",
    "        ax = axs[n+1,m]\n",
    "        im = ax.imshow(RF_all_up[1,cell,m],'RdBu_r',vmin=-crange0_FM,vmax=crange0_FM)\n",
    "        axs[n+1,0].set_ylabel('{}'.format(HF_FM[(n+1)%2]),fontsize=fontsize)\n",
    "    n = n+2\n",
    "\n",
    "\n",
    "ext = []\n",
    "for m in range(axs.shape[1]):\n",
    "    ext.append([axs[0,m].get_window_extent().x0, axs[0,m].get_window_extent().width])\n",
    "\n",
    "for ax in axs.flat:\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "        ax.spines[axis].set_visible(True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig1.savefig(paper_fig_dir/('Reviewer_ComplexOnOff_Fig.pdf'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data_T1[pparams['data_ani2'][0]]['CropInputs'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darkness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "args['free_move'] = True\n",
    "args['fm_dark'] = False\n",
    "if args['free_move']:\n",
    "    stim_type = 'fm1'\n",
    "elif args['fm_dark']:\n",
    "    stim_type = 'fm1_dark'\n",
    "else:\n",
    "    stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "dates_all = ['100821/J559TT', '101621/J559NC', '102721/J558NC', '110421/J558LT','110521/J569LT']\n",
    "args['data_dir'] = '~/Goeppert/nlab-nas/Dylan/freely_moving_ephys/ephys_recordings/'\n",
    "args['save_dir'] = '~/Research/SensoryMotorPred_Data/data2/'\n",
    "args['date_ani'] = dates_all[0]\n",
    "args['train_shifter']=True\n",
    "args['NoL1'] = False\n",
    "args['NoL2'] = False\n",
    "args['complex'] = False\n",
    "args['reg_lap'] = False\n",
    "args['do_shuffle']=False\n",
    "args['Nepochs'] = 10000\n",
    "\n",
    "\n",
    "params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "\n",
    "data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "params = get_modeltype(params)\n",
    "model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "ModelList = sorted(list(exp.save_dir.rglob(mod_name+'.h5')))\n",
    "model_info = params['date_ani2']+'_'+mod_name.replace('*', '')\n",
    "\n",
    "##### Set Train Test Splits #####\n",
    "Kfold = 0\n",
    "train_idx = train_idx_list[Kfold]\n",
    "test_idx = test_idx_list[Kfold]\n",
    "data = load_Kfold_data(data,train_idx,test_idx,params)\n",
    "locals().update(data)\n",
    "\n",
    "\n",
    "params, xtr, xtrm, xte, xtem, ytr, yte, shift_in_tr, shift_in_te, input_size, output_size, meanbias, model_move = load_GLM_data(data, params, train_idx, test_idx)\n",
    "print('Model: {}, LinMix: {}, move_features: {}, Ncells: {}, train_shifter: {}, NoL1: {}, NoL2: {}, reg_lap: {}, comples: {}'.format(\n",
    "    params['MovModel'],params['LinMix'],params['move_features'],params['Ncells'],params['train_shifter'],params['NoL1'],params['NoL2'],params['reg_lap'],params['complex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# celltype_dir = params['data_dir'].parent.parent.parent.parent / 'batch_files/120221_hffm'\n",
    "# celltype_file = celltype_dir / 'pooled_ephys_population_update_120621.pickle'\n",
    "# celltype = pickle.load(open(celltype_file,'rb'))\n",
    "\n",
    "Kfold = 0\n",
    "args['free_move']=True\n",
    "mod_titles = ['pos','vis','add','mul','HF']\n",
    "mod_clrs = [\"#f1c40f\",\"#06d6a0\",\"#118ab2\",\"#ef476f\",\"#073b4c\"]\n",
    "dates_all =  ['100821/J559TT', '101621/J559NC', '102721/J558NC', '110421/J558LT','110521/J569LT']\n",
    "ModelList_all,test_std,tuning_sig_all,tuning_sig_all2,NCells_all,bad_cells_all,test_nsp_all,hf_nsp_all,model_move_FM,model_move_HF = [],[], [], [], [], [], [], [], [], []\n",
    "for d,date_ani in enumerate(dates_all):\n",
    "    tuning_sig1,tuning_sig2a,NCells1,bad_cells1,test_nsp1,model_move1= [], [], [], [], [], [],\n",
    "    for fm_dark in [False,True]:\n",
    "        args['date_ani']    = date_ani\n",
    "        args['data_dir'] = '~/Goeppert/nlab-nas/Dylan/freely_moving_ephys/ephys_recordings/'\n",
    "        args['free_move']   = True\n",
    "        args['fm_dark']     = fm_dark\n",
    "        if args['free_move'] :\n",
    "            if args['fm_dark']:\n",
    "                stim_type   = 'fm1_dark'\n",
    "            else:\n",
    "                stim_type   = 'fm1'\n",
    "        else:\n",
    "            stim_type       = 'hf1_wn'\n",
    "        args['NoL1']        = False\n",
    "        args['NoL2']        = False\n",
    "        args['reg_lap']     = False\n",
    "        args['complex']     = False\n",
    "        args['do_shuffle']  = False\n",
    "        args['Nepochs']     = 10000\n",
    "        params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "        params['nt_glm_lag']=5\n",
    "        params['thresh_cells']=0\n",
    "        date_ani_dir = params['save_dir'].parent.parent.parent/date_ani\n",
    "        params = get_modeltype(params)\n",
    "        model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "        mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "        # ModelList = np.array(sorted([path for path in date_ani_dir.rglob(mod_name+'.h5') if ('RevisionSims' in path.as_posix()) &('VisNoShifter' not in path.stem) & ('SimRF' not in path.stem)]))\n",
    "        # ModelList_all.append(ModelList)\n",
    "        bad_cells = np.load(params['save_dir_hf']/'bad_cells.npy')\n",
    "        data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "        bad_cells1.append(bad_cells)\n",
    "        # model_move1.append(model_move)\n",
    "        tuning_sig1.append(tuning_sig)\n",
    "        tuning_sig2a.append(tuning_sig2)\n",
    "        print(stim_type,mod_name)\n",
    "    bad_cells_all.append(np.stack(bad_cells1))\n",
    "    # model_move_FM.append(np.stack(model_move1))\n",
    "    test_nsp_all.append(data['test_nsp'])\n",
    "    test_std.append(np.var(data['test_nsp'],axis=0))\n",
    "    tuning_sig_all.append(np.stack(tuning_sig1))\n",
    "    tuning_sig_all2.append(np.stack(tuning_sig2a))\n",
    "    NCells_all.append(len(tuning_sig2a))\n",
    "\n",
    "# ModelList_all = np.stack(ModelList_all)\n",
    "test_std=np.hstack(test_std)\n",
    "tuning_sig_all2 = np.hstack(tuning_sig_all2)\n",
    "tuning_sig_all = np.concatenate(tuning_sig_all,axis=1)\n",
    "# model_move_FM = np.concatenate(model_move_FM,axis=0)\n",
    "# model_move_HF = np.concatenate(model_move_HF,axis=0)\n",
    "\n",
    "ModelList_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Modulation Index Histograms #####\n",
    "dwidth = .02\n",
    "# fig,axs = plt.subplots(2,len(titles),figsize=(8,4))\n",
    "fig = plt.figure(constrained_layout=False, figsize=(12,6))\n",
    "gs0 = gridspec.GridSpec(nrows=1, ncols=5, figure=fig,wspace=.3,hspace=.8)\n",
    "gs00 = gridspec.GridSpecFromSubplotSpec(nrows=3, ncols=4, subplot_spec=gs0[0,:-1],wspace=.8,hspace=.7)\n",
    "# gs00 = gridspec.GridSpecFromSubplotSpec(nrows=2, ncols=4, subplot_spec=gs0[0,:-1],wspace=.8,hspace=.7)\n",
    "axs1 = np.array([fig.add_subplot(gs00[n,m]) for n in range(3) for m in range(4) ]).reshape(3,4)\n",
    "for dk in [0,1]:\n",
    "    for modeln in np.arange(0,len(titles)):\n",
    "        ax = axs1[dk,modeln]\n",
    "        count,edges = np.histogram(tuning_sig_all[dk,:,modeln], bins=np.arange(0,1.1,dwidth))\n",
    "        edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "        ax.bar(edges_mid, count/len(tuning_sig_all[dk,:,modeln]),color='k',width=dwidth,alpha=1)\n",
    "        ax.axvline(x=.33,ls='--',color='#6D6E71')\n",
    "        ax.set_title(r'{}'.format(titles[modeln].lower()), fontsize=fontsize)\n",
    "        ax.set_xlabel('MI', fontsize=fontsize)\n",
    "        # ax.set_ylabel('fraction of units', fontsize=fontsize)\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_yticks(np.round(np.arange(0,.1,.02),decimals=1))\n",
    "        ax.set_yticklabels(np.round(np.arange(0,.1,.02),decimals=1))\n",
    "        ax.set_ylim(0,.1)\n",
    "    axs1[dk,0].set_ylabel('fraction of units', fontsize=fontsize)\n",
    "\n",
    "# for dk in [2]:\n",
    "for modeln in np.arange(0,len(titles)):\n",
    "    ax = axs1[2,modeln]\n",
    "    ax.scatter(tuning_sig_all[0,:,modeln],tuning_sig_all[1,:,modeln],s=5,c='k')\n",
    "    ax.plot(np.linspace(0,1,10),np.linspace(0,1,10),ls='--',color='#6D6E71')\n",
    "    ax.set_xlabel('light',fontsize=fontsize)\n",
    "    ax.set_ylabel('dark',fontsize=fontsize)\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/('DarkTuning.pdf'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(tuning_sig_all[1]>.33,axis=0)/tuning_sig_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('light:',np.sum(np.any(tuning_sig_all[0]>.33,axis=1))/tuning_sig_all.shape[1],'dark:',np.sum(np.any(tuning_sig_all[1]>.33,axis=1))/tuning_sig_all.shape[1])\n",
    "print('light:',np.sum(np.any(tuning_sig_all[0]>.33,axis=1)),'dark:',np.sum(np.any(tuning_sig_all[1]>.33,axis=1)),'total: ',tuning_sig_all.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye/Head only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_type = 'EyeHead_only'\n",
    "vals_Eye = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_EyeOnly_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Head = np.hstack([All_data[pparams['date_ani2'][da]][exp_type]['Mot_HeadOnly_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_Mot = np.hstack([All_data[pparams['date_ani2'][da]]['CropInputs']['Mot_cc_test'] for da in range(len(pparams['dates_all']))])\n",
    "vals_all = np.stack((vals_Eye,vals_Head,vals_Mot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(4,2))\n",
    "ax = axs[0]\n",
    "ax.scatter(vals_Eye,vals_Head,s=2,c='k')\n",
    "ax.plot(np.linspace(0,.8,10),np.linspace(0,.8,10),ls='--',color='#6D6E71')\n",
    "ax.axis('square')\n",
    "ax.set_xlabel('Eye Only cc',fontsize=fontsize)\n",
    "ax.set_ylabel('Head Only cc',fontsize=fontsize)\n",
    "ax.set_xticks([0,.25,.5,.75])\n",
    "ax.set_yticks([0,.25,.5,.75])\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar([0,1,2],np.mean(vals_all,axis=1),yerr=np.std(vals_all,axis=1)/np.sqrt(vals_all.shape[0]),color='k',error_kw=dict(ecolor='#6D6E71', lw=2, capsize=2, capthick=2))\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(['Eye','Head','Both'],fontsize=fontsize)\n",
    "ax.set_ylabel('cc',fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/('EyeHeadOnly.pdf'), transparent=True, bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_rel(vals_Eye,vals_Head),ttest_rel(vals_Mot,vals_Head),ttest_rel(vals_Eye,vals_Mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change In Visual Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "args['free_move'] = True\n",
    "if args['free_move']:\n",
    "    stim_type = 'fm1'\n",
    "else:\n",
    "    stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT']#,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['date_ani']        = dates_all[1]\n",
    "args['free_move']       = True\n",
    "args['train_shifter']   = True\n",
    "args['NoL1']            = False\n",
    "args['NoL2']            = False\n",
    "args['reg_lap']         = False\n",
    "args['do_shuffle']      = False\n",
    "args['complex']         = False\n",
    "args['Nepochs']         = 10000\n",
    "\n",
    "params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "if args['train_shifter']:\n",
    "    params['lag_list']     = [0]\n",
    "    params['nt_glm_lag']   = len(params['lag_list'])\n",
    "    params['Nepochs']      = 5000\n",
    "data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "params = get_modeltype(params)\n",
    "model_type = '_'.join(params['model_type'].split('_')[2:])\n",
    "mod_name = '*{}_dt{:03d}_T{:02d}*_NB{}_Kfold{:02d}_best'.format(model_type,int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'],Kfold)\n",
    "ModelList = sorted(list(exp.save_dir.rglob(mod_name+'.h5')))\n",
    "model_info = params['date_ani2']+'_'+mod_name.replace('*', '')\n",
    "\n",
    "GLM_Shifter = ioh5.load(ModelList[0])\n",
    "params['save_model_shift'] = params['save_model'].parent.parent / 'Shifter'\n",
    "params['save_dir_testing'] = Path('/home/seuss/Research/SensoryMotorPred_Data/data_testing/070921/J553RT/fm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    logging_level=logging.ERROR,\n",
    "    # include_dashboard=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_dict, params, downsamp_vid=1, do_worldcam_correction=False, reprocess=False, medfiltbins=11):\n",
    "\n",
    "    # open worldcam\n",
    "    print('opening worldcam')\n",
    "    world_data = xr.open_dataset(file_dict['world'])\n",
    "    world_vid_raw = np.uint8(world_data['WORLD_video'])\n",
    "    # resize worldcam\n",
    "    sz = world_vid_raw.shape # raw video size\n",
    "    # if size is larger than the target 60x80, resize by 0.5\n",
    "    if sz[1]>160:\n",
    "        downsamp = 0.5\n",
    "        world_vid = np.zeros((sz[0],int(sz[1]*downsamp),int(sz[2]*downsamp)), dtype = 'uint8')\n",
    "        for f in range(sz[0]):\n",
    "            world_vid[f,:,:] = cv2.resize(world_vid_raw[f,:,:],(int(sz[2]*downsamp),int(sz[1]*downsamp)))\n",
    "    else:\n",
    "        # if the worldcam has already been resized when the nc file was written in preprocessing, don't resize\n",
    "        world_vid = world_vid_raw.copy()\n",
    "    del world_vid_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # world timestamps\n",
    "    worldT = world_data.timestamps.copy()\n",
    "\n",
    "    # load IMU data\n",
    "    if file_dict['imu'] is not None:\n",
    "        print('opening imu data')\n",
    "        imu_data = xr.open_dataset(file_dict['imu'])\n",
    "        try:\n",
    "            accT = imu_data.IMU_data.sample # imu timestamps\n",
    "            acc_chans = imu_data.IMU_data # imu dample data\n",
    "        except AttributeError:\n",
    "            accT = imu_data.__xarray_dataarray_variable__.sample\n",
    "            acc_chans = imu_data.__xarray_dataarray_variable__\n",
    "        # raw gyro values\n",
    "        gx = np.array(acc_chans.sel(channel='gyro_x_raw'))\n",
    "        gy = np.array(acc_chans.sel(channel='gyro_y_raw'))\n",
    "        gz = np.array(acc_chans.sel(channel='gyro_z_raw'))\n",
    "        # gyro values in degrees\n",
    "        gx_deg = np.array(acc_chans.sel(channel='gyro_x'))\n",
    "        gy_deg = np.array(acc_chans.sel(channel='gyro_y'))\n",
    "        gz_deg = np.array(acc_chans.sel(channel='gyro_z'))\n",
    "        # pitch and roll in deg\n",
    "        groll = medfilt(np.array(acc_chans.sel(channel='roll')),medfiltbins)\n",
    "        gpitch = medfilt(np.array(acc_chans.sel(channel='pitch')),medfiltbins)\n",
    "\n",
    "    else: \n",
    "        accT = []\n",
    "        gz = []\n",
    "        groll = []\n",
    "        gpitch = []\n",
    "\n",
    "\n",
    "    print('opening ephys data')\n",
    "    # ephys data for this individual recording\n",
    "    ephys_data = pd.read_json(file_dict['ephys'])\n",
    "    # sort units by shank and site order\n",
    "    ephys_data = ephys_data.sort_values(by='ch', axis=0, ascending=True)\n",
    "    ephys_data = ephys_data.reset_index()\n",
    "    ephys_data = ephys_data.drop('index', axis=1)\n",
    "    # spike times\n",
    "    ephys_data['spikeTraw'] = ephys_data['spikeT']\n",
    "    print('getting good cells')\n",
    "    # select good cells from phy2\n",
    "    goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "    units = goodcells.index.values\n",
    "    # get number of good units\n",
    "    n_units = len(goodcells)\n",
    "\n",
    "    print('opening eyecam data')\n",
    "    # load eye data\n",
    "    eye_data = xr.open_dataset(file_dict['eye'])\n",
    "    eye_vid = np.uint8(eye_data['REYE_video'])\n",
    "    eyeT = eye_data.timestamps.copy()\n",
    "\n",
    "    # plot eye postion across recording\n",
    "    eye_params = eye_data['REYE_ellipse_params']\n",
    "    # define theta, phi and zero-center\n",
    "    th = np.array((eye_params.sel(ellipse_params = 'theta'))*180/np.pi)\n",
    "    phi = np.array((eye_params.sel(ellipse_params = 'phi'))*180/np.pi)\n",
    "\n",
    "    print('adjusting camera times to match ephys')\n",
    "    # adjust eye/world/top times relative to ephys\n",
    "    ephysT0 = ephys_data.iloc[0,12]\n",
    "    eyeT = eye_data.timestamps  - ephysT0\n",
    "    if eyeT[0]<-600:\n",
    "        eyeT = eyeT + 8*60*60 # 8hr offset for some data\n",
    "    worldT = world_data.timestamps - ephysT0\n",
    "    if worldT[0]<-600:\n",
    "        worldT = worldT + 8*60*60\n",
    "    accTraw = accT - ephysT0\n",
    "\n",
    "    ##### Clear some memory #####\n",
    "    del eye_data \n",
    "    gc.collect()\n",
    "    print(world_vid.shape)\n",
    "\n",
    "\n",
    "    # calculate eye veloctiy\n",
    "    dEye = np.diff(th)\n",
    "    accT_correction_file = params['save_dir']/'acct_correction.h5'\n",
    "    if (accT_correction_file.exists()):\n",
    "        accT_correction = ioh5.load(accT_correction_file)\n",
    "        offset0    = accT_correction['offset0']\n",
    "        drift_rate = accT_correction['drift_rate']\n",
    "        accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "    else:\n",
    "        # check accelerometer / eye temporal alignment\n",
    "        if (file_dict['imu'] is not None):\n",
    "            print('checking accelerometer / eye temporal alignment')\n",
    "            lag_range = np.arange(-0.2,0.2,0.002)\n",
    "            cc = np.zeros(np.shape(lag_range))\n",
    "            t1 = np.arange(5,len(dEye)/60-120,20).astype(int) # was np.arange(5,1600,20), changed for shorter videos\n",
    "            t2 = t1 + 60\n",
    "            offset = np.zeros(np.shape(t1))\n",
    "            ccmax = np.zeros(np.shape(t1))\n",
    "            acc_interp = interp1d(accTraw, (gz-3)*7.5)\n",
    "            for tstart in tqdm(range(len(t1))):\n",
    "                for l in range(len(lag_range)):\n",
    "                    try:\n",
    "                        c, lag= nanxcorr(-dEye[t1[tstart]*60 : t2[tstart]*60] , acc_interp(eyeT[t1[tstart]*60:t2[tstart]*60]+lag_range[l]),1)\n",
    "                        cc[l] = c[1]\n",
    "                    except: # occasional problem with operands that cannot be broadcast togther because of different shapes\n",
    "                        cc[l] = np.nan\n",
    "                offset[tstart] = lag_range[np.argmax(cc)]    \n",
    "                ccmax[tstart] = np.max(cc)\n",
    "            offset[ccmax<0.1] = np.nan\n",
    "            del ccmax, dEye\n",
    "            gc.collect()\n",
    "            if np.isnan(offset).all():\n",
    "                found_good_offset = False\n",
    "            else:\n",
    "                found_good_offset = True\n",
    "\n",
    "        if file_dict['imu'] is not None and found_good_offset is True:\n",
    "            print('fitting regression to timing drift')\n",
    "            # fit regression to timing drift\n",
    "            model = LinearRegression()\n",
    "            dataT = np.array(eyeT[t1*60 + 30*60])\n",
    "            model.fit(dataT[~np.isnan(offset)].reshape(-1,1),offset[~np.isnan(offset)]) \n",
    "            offset0 = model.intercept_\n",
    "            drift_rate = model.coef_\n",
    "            del dataT\n",
    "            gc.collect()\n",
    "        elif file_dict['speed'] is not None or found_good_offset is False:\n",
    "            offset0 = 0.1\n",
    "            drift_rate = -0.000114\n",
    "        if file_dict['imu'] is not None:\n",
    "            accT_correction = {'offset0': offset0, 'drift_rate': drift_rate}\n",
    "            ioh5.save(accT_correction_file,accT_correction)\n",
    "            accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "            del accTraw\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    print('correcting ephys spike times for offset and timing drift')\n",
    "    for i in ephys_data.index:\n",
    "        ephys_data.at[i,'spikeT'] = np.array(ephys_data.at[i,'spikeTraw']) - (offset0 + np.array(ephys_data.at[i,'spikeTraw']) *drift_rate)\n",
    "    goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "\n",
    "    return eyeT,accT,worldT,th,phi,groll,gpitch,gz_deg,world_vid,ephys_data\n",
    "\n",
    "eyeT,accT,worldT,th,phi,groll,gpitch,gz_deg,world_vid,ephys_data = load_raw_data(file_dict,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timestamps = {\n",
    "'eyeT':   eyeT.data.copy(),\n",
    "'accT':   accT.data.copy(),\n",
    "'worldT': worldT.data.copy(),\n",
    "}\n",
    "pos_data = {\n",
    "'th':       th.copy(),\n",
    "'phi':      phi.copy(),\n",
    "'groll':    groll.copy(),\n",
    "'gpitch':   gpitch.copy(),\n",
    "'gz_deg':   gz_deg.copy(),\n",
    "}\n",
    "# ioh5.save(params['save_dir_testing']/'img_registration_correction_timestamps.h5',Timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add in interpolaters for getting same time basis.\n",
    "interp_th = interp1d(Timestamps['eyeT'],pos_data['th'], bounds_error=False)\n",
    "interp_phi = interp1d(Timestamps['eyeT'],pos_data['phi'], bounds_error=False)\n",
    "interp_groll = interp1d(Timestamps['accT'],pos_data['groll'], bounds_error=False)\n",
    "interp_gpitch = interp1d(Timestamps['accT'],pos_data['gpitch'], bounds_error=False)\n",
    "interp_gz_deg = interp1d(Timestamps['accT'],pos_data['gz_deg'], bounds_error=False)\n",
    "\n",
    "model_pos = np.vstack((interp_th(Timestamps['worldT']),interp_phi(Timestamps['worldT']),interp_groll(Timestamps['worldT']),interp_gpitch(Timestamps['worldT']),interp_gz_deg(Timestamps['worldT']))).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_med_win = 11\n",
    "\n",
    "FM_move_avg = np.load(params['save_dir_fm']/'FM_MovAvg_dt{:03d}.npy'.format(int(params['model_dt']*1000)))\n",
    "for n in np.arange(4):\n",
    "    model_pos[:,n] = ((pd.DataFrame(model_pos[:,n]).interpolate(axis=0).to_numpy() - FM_move_avg[0,n])/FM_move_avg[1,n]).squeeze()\n",
    "\n",
    "model_pos[:,2] = medfilt(model_pos[:,2],move_med_win)\n",
    "model_pos[:,3] = medfilt(model_pos[:,3],move_med_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del accT, ephys_data, gpitch, groll, gz_deg, phi, th, \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading and processing shifter')\n",
    "params['model_type'] = 'Pytorch_VisShifter_NoL1'\n",
    "##### Load single shifter network and shift videos #####\n",
    "best_shifter=np.nanargmin(np.nanmean(GLM_Shifter['loss_regcv'][0],axis=-1))\n",
    "reg_alph=a=0; l=best_shifter\n",
    "print(best_shifter)\n",
    "Kfold=0\n",
    "shift_in=3\n",
    "shift_hidden=20\n",
    "shift_out=3\n",
    "# save_model_fm = params['save_model_shift']\n",
    "save_model_fm = params['save_model_shift'].parent.parent.parent / 'fm1' / 'CropInputs/Shifter'\n",
    "\n",
    "shifter_nn = nn.Sequential(\n",
    "                nn.Linear(shift_in,shift_hidden),\n",
    "                nn.Softplus(),\n",
    "                nn.Linear(shift_hidden, shift_out))#.to(device)\n",
    "model_name = 'GLM_{}_WC{}_dt{:03d}_T{:02d}_MovModel{:d}_NB{}_alph{}_lam{}_Kfold{:01d}.pth'.format(params['model_type'],'UC',int(.05*1000), 1, MovModel, 5000,a,l,Kfold)\n",
    "# model_name = 'GLMShifter_WCUC_dt050_T01_MovModel1_NB5000_alph0_lam7.pth'\n",
    "checkpoint = torch.load(save_model_fm/model_name,map_location='cuda')\n",
    "pretrained_dict = {'.'.join(k.split('.')[1:]): v for k, v in checkpoint['model_state_dict'].items() if 'shift' in k}\n",
    "shifter_nn.load_state_dict(pretrained_dict)\n",
    "\n",
    "ds = 4/1#params['downsamp_vid']\n",
    "shift_out = shifter_nn(torch.from_numpy(model_pos[:,(0,1,3)].astype(np.float32)))#.to(device))\n",
    "shift = Affine(angle=torch.clamp(shift_out[:,-1],min=-45,max=45),translation=torch.clamp(shift_out[:,:2]*ds,min=-20*ds,max=20*ds))\n",
    "vid_tensor=torch.from_numpy(world_vid[:,np.newaxis].astype(np.float32))#.to(device)\n",
    "model_vid_sm_shift = shift(vid_tensor.contiguous()).cpu().detach().numpy().squeeze()\n",
    "model_vid_sm_shift = model_vid_sm_shift.astype(np.uint8)\n",
    "del vid_tensor, shift\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def shift_vid_parallel(x, world_vid, world_vid_shift, warp_mode, criteria, dt):\n",
    "    xshift_t = []\n",
    "    yshift_t = []\n",
    "    roation_t = []\n",
    "    cc_t = []\n",
    "    xshift_t_shift = []\n",
    "    yshift_t_shift = []\n",
    "    roation_t_shift = []\n",
    "    cc_t_shift = []\n",
    "    for i in range(x,x+dt):\n",
    "        warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "        warp_matrix_shift = np.eye(2, 3, dtype=np.float32)\n",
    "        try: \n",
    "            (cc, warp_matrix) = cv2.findTransformECC(world_vid[i,:,:], world_vid[i+1,:,:], warp_matrix, warp_mode, criteria, inputMask=None, gaussFiltSize=1)\n",
    "            xshift = warp_matrix[0,2]\n",
    "            yshift = warp_matrix[1,2]\n",
    "            rotation = np.arctan2(warp_matrix[1,0],warp_matrix[0,0])\n",
    "        except:\n",
    "            cc = np.nan\n",
    "            xshift=np.nan\n",
    "            yshift = np.nan\n",
    "            rotation = np.nan\n",
    "        try:\n",
    "            (cc_shift, warp_matrix_shift) = cv2.findTransformECC(world_vid_shift[i,:,:], world_vid_shift[i+1,:,:], warp_matrix_shift, warp_mode, criteria, inputMask=None, gaussFiltSize=1)\n",
    "            xshift_shift = warp_matrix_shift[0,2]\n",
    "            yshift_shift = warp_matrix_shift[1,2]\n",
    "            rotation_shift = np.arctan2(warp_matrix_shift[1,0],warp_matrix_shift[0,0])\n",
    "        except:\n",
    "            cc_shift = np.nan\n",
    "            xshift_shift=np.nan\n",
    "            yshift_shift = np.nan\n",
    "            rotation_shift = np.nan\n",
    "        xshift_t.append(xshift)\n",
    "        yshift_t.append(yshift)\n",
    "        cc_t.append(cc)\n",
    "        roation_t.append(rotation)\n",
    "        xshift_t_shift.append(xshift_shift)\n",
    "        yshift_t_shift.append(yshift_shift)\n",
    "        cc_t_shift.append(cc_shift)\n",
    "        roation_t_shift.append(rotation_shift)\n",
    "    return xshift_t, yshift_t, cc_t, roation_t, xshift_t_shift, yshift_t_shift, cc_t_shift, roation_t_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# calculate x-y shift for each worldcam frame  \n",
    "number_of_iterations = 5000\n",
    "termination_eps = 1e-4\n",
    "criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations, termination_eps)\n",
    "# warp_mode = cv2.MOTION_TRANSLATION\n",
    "warp_mode = cv2.MOTION_AFFINE\n",
    "\n",
    "# Parallel Testing\n",
    "world_vid_r = ray.put(world_vid) # world_vid model_vid_sm_shift\n",
    "world_vid_shift_r = ray.put(model_vid_sm_shift) # world_vid model_vid_sm_shift\n",
    "warp_mode_r = ray.put(warp_mode)\n",
    "criteria_r = ray.put(criteria)\n",
    "\n",
    "dt = 100\n",
    "max_frames = world_vid.shape[0]\n",
    "result_ids = []\n",
    "[result_ids.append(shift_vid_parallel.remote(i, world_vid_r, world_vid_shift_r, warp_mode_r, criteria_r, dt)) for i in range(0, max_frames, dt)]\n",
    "results_p = ray.get(result_ids)\n",
    "results_p = np.array(results_p).transpose(0,2,1).reshape(-1,8)\n",
    "\n",
    "xshift = results_p[:,0]\n",
    "yshift = results_p[:,1]\n",
    "cc = results_p[:,2]\n",
    "rotation = results_p[:,3]\n",
    "\n",
    "xshift_shift = results_p[:,4]\n",
    "yshift_shift = results_p[:,5]\n",
    "cc_shift = results_p[:,6]\n",
    "rotation_shift = results_p[:,7]\n",
    "del results_p, warp_mode_r, criteria_r, result_ids, world_vid_r, world_vid_shift_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reg = {\n",
    "            'xshift': xshift,\n",
    "            'yshift': yshift,\n",
    "            'cc':     cc, \n",
    "            'worldT': worldT.data,\n",
    "            'rotation': rotation,\n",
    "            'xshift_shift': xshift_shift,\n",
    "            'yshift_shift': yshift_shift,\n",
    "            'cc_shift':     cc_shift, \n",
    "            'rotation_shift': rotation_shift\n",
    "            }\n",
    "\n",
    "# img_reg.update(world_shifts)\n",
    "ioh5.save(params['save_dir_testing']/'img_registration_correction2b.h5',img_reg) #_shifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_reg1 = ioh5.load(params['save_dir_testing']/'img_registration_correction2.h5')\n",
    "# img_reg_shift = ioh5.load(params['save_dir_testing']/'img_registration_correction_shifter2.h5')\n",
    "img_reg = ioh5.load(params['save_dir_testing']/'img_registration_correction4.h5')\n",
    "Timestamps = ioh5.load(params['save_dir_testing']/'img_registration_correction_timestamps.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Add in interpolaters for getting same time basis.\n",
    "interp_th = interp1d(Timestamps['eyeT'],pos_data['th'], bounds_error=False)\n",
    "interp_phi = interp1d(Timestamps['eyeT'],pos_data['phi'], bounds_error=False)\n",
    "interp_groll = interp1d(Timestamps['accT'],pos_data['groll'], bounds_error=False)\n",
    "interp_gpitch = interp1d(Timestamps['accT'],pos_data['gpitch'], bounds_error=False)\n",
    "interp_gz_deg = interp1d(Timestamps['accT'],pos_data['gz_deg'], bounds_error=False)\n",
    "\n",
    "model_pos = np.vstack((interp_th(Timestamps['worldT']),interp_phi(Timestamps['worldT']),interp_groll(Timestamps['worldT']),interp_gpitch(Timestamps['worldT']),interp_gz_deg(Timestamps['worldT']))).T\n",
    "\n",
    "##### Defining Saccades #####\n",
    "dth = np.diff(model_pos[:,0])/np.nanmean(np.diff(Timestamps['worldT']))\n",
    "dgz = model_pos[:,-1]\n",
    "dsum = dth + dgz[:-1]\n",
    "saccades = np.abs(dsum)>120 # threshold for saccades\n",
    "ind_s = np.where(saccades)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saccade_info = ['sac_r','sac_s','ind_ss','ind_se']\n",
    "ctype_list = ['xshift','yshift','rotation','xshift_shift','yshift_shift','rotation_shift']\n",
    "Sdata = {s:{} for s in Saccade_info}\n",
    "\n",
    "Sdata['ind_ss'][ctype] = np.hstack([ind_s[n]for n in range(ind_s.shape[0]-1) if ((ind_s[n+1] - ind_s[n])>10)&(np.all(~np.isnan(model_pos[ind_s[n]:ind_s[n+1]])))])\n",
    "Sdata['ind_se'][ctype] = np.hstack([ind_s[n+1]for n in range(ind_s.shape[0]-1) if ((ind_s[n+1] - ind_s[n])>10)&(np.all(~np.isnan(model_pos[ind_s[n]:ind_s[n+1]])))])\n",
    "for s in Saccade_info:\n",
    "    for ctype in ctype_list:\n",
    "        Sdata[s][ctype] = np.hstack([np.nanstd(np.nancumsum(img_reg[ctype])[ind_s[n]+1:ind_s[n+1]-1]) for n in range(ind_s.shape[0]-1) if ((ind_s[n+1] - ind_s[n])>10)&(np.all(~np.isnan(model_pos[ind_s[n]:ind_s[n+1]])))])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctype = 'xshift'\n",
    "sac_r2 = np.nanmean(Sdata['sac_r'][ctype][Sdata['sac_r'][ctype]<20])/2\n",
    "sac_s2 = np.nanmean(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/2\n",
    "sac_r2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))\n",
    "sac_s2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(8,2))\n",
    "dcsum = 15\n",
    "lims=10\n",
    "dlim = 10\n",
    "startT = 35 #47\n",
    "endT = 40 # 240\n",
    "moveTmin = np.nanargmin(np.abs(Timestamps['worldT']-startT))\n",
    "moveTmax = np.nanargmin(np.abs(Timestamps['worldT']-endT))\n",
    "t_range = Timestamps['worldT'][moveTmin:moveTmax]\n",
    "\n",
    "ax = axs[0]\n",
    "ax.plot(t_range,np.nancumsum(img_reg['xshift'][moveTmin:moveTmax])/2,c='k',label='raw')\n",
    "ax.plot(t_range,np.nancumsum(img_reg['xshift_shift'][moveTmin:moveTmax])/2,c='r',label='shifter')\n",
    "ax.set_ylabel('total displacement (deg)',fontsize=fontsize)\n",
    "ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "# ax.set_title('xshift',fontsize=fontsize)\n",
    "ax.legend(frameon=False,fontsize=fontsize,loc='lower left',labelcolor='linecolor', handlelength=0, handletextpad=0)\n",
    "ax.set_ylim([-50,50])\n",
    "\n",
    "ax = axs[1]\n",
    "ctype = 'xshift'\n",
    "sac_r2 = np.nanmean(Sdata['sac_r'][ctype][Sdata['sac_r'][ctype]<20])/2\n",
    "sac_r2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))\n",
    "ctype = 'xshift_shift'\n",
    "sac_s2 = np.nanmean(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/2\n",
    "sac_s2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))\n",
    "ax.bar([0,1],[sac_r2,sac_s2],color='k',yerr=[sac_r2_ste,sac_s2_ste],error_kw=dict(ecolor='#6D6E71', lw=2, capsize=5, capthick=2))\n",
    "ax.plot([0,0, 1, 1], [2.5, 2.62, 2.62, 2.5], linewidth=1, color='k')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['raw','shifter'])\n",
    "ax.set_ylabel('horiz. stability (deg)')\n",
    "\n",
    "\n",
    "# ax = axs[1,0]\n",
    "# ax.plot(t_range,np.nancumsum(img_reg['yshift'][moveTmin:moveTmax]),c='k',label='raw')\n",
    "# ax.plot(t_range,np.nancumsum(img_reg['yshift_shift'][moveTmin:moveTmax]),c='r',label='shifter')\n",
    "# # ax.plot(t_range,img_reg_shift['shift_out'][moveTmin:moveTmax,1],c='g', label='shifty')\n",
    "# ax.set_ylabel('cumulative sum',fontsize=fontsize)\n",
    "# ax.set_xlabel('time (s)',fontsize=fontsize)\n",
    "# ax.set_title('yshift',fontsize=fontsize)\n",
    "# ax.set_ylim([-100,100])\n",
    "\n",
    "ax = axs[2]\n",
    "ctype = 'yshift'\n",
    "sac_r2 = np.nanmean(Sdata['sac_r'][ctype][Sdata['sac_r'][ctype]<20])/2\n",
    "sac_r2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))\n",
    "ctype = 'yshift_shift'\n",
    "sac_s2 = np.nanmean(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/2\n",
    "sac_s2_ste = np.nanstd(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20])/(2*np.sqrt(Sdata['sac_s'][ctype][Sdata['sac_s'][ctype]<20].shape[0]))\n",
    "ax.bar([0,1],[sac_r2,sac_s2],color='k',yerr=[sac_r2_ste,sac_s2_ste],error_kw=dict(ecolor='#6D6E71', lw=2, capsize=5, capthick=2))\n",
    "ax.plot([0,0, 1, 1], [2.5, 2.62, 2.62, 2.5], linewidth=1, color='k')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['raw','shifter'])\n",
    "ax.set_ylabel('vert. stability (deg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(paper_fig_dir/'d_vis_scene.pdf',ppi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26298cf",
   "metadata": {},
   "source": [
    "## Shifter Matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb790c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 0\n",
    "fontsize=10\n",
    "args['date_ani'] = dates_all[da]\n",
    "params,_,_ = load_params(0,Kfold,args,debug=True)\n",
    "data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n",
    "\n",
    "save_dir_fm = params['save_dir_fm'] /'GLM_Network/MovModel1/version_0'\n",
    "\n",
    "save_datafile = save_dir_fm/'GLM_Pytorch_VisShifter_NoL1_dt050_T01_MovModel1_NB5000_Kfold00.h5'\n",
    "GLM_CV = ioh5.load(save_datafile)\n",
    "\n",
    "best_shifter = np.nanargmin(np.nanmean(GLM_CV['loss_regcv'][0,:],axis=-1))\n",
    "\n",
    "##### Load single shifter network and shift videos #####\n",
    "reg_alph=a=0; l=best_shifter\n",
    "Kfold=0\n",
    "shift_in=3\n",
    "shift_hidden=20\n",
    "shift_out=3\n",
    "save_model_fm = params['save_dir'] / 'models/Shifter'\n",
    "\n",
    "shifter_nn = nn.Sequential(\n",
    "                nn.Linear(shift_in,shift_hidden),\n",
    "                nn.Softplus(),\n",
    "                nn.Linear(shift_hidden, shift_out))#.to(device)\n",
    "model_name = 'GLM_{}_WC{}_dt{:03d}_T{:02d}_MovModel{:d}_NB{}_alph{}_lam{}_Kfold{:01d}.pth'.format('Pytorch_VisShifter_NoL1','UC',int(.05*1000), 1, MovModel, 5000,a,l,Kfold)\n",
    "# model_name = 'GLMShifter_WCUC_dt050_T01_MovModel1_NB5000_alph0_lam7.pth'\n",
    "checkpoint = torch.load(save_model_fm/model_name)\n",
    "pretrained_dict = {'.'.join(k.split('.')[1:]): v for k, v in checkpoint['model_state_dict'].items() if 'shift' in k}\n",
    "shifter_nn.load_state_dict(pretrained_dict)\n",
    "\n",
    "FM_move_avg = np.load(params['save_dir_fm']/'FM_MovAvg_dt{:03d}.npy'.format(int(params['model_dt']*1000)))\n",
    "th_range = 40/FM_move_avg[1,0]\n",
    "phi_range = 40/FM_move_avg[1,1]\n",
    "pitch_range = 40/FM_move_avg[1,2]\n",
    "ang_sweepx,ang_sweepy,ang_sweepz = np.meshgrid(np.linspace(-th_range,th_range,81),np.linspace(-phi_range,phi_range,81),np.linspace(-pitch_range,pitch_range,81),sparse=False,indexing='ij')\n",
    "# ang_sweepx,ang_sweepy,ang_sweepz = np.meshgrid(np.arange(-40,40),np.arange(-40,40),np.arange(-40,40),sparse=False,indexing='ij')\n",
    "\n",
    "shift_mat = np.zeros((3,) + ang_sweepx.shape)\n",
    "for i in range(ang_sweepx.shape[0]):\n",
    "    for j in range(ang_sweepy.shape[1]):\n",
    "        ang_sweep = torch.from_numpy(np.vstack((ang_sweepx[i,j,:],ang_sweepy[i,j,:],ang_sweepz[i,j,:])).astype(np.float32).T)\n",
    "        shift_vec = shifter_nn(ang_sweep).detach().cpu().numpy()\n",
    "        shift_mat[0,i,j] = shift_vec[:,0] #np.clip(shift_vec[:,0],a_min=-15,a_max=15)\n",
    "        shift_mat[1,i,j] = shift_vec[:,1] #np.clip(shift_vec[:,1],a_min=-15,a_max=15)\n",
    "        shift_mat[2,i,j] = shift_vec[:,2] #np.clip(shift_vec[:,2],a_min=-30,a_max=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e80fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 10\n",
    "\n",
    "fig = plt.figure(constrained_layout=False, figsize=(8,5))\n",
    "gs0 = gridspec.GridSpec(1,3, figure=fig,wspace=.9,hspace=0)\n",
    "# gs01 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs0[0,:3], wspace=.1,hspace=0)\n",
    "# gs02 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs0[0,3:6], wspace=.1,hspace=.1)\n",
    "# gs03 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs0[0,6:], wspace=.1,hspace=.1)\n",
    "\n",
    "# gs01a = gridspec.GridSpecFromSubplotSpec(4, 5, subplot_spec=gs0b[:2,:],wspace=1.2,hspace=.2)\n",
    "# gs02a = gridspec.GridSpecFromSubplotSpec(1, 5, subplot_spec=gs0b[2:,:],wspace=1.2,hspace=.7)\n",
    "# gs03a = gridspec.GridSpecFromSubplotSpec(1, 5, subplot_spec=gs0b[2:,:],wspace=1.2,hspace=.7)\n",
    "\n",
    "axs1a = np.array([fig.add_subplot(gs0[0,0])])#,fig.add_subplot(gs01[0,0]),fig.add_subplot(gs01[1,1])])\n",
    "axs2a = np.array([fig.add_subplot(gs0[0,1])])#,fig.add_subplot(gs02[1,0]),fig.add_subplot(gs02[1,1])])\n",
    "axs3a = np.array([fig.add_subplot(gs0[0,2])])#,fig.add_subplot(gs03[1,0]),fig.add_subplot(gs03[1,1])])\n",
    "\n",
    "\n",
    "\n",
    "shift_titles = [r'$dx$',r'$dy$',r'$d\\alpha,\\phi=0$',r'$d\\alpha,\\theta=0$']\n",
    "cbar_label = ['pixels','pixels','deg','deg']\n",
    "ticks=np.arange(0,90,20)\n",
    "ticklabels=np.arange(-40,50,20)\n",
    "shift_matshow=np.stack((shift_mat[0,:,:,40].T,shift_mat[1,:,:,40].T,shift_mat[2,:,40,:].T,shift_mat[2,40,:,:].T))\n",
    "crange_list = np.stack((np.max(np.abs(shift_mat[:2])),np.max(np.abs(shift_mat[:2])), np.max(np.abs(shift_mat[2])), np.max(np.abs(shift_mat[2]))))\n",
    "# for n in range(2):\n",
    "ax = axs1a[0]\n",
    "n=0\n",
    "im1=ax.imshow(shift_matshow[n],vmin=-crange_list[n], vmax=crange_list[n], origin='lower', cmap='RdBu_r')\n",
    "cbar1 = add_colorbar(im1)\n",
    "cbar1.set_label(cbar_label[n],fontsize=fontsize)\n",
    "cbar1.outline.set_linewidth(1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticklabels)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(ticklabels)\n",
    "ax.set_xlabel(r'$\\theta$',fontsize=fontsize)\n",
    "ax.set_ylabel(r'$\\phi$',fontsize=fontsize)\n",
    "ax.set_title(shift_titles[n],fontsize=fontsize)\n",
    "# ax = axs1a[1]\n",
    "# ax.imshow(icons[0])\n",
    "# ax.axis('off')\n",
    "# ax = axs1a[2]\n",
    "# ax.imshow(icons[1])\n",
    "# ax.axis('off')\n",
    "\n",
    "ax = axs2a[0]\n",
    "n = 1\n",
    "im1=ax.imshow(shift_matshow[n],vmin=-crange_list[n], vmax=crange_list[n], origin='lower', cmap='RdBu_r')\n",
    "cbar1 = add_colorbar(im1)\n",
    "cbar1.set_label(cbar_label[n],fontsize=fontsize)\n",
    "cbar1.outline.set_linewidth(1)\n",
    "\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticklabels)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(ticklabels)\n",
    "ax.set_xlabel(r'$\\theta$',fontsize=fontsize)\n",
    "ax.set_ylabel(r'$\\phi$',fontsize=fontsize)\n",
    "ax.set_title(shift_titles[n],fontsize=fontsize)\n",
    "# ax = axs1a[1]\n",
    "# ax.imshow(icons[0])\n",
    "# ax.axis('off')\n",
    "# ax = axs1a[2]\n",
    "# ax.imshow(icons[1])\n",
    "# ax.axis('off')\n",
    "\n",
    "ax = axs3a[0]\n",
    "ax.plot(FM_move_avg[1, 2]*np.linspace(-pitch_range,pitch_range,81), shift_mat[2, 40, 40, :], 'k')\n",
    "ax.set_xlabel(r'pitch ($\\rho$)',fontsize=fontsize)\n",
    "ax.set_ylabel(r'$d\\alpha$',fontsize=fontsize)\n",
    "ax.set_xticks(ticklabels)\n",
    "ax.set_xticklabels(ticklabels)\n",
    "ax.set_yticks([-40,-20,0,20,40])\n",
    "ax.set_yticklabels([-40, -20, 0, 20, 40])\n",
    "ax.axis('square')\n",
    "ax.set_ylim(-45, 45)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/'FigureS1.pdf', facecolor='white', transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061a9bd",
   "metadata": {},
   "source": [
    "### SimRF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3,figsize=(7,2))\n",
    "lag = 1\n",
    "da = 0\n",
    "sf=4\n",
    "cells = [0]\n",
    "for n, cell in enumerate(cells):\n",
    "    ax = axs[n]\n",
    "    crange2 = np.max(np.abs(All_data['SimData']['Simact_RF'][cell]))\n",
    "    RF_Simact_up = cv2.resize(All_data['SimData']['Simact_RF'][cell], (sf*(All_data['SimData']['Simact_RF'][cell].shape[-1]), sf*(All_data['SimData']['Simact_RF'][cell].shape[-2])))\n",
    "    im2 = ax.imshow(RF_Simact_up,'RdBu_r', vmin=-crange2, vmax=crange2)\n",
    "    ax.set_title('simulated RF', fontsize=fontsize)\n",
    "    ax = axs[n+1]\n",
    "    crange1 = np.max(np.abs(All_data['SimData']['Simfit_RF'][cell]))\n",
    "    RF_Simfit_up = cv2.resize(All_data['SimData']['Simfit_RF'][cell,0], (sf*(All_data['SimData']['Simact_RF'][cell].shape[-1]), sf*(All_data['SimData']['Simact_RF'][cell].shape[-2])))\n",
    "    im2 = ax.imshow(RF_Simfit_up,'RdBu_r', vmin=-crange1, vmax=crange1)\n",
    "    ax.set_title('reconstructed RF', fontsize=fontsize)\n",
    "scalebar = AnchoredSizeBar(axs[0].transData,\n",
    "                    20, '10 deg', 'lower left', \n",
    "                    pad=0.1,\n",
    "                    color='black',\n",
    "                    frameon=False,\n",
    "                    size_vertical=1,\n",
    "                    )\n",
    "axs[0].add_artist(scalebar)  \n",
    "\n",
    "for ax in axs.flat[:-1]:\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(0.5)\n",
    "        ax.spines[axis].set_visible(True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "########## Fig 2G ########## \n",
    "# gs05 = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=gs0[2,6:],wspace=.1,hspace=.1)\n",
    "# axs5 = np.array([fig1.add_subplot(gs05[0,0])])\n",
    "ax=axs[2]\n",
    "hbins = .05\n",
    "lim0 = .2\n",
    "lim1 = 1.1\n",
    "dlim = .5\n",
    "xlab = 'RF cc'\n",
    "simfr_mean = np.mean(All_data['SimData']['Sim_yte'],axis=0)/params['model_dt']\n",
    "simfr_low = All_data['SimData']['Sim_cc'][2::3] #Sim_r2[((simfr_mean<5))]\n",
    "simfr_med = All_data['SimData']['Sim_cc'][1::3] #Sim_r2[((simfr_mean>5) & (simfr_mean<10))]\n",
    "simfr_high = All_data['SimData']['Sim_cc'][0::3] #Sim_r2[((simfr_mean>10))]\n",
    "\n",
    "mean_fr_low = np.nanmean(simfr_mean[2::3])\n",
    "mean_fr_med = np.nanmean(simfr_mean[1::3])\n",
    "mean_fr_high = np.nanmean(simfr_mean[0::3])\n",
    "\n",
    "count,edges = np.histogram(simfr_med,bins=np.arange(lim0,lim1,hbins))\n",
    "edges_mid = np.array([(edges[i]+edges[i+1])/2 for i in range(len(edges)-1)])\n",
    "ax.bar(edges_mid, count/len(simfr_med),color='k',width=hbins, alpha=1,label='high FR')\n",
    "\n",
    "ax.set_xticks([0,.5,1])\n",
    "ax.set_xticklabels([0,.5,1],fontsize=fontsize-2)\n",
    "ax.set_xlabel('cc')\n",
    "# ax.legend(labelcolor='linecolor', fontsize=14, handlelength=0, handletextpad=0,loc='upper left',ncol=1)#, bbox_to_anchor=(.1, .9))\n",
    "\n",
    "ax.set_yticks(np.linspace(0,np.round(np.max(count/len(simfr_med)),1),3))\n",
    "ax.set_yticklabels(np.linspace(0,np.round(np.max(count/len(simfr_med)),1),3),fontsize=fontsize-2)\n",
    "ax.set_xlabel(xlab,fontsize=fontsize)\n",
    "ax.axvline(x=np.nanmean(simfr_med),lw=2,c='#6D6E71',ls='--',zorder=1)\n",
    "ax.set_ylabel('fraction of units',fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "fig.savefig(paper_fig_dir/'SimRF.pdf', ppi=300, transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74afee",
   "metadata": {},
   "source": [
    "## Figure S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 2\n",
    "args['free_move']=True\n",
    "args['date_ani'] = dates_all[da]\n",
    "params,_,_ = load_params(2,Kfold,args,debug=True)\n",
    "data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "\n",
    "icons_list = sorted(list(paper_fig_dir.glob('*icon.png')))\n",
    "\n",
    "icons = []\n",
    "for n in icons_list:\n",
    "   icons.append(image.imread(n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1451d6",
   "metadata": {},
   "source": [
    "## Video S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd1342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading Unaligned data\n",
      "Tot_units: (128,)\n"
     ]
    }
   ],
   "source": [
    "args = arg_parser(jupyter=True)\n",
    "MovModel = 1\n",
    "Kfold = 0\n",
    "args['free_move'] = True\n",
    "if args['free_move']:\n",
    "    stim_type = 'fm1'\n",
    "else:\n",
    "    stim_type = 'hf1_wn'  # 'fm1' #\n",
    "\n",
    "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
    "args['date_ani'] = dates_all[0]\n",
    "params,file_dict,exp = load_params(MovModel,Kfold,args,debug=True)\n",
    "\n",
    "# data, train_idx_list, test_idx_list = load_train_test(file_dict, **params)\n",
    "\n",
    "# ##### Set Train Test Splits #####\n",
    "# Kfold = 0\n",
    "# train_idx = train_idx_list[Kfold]\n",
    "# test_idx = test_idx_list[Kfold]\n",
    "# data = load_Kfold_data(data,train_idx,test_idx,params)\n",
    "\n",
    "data,move_train,move_test,model_move,nsp_raw,move_data,tuning_curves,tuning_stds,tuning_curve_edges,ax_ylims,tc_mod,avg_fr,tuning_sig,tuning_sig2,tuning_idx=load_Kfold_forPlots(params, Kfold=Kfold, dataset_type='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ani = '070921/J553RT'\n",
    "stim_type = 'fm1'\n",
    "fmimu_file = list((Path('~/Goeppert/nlab-nas/freely_moving_ephys/ephys_recordings/').expanduser() / date_ani / stim_type).glob('*imu.nc'))[0]\n",
    "eye_file = list((Path('~/Goeppert/nlab-nas/freely_moving_ephys/ephys_recordings/').expanduser() / date_ani / stim_type).glob('*REYE.nc'))[0]\n",
    "world_file = list((Path('~/Goeppert/nlab-nas/freely_moving_ephys/ephys_recordings/').expanduser() / date_ani / stim_type).glob('*world.nc'))[0]\n",
    "ephys_file = params['save_dir']/'RawEphysData.h5'\n",
    "\n",
    "ephys_data = pd.read_hdf(ephys_file)\n",
    "spikeT = ephys_data['spikeT']\n",
    "\n",
    "imu_data = xr.open_dataset(fmimu_file)\n",
    "accT = imu_data.IMU_data.sample # imu timestamps\n",
    "acc_chans = imu_data.IMU_data # imu dample data\n",
    "# raw gyro values\n",
    "gx = np.array(acc_chans.sel(channel='gyro_x_raw'))\n",
    "gy = np.array(acc_chans.sel(channel='gyro_y_raw'))\n",
    "gz = np.array(acc_chans.sel(channel='gyro_z_raw'))\n",
    "# gyro values in degrees\n",
    "gx_deg = np.array(acc_chans.sel(channel='gyro_x'))\n",
    "gy_deg = np.array(acc_chans.sel(channel='gyro_y'))\n",
    "gz_deg = np.array(acc_chans.sel(channel='gyro_z'))\n",
    "# pitch and roll in deg\n",
    "groll = medfilt(np.array(acc_chans.sel(channel='roll')),11)\n",
    "gpitch = medfilt(np.array(acc_chans.sel(channel='pitch')),11)\n",
    "\n",
    "eye_data = xr.open_dataset(eye_file)\n",
    "eyeT = eye_data.timestamps.copy()\n",
    "eye_params = eye_data['REYE_ellipse_params']\n",
    "eyeT = eye_data.timestamps  - ephys_data['t0'].iloc[0]\n",
    "\n",
    "th = np.array((eye_params.sel(ellipse_params = 'theta'))*180/np.pi)#-np.nanmean(eye_params.sel(ellipse_params = 'theta')))*180/3.14159)\n",
    "phi = np.array((eye_params.sel(ellipse_params = 'phi'))*180/np.pi)#-np.nanmean(eye_params.sel(ellipse_params = 'phi')))*180/3.14159)\n",
    "\n",
    "world_data = xr.open_dataset(file_dict['world'])\n",
    "world_vid_raw = np.uint8(world_data['WORLD_video'])\n",
    "worldT = world_data.timestamps.copy()\n",
    "worldT = worldT - ephys_data['t0'].iloc[0]\n",
    "\n",
    "if eyeT[0]<-600:\n",
    "    eyeT = eyeT + 8*60*60 # 8hr offset for some data\n",
    "if worldT[0]<-600:\n",
    "    worldT = worldT + 8*60*60\n",
    "accT2 = accT.sample.data - ephys_data['t0'].iloc[0]\n",
    "\n",
    "# isfast = np.diff(eyeT)<=0.05\n",
    "# isslow = sorted(list(set(chain.from_iterable([list(range(int(i)-3,int(i)+4)) for i in np.where(isfast==False)[0]]))))\n",
    "# th[isslow] = np.nan\n",
    "# phi[isslow] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_dict['spikeT']=spikeT\n",
    "data_dict['gz_deg']=gz_deg\n",
    "data_dict['groll']=groll\n",
    "data_dict['gpitch']=gpitch\n",
    "data_dict['eyeT']=eyeT\n",
    "data_dict['th']=th\n",
    "data_dict['phi']=phi\n",
    "data_dict['world_vid_raw']=world_vid_raw\n",
    "data_dict['worldT']=worldT\n",
    "data_dict['accT2']=accT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_active', 'model_eyerad', 'model_gz', 'model_nsp', 'model_phi', 'model_pitch', 'model_roll', 'model_speed', 'model_t', 'model_th', 'model_vid_sm', 'unit_nums', 'raw_nsp', 'model_dth', 'model_dphi', 'train_vid', 'test_vid', 'train_nsp', 'test_nsp', 'train_th', 'test_th', 'train_phi', 'test_phi', 'train_roll', 'test_roll', 'train_pitch', 'test_pitch', 'train_t', 'test_t', 'train_dth', 'test_dth', 'train_dphi', 'test_dphi', 'train_gz', 'test_gz', 'train_speed', 'test_speed', 'train_eyerad', 'test_eyerad'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efee01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_file = list((Path('~/Goeppert/nlab-nas/freely_moving_ephys/ephys_recordings/').expanduser() / date_ani / stim_type).rglob('*REYEdeinter.avi'))[0]\n",
    "\n",
    "cap = cv2.VideoCapture(eye_file.as_posix())\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float `height`\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) # float `fps`\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # float `total_frame_in_the_video` (should not be applicable for camera)\n",
    "sf=2\n",
    "eye_vid = np.zeros((int(total_frames),140, 175))\n",
    "for t in tqdm(range(total_frames)):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray,(int(width//sf),int(height//sf)))\n",
    "    eye_vid[t] = gray[100:,75:250]\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dT = 5\n",
    "Tmin = 1497\n",
    "Tmax = Tmin + 15\n",
    "dT_all = Tmax-Tmin\n",
    "\n",
    "moveT0 = np.nanargmin(np.abs(accT2-0))\n",
    "moveTmin = np.nanargmin(np.abs(accT2-Tmin))\n",
    "moveTmax = np.nanargmin(np.abs(accT2-Tmax))\n",
    "\n",
    "eyeT0 = np.nanargmin(np.abs(eyeT-0))\n",
    "eyeTmin = np.nanargmin(np.abs(eyeT-Tmin))\n",
    "eyeTmax = np.nanargmin(np.abs(eyeT-Tmax))\n",
    "\n",
    "worldT0 = np.nanargmin(np.abs(worldT-0))\n",
    "worldTmin = np.nanargmin(np.abs(worldT-Tmin))\n",
    "worldTmax = np.nanargmin(np.abs(worldT-Tmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557aeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldT0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "\n",
    "icons_list = sorted(list(paper_fig_dir.glob('*icon.png')))\n",
    "\n",
    "icons = []\n",
    "for n in icons_list:\n",
    "   icons.append(image.imread(n))\n",
    "\n",
    "probe = image.imread(list(paper_fig_dir.glob('*probe.png'))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d668425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = 5\n",
    "Tmin = 1497\n",
    "Tmax = Tmin + 15\n",
    "dT_all = Tmax-Tmin\n",
    "cell_spikes = []\n",
    "for celln,spks in spikeT.iteritems():\n",
    "    cell_spikes.append(spks[(spks>Tmin) &(spks < Tmax)])\n",
    "\n",
    "\n",
    "wt=500\n",
    "fontsize=24\n",
    "t=worldTmin + wt\n",
    "tt = worldT[t].data\n",
    "eye_interp_t = np.nanargmin(np.abs(eyeT-(tt)))\n",
    "acc_interp_t = np.nanargmin(np.abs(accT2-(tt)))\n",
    "cellT = np.linspace(Tmin,Tmax,worldTmax-worldTmin)\n",
    "cell_t = np.nanargmin(np.abs(cellT-(tt)))\n",
    "\n",
    "fig = plt.figure(constrained_layout=False, figsize=(20,10))\n",
    "gs0 = gridspec.GridSpec(1,3, figure=fig,wspace=.2,hspace=.05)\n",
    "gs0a = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=gs0[0,0], wspace=.01,hspace=.1)\n",
    "gs0b = gridspec.GridSpecFromSubplotSpec(5, 1, subplot_spec=gs0[0,1:], wspace=.01,hspace=.3)\n",
    "\n",
    "gs01 = gridspec.GridSpecFromSubplotSpec(2, 5, subplot_spec=gs0b[:2,:],wspace=1.2,hspace=.5) # Eye Traces\n",
    "gs02 = gridspec.GridSpecFromSubplotSpec(1, 5, subplot_spec=gs0b[2:,:],wspace=1.2,hspace=.7)\n",
    "\n",
    "axs1a = np.array([fig.add_subplot(gs0a[n,m]) for n in range(2) for m in range(1)])\n",
    "axs1b = np.array([fig.add_subplot(gs01[n,1:]) for n in range(2) for m in range(1)])\n",
    "axs1bb = np.array([fig.add_subplot(gs01[n,0]) for n in range(2) for m in range(1)])\n",
    "axs1cc = np.array([fig.add_subplot(gs02[0,0])]) # Raster\n",
    "axs1c = np.array([fig.add_subplot(gs02[0,1:])])\n",
    "\n",
    "#### World/Eye Camera\n",
    "ax = axs1a[0]\n",
    "ax.imshow(world_vid_raw[t],cmap='gray')\n",
    "ax.axis('off')\n",
    "ax = axs1a[1]\n",
    "ax.imshow(eye_vid[eye_interp_t],cmap='gray')\n",
    "ax.axis('off')\n",
    "\n",
    "##### Theta #####\n",
    "ax = axs1bb[0]\n",
    "ax.imshow(icons[0])\n",
    "ax.axis('off')\n",
    "ax = axs1b[0]\n",
    "ax.plot(eyeT[eyeTmin:eyeTmax],th[eyeTmin:eyeTmax]-np.nanmean(th),c='k')\n",
    "ax.set_ylabel('deg',fontsize=fontsize)\n",
    "ax.axvline(x=eyeT[eye_interp_t],c='b',lw=2)\n",
    "\n",
    "##### Phi #####\n",
    "# ax = axs1bb[1]\n",
    "# ax.imshow(icons[1])\n",
    "# ax.axis('off')\n",
    "# ax = axs1b[1]\n",
    "# ax.plot(eyeT[eyeTmin:eyeTmax],-1*(phi[eyeTmin:eyeTmax]-np.nanmean(phi)),c='k')\n",
    "# ax.set_ylabel('deg',fontsize=fontsize)\n",
    "# ax.axvline(x=eyeT[eye_interp_t],c='b',lw=2)\n",
    "\n",
    "##### Pitch #####\n",
    "ax = axs1bb[1]\n",
    "ax.imshow(icons[2])\n",
    "ax.axis('off')\n",
    "ax = axs1b[1]\n",
    "ax.plot(accT2[moveTmin:moveTmax],gpitch[moveTmin:moveTmax]-np.nanmean(gpitch),c='k')\n",
    "ax.set_ylabel('deg',fontsize=fontsize)\n",
    "ax.axvline(x=accT2[acc_interp_t],c='b',lw=2)\n",
    "\n",
    "\n",
    "##### Roll #####\n",
    "# ax = axs1bb[3]\n",
    "# ax.imshow(icons[3])\n",
    "# ax.axis('off')\n",
    "# ax = axs1b[3]\n",
    "# ax.plot(accT2[moveTmin:moveTmax],groll[moveTmin:moveTmax]-np.nanmean(groll),c='k')\n",
    "# ax.set_ylabel('deg',fontsize=fontsize)\n",
    "# ax.axvline(x=accT2[acc_interp_t],c='b',lw=2)\n",
    "\n",
    "for ax, modeln in zip(axs1b,range(len(titles))):\n",
    "    ax.set_yticks([-30,0,30])\n",
    "    ax.set_yticklabels([-30,0,30], fontsize=fontsize)\n",
    "    ax.set_xticks([])#np.arange(Tmin,Tmax+1,5))\n",
    "    # ax.set_xticklabels(np.arange(0,Tmax-Tmin+1,5), fontsize=fontsize)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xlim(Tmin,Tmax+1)\n",
    "\n",
    "\n",
    "ax = axs1cc[0]\n",
    "ax.imshow(probe)\n",
    "ax.axis('off')\n",
    "ax = axs1c[0]\n",
    "ax.eventplot(cell_spikes,color='k',linelengths=.5)\n",
    "ax.set_xlabel('time (sec)',fontsize=fontsize)\n",
    "ax.set_ylabel('cell #',fontsize=fontsize)\n",
    "ax.set_xticks(np.arange(Tmin, Tmax+dT, dT))\n",
    "ax.set_xticklabels(np.arange(0, dT_all+dT, dT), fontsize=fontsize)\n",
    "ax.set_yticks([0,50,100])\n",
    "ax.set_yticklabels([0,50,100], fontsize=fontsize)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.axvline(x=cellT[cell_t],c='b',lw=2)\n",
    "ax.set_xlim(Tmin,Tmax+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(wt, worldTmin,worldTmax,worldT,eyeTmin,eyeTmax,eyeT,eye_vid,accT,moveTmin,moveTmax,world_vid_raw,th,phi,gpitch,groll,cell_spikes, Tmin,Tmax,dT,dT_all, pbar: ActorHandle,):  #\n",
    "    t=worldTmin + wt\n",
    "    tt = worldT[t].data\n",
    "    eye_interp_t = np.nanargmin(np.abs(eyeT-(tt)))\n",
    "    acc_interp_t = np.nanargmin(np.abs(accT2-(tt)))\n",
    "    cellT = np.linspace(Tmin,Tmax,worldTmax-worldTmin)\n",
    "    cell_t = np.nanargmin(np.abs(cellT-(tt)))\n",
    "\n",
    "    fig = plt.figure(constrained_layout=False, figsize=(20,10))\n",
    "    gs0 = gridspec.GridSpec(1,3, figure=fig,wspace=.2,hspace=.05)\n",
    "    gs0a = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=gs0[0,0], wspace=.01,hspace=.1)\n",
    "    gs0b = gridspec.GridSpecFromSubplotSpec(5, 1, subplot_spec=gs0[0,1:], wspace=.01,hspace=.3)\n",
    "\n",
    "    gs01 = gridspec.GridSpecFromSubplotSpec(2, 5, subplot_spec=gs0b[:2,:],wspace=1.2,hspace=.5) # Eye Traces\n",
    "    gs02 = gridspec.GridSpecFromSubplotSpec(1, 5, subplot_spec=gs0b[2:,:],wspace=1.2,hspace=.7)\n",
    "\n",
    "    axs1a = np.array([fig.add_subplot(gs0a[n,m]) for n in range(2) for m in range(1)])\n",
    "    axs1b = np.array([fig.add_subplot(gs01[n,1:]) for n in range(2) for m in range(1)])\n",
    "    axs1bb = np.array([fig.add_subplot(gs01[n,0]) for n in range(2) for m in range(1)])\n",
    "    axs1cc = np.array([fig.add_subplot(gs02[0,0])]) # Raster\n",
    "    axs1c = np.array([fig.add_subplot(gs02[0,1:])])\n",
    "\n",
    "    #### World/Eye Camera\n",
    "    ax = axs1a[0]\n",
    "    ax.imshow(world_vid_raw[t],cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax = axs1a[1]\n",
    "    ax.imshow(eye_vid[eye_interp_t],cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    ##### Theta #####\n",
    "    ax = axs1bb[0]\n",
    "    ax.imshow(icons[0])\n",
    "    ax.axis('off')\n",
    "    ax = axs1b[0]\n",
    "    ax.plot(eyeT[eyeTmin:eyeTmax],th[eyeTmin:eyeTmax]-np.nanmean(th),c='k')\n",
    "    ax.set_ylabel('deg',fontsize=fontsize)\n",
    "    ax.axvline(x=eyeT[eye_interp_t],c='b',lw=2)\n",
    "\n",
    "    ##### Phi #####\n",
    "    # ax = axs1bb[1]\n",
    "    # ax.imshow(icons[1])\n",
    "    # ax.axis('off')\n",
    "    # ax = axs1b[1]\n",
    "    # ax.plot(eyeT[eyeTmin:eyeTmax],-1*(phi[eyeTmin:eyeTmax]-np.nanmean(phi)),c='k')\n",
    "    # ax.set_ylabel('deg',fontsize=fontsize)\n",
    "    # ax.axvline(x=eyeT[eye_interp_t],c='b',lw=2)\n",
    "\n",
    "    ##### Pitch #####\n",
    "    ax = axs1bb[1]\n",
    "    ax.imshow(icons[2])\n",
    "    ax.axis('off')\n",
    "    ax = axs1b[1]\n",
    "    ax.plot(accT2[moveTmin:moveTmax],gpitch[moveTmin:moveTmax]-np.nanmean(gpitch),c='k')\n",
    "    ax.set_ylabel('deg',fontsize=fontsize)\n",
    "    ax.axvline(x=accT2[acc_interp_t],c='b',lw=2)\n",
    "\n",
    "\n",
    "    ##### Roll #####\n",
    "    # ax = axs1bb[3]\n",
    "    # ax.imshow(icons[3])\n",
    "    # ax.axis('off')\n",
    "    # ax = axs1b[3]\n",
    "    # ax.plot(accT2[moveTmin:moveTmax],groll[moveTmin:moveTmax]-np.nanmean(groll),c='k')\n",
    "    # ax.set_ylabel('deg',fontsize=fontsize)\n",
    "    # ax.axvline(x=accT2[acc_interp_t],c='b',lw=2)\n",
    "\n",
    "    for ax, modeln in zip(axs1b,range(len(titles))):\n",
    "        ax.set_yticks([-30,0,30])\n",
    "        ax.set_yticklabels([-30,0,30], fontsize=fontsize)\n",
    "        ax.set_xticks([])#np.arange(Tmin,Tmax+1,5))\n",
    "        # ax.set_xticklabels(np.arange(0,Tmax-Tmin+1,5), fontsize=fontsize)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xlim(Tmin,Tmax+1)\n",
    "\n",
    "\n",
    "    ax = axs1cc[0]\n",
    "    ax.imshow(probe)\n",
    "    ax.axis('off')\n",
    "    ax = axs1c[0]\n",
    "    ax.eventplot(cell_spikes,color='k',linelengths=.5)\n",
    "    ax.set_xlabel('time (sec)',fontsize=fontsize)\n",
    "    ax.set_ylabel('cell #',fontsize=fontsize)\n",
    "    ax.set_xticks(np.arange(Tmin, Tmax+dT, dT))\n",
    "    ax.set_xticklabels(np.arange(0, dT_all+dT, dT), fontsize=fontsize)\n",
    "    ax.set_yticks([0,50,100])\n",
    "    ax.set_yticklabels([0,50,100], fontsize=fontsize)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.axvline(x=cellT[cell_t],c='b',lw=2)\n",
    "    ax.set_xlim(Tmin,Tmax+1)\n",
    "\n",
    "    plt.show()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    \n",
    "    plt.close()\n",
    "    pbar.update.remote(1)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use('agg')\n",
    "\n",
    "dT = 5\n",
    "Tmin = 1497\n",
    "Tmax = Tmin + 15\n",
    "dT_all = Tmax-Tmin\n",
    "\n",
    "moveT0 = np.nanargmin(np.abs(accT2-0))\n",
    "moveTmin = np.nanargmin(np.abs(accT2-Tmin))\n",
    "moveTmax = np.nanargmin(np.abs(accT2-Tmax))\n",
    "\n",
    "eyeT0 = np.nanargmin(np.abs(eyeT-0))\n",
    "eyeTmin = np.nanargmin(np.abs(eyeT-Tmin))\n",
    "eyeTmax = np.nanargmin(np.abs(eyeT-Tmax))\n",
    "\n",
    "worldT0 = np.nanargmin(np.abs(worldT-0))\n",
    "worldTmin = np.nanargmin(np.abs(worldT-Tmin))\n",
    "worldTmax = np.nanargmin(np.abs(worldT-Tmax))\n",
    "\n",
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0\n",
    "time_range = np.arange(worldTmax-worldTmin)\n",
    "num_ticks = np.size(time_range)\n",
    "pb = ProgressBar(num_ticks)\n",
    "actor = pb.actor\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "worldT_r = ray.put(worldT.data)\n",
    "eyeT_r = ray.put(eyeT)\n",
    "accT_r = ray.put(accT)\n",
    "world_vid_raw_r = ray.put(world_vid_raw)\n",
    "th_r = ray.put(th)\n",
    "phi_r = ray.put(phi)\n",
    "gpitch_r = ray.put(gpitch)\n",
    "groll_r = ray.put(groll)\n",
    "cell_spikes_r = ray.put(cell_spikes)\n",
    "eye_vid_r = ray.put(eye_vid)\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t,worldTmin,worldTmax,worldT_r,eyeTmin,eyeTmax,eyeT_r,eye_vid_r,accT_r,moveTmin,moveTmax,world_vid_raw_r,th_r,phi_r,gpitch_r,groll_r,cell_spikes_r, Tmin,Tmax,dT,dT_all, actor))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "pb.print_until_done()\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'ExampleData.avi'\n",
    "\n",
    "\n",
    "vid_name = paper_fig_dir / aniname\n",
    "FPS = 60\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc('M','J','P','G'), FPS, (images.shape[-2], images.shape[-3])) #cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, pb,worldT_r,eyeT_r,accT_r,world_vid_raw_r,th_r,phi_r,gpitch_r,groll_r,cell_spikes_r,eye_vid_r\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bf83c",
   "metadata": {},
   "source": [
    "## Video S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_dict, params, downsamp_vid=1, do_worldcam_correction=False, reprocess=False, medfiltbins=11):\n",
    "\n",
    "    # open worldcam\n",
    "    print('opening worldcam')\n",
    "    world_data = xr.open_dataset(file_dict['world'])\n",
    "    world_vid_raw = np.uint8(world_data['WORLD_video'])\n",
    "    # resize worldcam\n",
    "    sz = world_vid_raw.shape # raw video size\n",
    "    # if size is larger than the target 60x80, resize by 0.5\n",
    "    if sz[1]>160:\n",
    "        downsamp = 0.5\n",
    "        world_vid = np.zeros((sz[0],int(sz[1]*downsamp),int(sz[2]*downsamp)), dtype = 'uint8')\n",
    "        for f in range(sz[0]):\n",
    "            world_vid[f,:,:] = cv2.resize(world_vid_raw[f,:,:],(int(sz[2]*downsamp),int(sz[1]*downsamp)))\n",
    "    else:\n",
    "        # if the worldcam has already been resized when the nc file was written in preprocessing, don't resize\n",
    "        world_vid = world_vid_raw.copy()\n",
    "    del world_vid_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # world timestamps\n",
    "    worldT = world_data.timestamps.copy()\n",
    "\n",
    "    # load IMU data\n",
    "    if file_dict['imu'] is not None:\n",
    "        print('opening imu data')\n",
    "        imu_data = xr.open_dataset(file_dict['imu'])\n",
    "        try:\n",
    "            accT = imu_data.IMU_data.sample # imu timestamps\n",
    "            acc_chans = imu_data.IMU_data # imu dample data\n",
    "        except AttributeError:\n",
    "            accT = imu_data.__xarray_dataarray_variable__.sample\n",
    "            acc_chans = imu_data.__xarray_dataarray_variable__\n",
    "        # raw gyro values\n",
    "        gx = np.array(acc_chans.sel(channel='gyro_x_raw'))\n",
    "        gy = np.array(acc_chans.sel(channel='gyro_y_raw'))\n",
    "        gz = np.array(acc_chans.sel(channel='gyro_z_raw'))\n",
    "        # gyro values in degrees\n",
    "        gx_deg = np.array(acc_chans.sel(channel='gyro_x'))\n",
    "        gy_deg = np.array(acc_chans.sel(channel='gyro_y'))\n",
    "        gz_deg = np.array(acc_chans.sel(channel='gyro_z'))\n",
    "        # pitch and roll in deg\n",
    "        groll = medfilt(np.array(acc_chans.sel(channel='roll')),medfiltbins)\n",
    "        gpitch = medfilt(np.array(acc_chans.sel(channel='pitch')),medfiltbins)\n",
    "\n",
    "    else: \n",
    "        accT = []\n",
    "        gz = []\n",
    "        groll = []\n",
    "        gpitch = []\n",
    "\n",
    "\n",
    "    print('opening ephys data')\n",
    "    # ephys data for this individual recording\n",
    "    ephys_data = pd.read_json(file_dict['ephys'])\n",
    "    # sort units by shank and site order\n",
    "    ephys_data = ephys_data.sort_values(by='ch', axis=0, ascending=True)\n",
    "    ephys_data = ephys_data.reset_index()\n",
    "    ephys_data = ephys_data.drop('index', axis=1)\n",
    "    # spike times\n",
    "    ephys_data['spikeTraw'] = ephys_data['spikeT']\n",
    "    print('getting good cells')\n",
    "    # select good cells from phy2\n",
    "    goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "    units = goodcells.index.values\n",
    "    # get number of good units\n",
    "    n_units = len(goodcells)\n",
    "\n",
    "    print('opening eyecam data')\n",
    "    # load eye data\n",
    "    eye_data = xr.open_dataset(file_dict['eye'])\n",
    "    eye_vid = np.uint8(eye_data['REYE_video'])\n",
    "    eyeT = eye_data.timestamps.copy()\n",
    "\n",
    "    # plot eye postion across recording\n",
    "    eye_params = eye_data['REYE_ellipse_params']\n",
    "    # define theta, phi and zero-center\n",
    "    th = np.array((eye_params.sel(ellipse_params = 'theta'))*180/np.pi)\n",
    "    phi = np.array((eye_params.sel(ellipse_params = 'phi'))*180/np.pi)\n",
    "\n",
    "    print('adjusting camera times to match ephys')\n",
    "    # adjust eye/world/top times relative to ephys\n",
    "    ephysT0 = ephys_data.iloc[0,12]\n",
    "    eyeT = eye_data.timestamps  - ephysT0\n",
    "    if eyeT[0]<-600:\n",
    "        eyeT = eyeT + 8*60*60 # 8hr offset for some data\n",
    "    worldT = world_data.timestamps - ephysT0\n",
    "    if worldT[0]<-600:\n",
    "        worldT = worldT + 8*60*60\n",
    "    accTraw = accT - ephysT0\n",
    "\n",
    "    ##### Clear some memory #####\n",
    "    del eye_data \n",
    "    gc.collect()\n",
    "    print(world_vid.shape)\n",
    "\n",
    "\n",
    "    # calculate eye veloctiy\n",
    "    dEye = np.diff(th)\n",
    "    accT_correction_file = params['save_dir']/'acct_correction.h5'\n",
    "    if (accT_correction_file.exists()):\n",
    "        accT_correction = ioh5.load(accT_correction_file)\n",
    "        offset0    = accT_correction['offset0']\n",
    "        drift_rate = accT_correction['drift_rate']\n",
    "        accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "    else:\n",
    "        # check accelerometer / eye temporal alignment\n",
    "        if (file_dict['imu'] is not None):\n",
    "            print('checking accelerometer / eye temporal alignment')\n",
    "            lag_range = np.arange(-0.2,0.2,0.002)\n",
    "            cc = np.zeros(np.shape(lag_range))\n",
    "            t1 = np.arange(5,len(dEye)/60-120,20).astype(int) # was np.arange(5,1600,20), changed for shorter videos\n",
    "            t2 = t1 + 60\n",
    "            offset = np.zeros(np.shape(t1))\n",
    "            ccmax = np.zeros(np.shape(t1))\n",
    "            acc_interp = interp1d(accTraw, (gz-3)*7.5)\n",
    "            for tstart in tqdm(range(len(t1))):\n",
    "                for l in range(len(lag_range)):\n",
    "                    try:\n",
    "                        c, lag= nanxcorr(-dEye[t1[tstart]*60 : t2[tstart]*60] , acc_interp(eyeT[t1[tstart]*60:t2[tstart]*60]+lag_range[l]),1)\n",
    "                        cc[l] = c[1]\n",
    "                    except: # occasional problem with operands that cannot be broadcast togther because of different shapes\n",
    "                        cc[l] = np.nan\n",
    "                offset[tstart] = lag_range[np.argmax(cc)]    \n",
    "                ccmax[tstart] = np.max(cc)\n",
    "            offset[ccmax<0.1] = np.nan\n",
    "            del ccmax, dEye\n",
    "            gc.collect()\n",
    "            if np.isnan(offset).all():\n",
    "                found_good_offset = False\n",
    "            else:\n",
    "                found_good_offset = True\n",
    "\n",
    "        if file_dict['imu'] is not None and found_good_offset is True:\n",
    "            print('fitting regression to timing drift')\n",
    "            # fit regression to timing drift\n",
    "            model = LinearRegression()\n",
    "            dataT = np.array(eyeT[t1*60 + 30*60])\n",
    "            model.fit(dataT[~np.isnan(offset)].reshape(-1,1),offset[~np.isnan(offset)]) \n",
    "            offset0 = model.intercept_\n",
    "            drift_rate = model.coef_\n",
    "            del dataT\n",
    "            gc.collect()\n",
    "        elif file_dict['speed'] is not None or found_good_offset is False:\n",
    "            offset0 = 0.1\n",
    "            drift_rate = -0.000114\n",
    "        if file_dict['imu'] is not None:\n",
    "            accT_correction = {'offset0': offset0, 'drift_rate': drift_rate}\n",
    "            ioh5.save(accT_correction_file,accT_correction)\n",
    "            accT = accTraw - (offset0 + accTraw*drift_rate)\n",
    "            del accTraw\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    print('correcting ephys spike times for offset and timing drift')\n",
    "    for i in ephys_data.index:\n",
    "        ephys_data.at[i,'spikeT'] = np.array(ephys_data.at[i,'spikeTraw']) - (offset0 + np.array(ephys_data.at[i,'spikeTraw']) *drift_rate)\n",
    "    goodcells = ephys_data.loc[ephys_data['group']=='good']\n",
    "\n",
    "    return eyeT,accT,worldT,th,phi,groll,gpitch,gz_deg,world_vid,ephys_data\n",
    "\n",
    "eyeT,accT,worldT,th,phi,groll,gpitch,gz_deg,world_vid,ephys_data = load_raw_data(file_dict,params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Load single shifter network and shift videos #####\n",
    "best_shifter=np.nanargmin(np.nanmean(GLM_Shifter['loss_regcv'][0],axis=-1))\n",
    "reg_alph=a=0; l=best_shifter\n",
    "print(best_shifter)\n",
    "Kfold=0\n",
    "shift_in=3\n",
    "shift_hidden=20\n",
    "shift_out=3\n",
    "save_model_fm = params['save_model_shift'].parent.parent.parent / fm_dir / 'CropInputs/Shifter'\n",
    "\n",
    "shifter_nn = nn.Sequential(\n",
    "                nn.Linear(shift_in,shift_hidden),\n",
    "                nn.Softplus(),\n",
    "                nn.Linear(shift_hidden, shift_out))#.to(device)\n",
    "model_name = 'GLM_{}_WC{}_dt{:03d}_T{:02d}_MovModel{:d}_NB{}_alph{}_lam{}_Kfold{:01d}.pth'.format('Pytorch_VisShifter_NoL1','UC',int(.05*1000), 1, MovModel, 5000,a,l,Kfold)\n",
    "\n",
    "checkpoint = torch.load(save_model_fm/model_name)\n",
    "pretrained_dict = {'.'.join(k.split('.')[1:]): v for k, v in checkpoint['model_state_dict'].items() if 'shift' in k}\n",
    "shifter_nn.load_state_dict(pretrained_dict)\n",
    "\n",
    "ds=4/params['downsamp_vid']\n",
    "shift_out = shifter_nn(torch.from_numpy(model_move[:,(0,1,3)].astype(np.float32)))#.to(device))\n",
    "shift = Affine(angle=torch.clamp(shift_out[:,-1],min=-45,max=45),translation=torch.clamp(shift_out[:,:2]*ds,min=-20*ds,max=20*ds))\n",
    "vid_tensor=torch.from_numpy(data['model_vid_sm'][:,np.newaxis].astype(np.float32))#.to(device)\n",
    "model_vid_sm_shift = shift(vid_tensor.contiguous()).detach().numpy().squeeze()\n",
    "model_vid_sm_shift = model_vid_sm_shift.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee38fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_plt_im(t, model_vid_sm, model_vid_sm_shift, pbar: ActorHandle,):  #\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    axs[0].imshow(model_vid_sm[t], cmap='gray')\n",
    "    axs[0].set_title('Head-centric'.format(t),fontsize=20)\n",
    "    axs[1].imshow(model_vid_sm_shift[t], cmap='gray')\n",
    "    axs[1].set_title('Retinocentric'.format(t),fontsize=20)\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi()\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    images = np.frombuffer(fig.canvas.tostring_rgb(),\n",
    "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
    "    \n",
    "    plt.close()\n",
    "    pbar.update.remote(1)\n",
    "    return images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cac11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.use('agg')\n",
    "\n",
    "start = time.time()\n",
    "##### initialize time points for animation and progressbar #####\n",
    "t = 0\n",
    "time_range = np.arange(moveTmin,moveTmax)# model_vid_sm_shift.shape[0])\n",
    "num_ticks = np.size(time_range)\n",
    "pb = ProgressBar(num_ticks)\n",
    "actor = pb.actor\n",
    "\n",
    "##### Put large arrays into shared memory #####\n",
    "model_vid_sm_shfit_r = ray.put(model_vid_sm_shift)\n",
    "model_vid_sm_r = ray.put(world_vid)\n",
    "\n",
    "##### Loop over parameters appending process ids #####\n",
    "result_ids = []\n",
    "for t in time_range:\n",
    "    result_ids.append(make_plt_im.remote(t, model_vid_sm_r, model_vid_sm_shfit_r, actor))\n",
    "\n",
    "##### pring progressbar and get results #####\n",
    "pb.print_until_done()\n",
    "results_p = ray.get(result_ids)\n",
    "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
    "\n",
    "##### Make video with opencv #####\n",
    "aniname = 'Figure_SVideo2b.mp4'\n",
    "\n",
    "\n",
    "vid_name = paper_fig_dir / aniname\n",
    "FPS = int(1/.016)\n",
    "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
    "\n",
    "for fm in tqdm(range(images.shape[0])):\n",
    "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
    "out.release()\n",
    "print('Making Animation {}: {}'.format(aniname, time.time()-start))\n",
    "del results_p, pb, model_vid_sm_r, model_vid_sm_shfit_r\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('pytorchGLM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3496b118a4d22d6a6f5485f441160c3e17db178642e3548badf1988c7bf383fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
